{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135423c2",
   "metadata": {},
   "source": [
    "# Tiny Neural Network using The Autograd Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfab81",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, I build a tiny neural network entirely on top of my own reverse-mode autodiff engine.\n",
    "\n",
    "There is:\n",
    "- No NumPy\n",
    "- No PyTorch\n",
    "- No vectorization\n",
    "- Only scalar Nodes and a computational graph\n",
    "\n",
    "The goal is not performance — it is understanding gradient flow deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6146a85",
   "metadata": {},
   "source": [
    "Importing our autograd engine as a library to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3561fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de77e43",
   "metadata": {},
   "source": [
    "## Phase 1 — Single Input, Single Neuron\n",
    "\n",
    "We start with the simplest possible model:\n",
    "\n",
    "y = w·x + b\n",
    "\n",
    "Loss:\n",
    "L = (y − y_true)²\n",
    "\n",
    "This phase validates:\n",
    "\n",
    "- Parameter nodes store gradients correctly\n",
    "- Multiplication and addition propagate gradients\n",
    "- reverse-mode traversal works\n",
    "- zero_grad lifecycle is correct\n",
    "- manual derivative matches engine output\n",
    "\n",
    "If this fails, the engine is broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50610c",
   "metadata": {},
   "source": [
    "### Single-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e648c",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e394a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba937",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5697f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "ytrue = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75b56e",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1adb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -40 -20\n",
      "manual: -40 -20\n",
      "0 100\n",
      "engine: -36.0 -18.0\n",
      "manual: -36.0 -18.0\n",
      "1 81.0\n",
      "engine: -32.4 -16.2\n",
      "manual: -32.4 -16.2\n",
      "2 65.61\n",
      "engine: -29.16 -14.58\n",
      "manual: -29.16 -14.58\n",
      "3 53.1441\n",
      "engine: -26.244 -13.122\n",
      "manual: -26.244 -13.122\n",
      "4 43.046721\n",
      "engine: -23.6196 -11.8098\n",
      "manual: -23.6196 -11.8098\n",
      "5 34.86784400999999\n",
      "engine: -21.25764 -10.62882\n",
      "manual: -21.25764 -10.62882\n",
      "6 28.242953648099995\n",
      "engine: -19.131876 -9.565938\n",
      "manual: -19.131876 -9.565938\n",
      "7 22.876792454960995\n",
      "engine: -17.218688399999994 -8.609344199999997\n",
      "manual: -17.218688399999994 -8.609344199999997\n",
      "8 18.5302018885184\n",
      "engine: -15.496819559999999 -7.748409779999999\n",
      "manual: -15.496819559999999 -7.748409779999999\n",
      "9 15.009463529699909\n",
      "engine: -13.947137603999998 -6.973568801999999\n",
      "manual: -13.947137603999998 -6.973568801999999\n",
      "10 12.157665459056926\n",
      "engine: -12.552423843599996 -6.276211921799998\n",
      "manual: -12.552423843599996 -6.276211921799998\n",
      "11 9.847709021836106\n",
      "engine: -11.29718145924 -5.64859072962\n",
      "manual: -11.29718145924 -5.64859072962\n",
      "12 7.976644307687252\n",
      "engine: -10.167463313315999 -5.083731656657999\n",
      "manual: -10.167463313315999 -5.083731656657999\n",
      "13 6.461081889226672\n",
      "engine: -9.150716981984395 -4.575358490992198\n",
      "manual: -9.150716981984395 -4.575358490992198\n",
      "14 5.2334763302736\n",
      "engine: -8.235645283785956 -4.117822641892978\n",
      "manual: -8.235645283785956 -4.117822641892978\n",
      "15 4.2391158275216165\n",
      "engine: -7.412080755407359 -3.7060403777036797\n",
      "manual: -7.412080755407359 -3.7060403777036797\n",
      "16 3.433683820292508\n",
      "engine: -6.670872679866626 -3.335436339933313\n",
      "manual: -6.670872679866626 -3.335436339933313\n",
      "17 2.781283894436934\n",
      "engine: -6.00378541187996 -3.00189270593998\n",
      "manual: -6.00378541187996 -3.00189270593998\n",
      "18 2.252839954493914\n",
      "engine: -5.403406870691967 -2.7017034353459835\n",
      "manual: -5.403406870691967 -2.7017034353459835\n",
      "19 1.8248003631400722\n",
      "engine: -4.86306618362277 -2.431533091811385\n",
      "manual: -4.86306618362277 -2.431533091811385\n",
      "20 1.4780882941434585\n",
      "engine: -4.3767595652604925 -2.1883797826302462\n",
      "manual: -4.3767595652604925 -2.1883797826302462\n",
      "21 1.197251518256201\n",
      "engine: -3.9390836087344425 -1.9695418043672213\n",
      "manual: -3.9390836087344425 -1.9695418043672213\n",
      "22 0.9697737297875224\n",
      "engine: -3.545175247860996 -1.772587623930498\n",
      "manual: -3.545175247860996 -1.772587623930498\n",
      "23 0.7855167211278922\n",
      "engine: -3.1906577230748994 -1.5953288615374497\n",
      "manual: -3.1906577230748994 -1.5953288615374497\n",
      "24 0.6362685441135938\n",
      "engine: -2.87159195076741 -1.435795975383705\n",
      "manual: -2.87159195076741 -1.435795975383705\n",
      "25 0.5153775207320113\n",
      "engine: -2.5844327556906705 -1.2922163778453353\n",
      "manual: -2.5844327556906705 -1.2922163778453353\n",
      "26 0.4174557917929296\n",
      "engine: -2.3259894801215992 -1.1629947400607996\n",
      "manual: -2.3259894801215992 -1.1629947400607996\n",
      "27 0.3381391913522717\n",
      "engine: -2.0933905321094386 -1.0466952660547193\n",
      "manual: -2.0933905321094386 -1.0466952660547193\n",
      "28 0.2738927449953399\n",
      "engine: -1.884051478898499 -0.9420257394492495\n",
      "manual: -1.884051478898499 -0.9420257394492495\n",
      "29 0.22185312344622632\n",
      "engine: -1.6956463310086463 -0.8478231655043231\n",
      "manual: -1.6956463310086463 -0.8478231655043231\n",
      "30 0.17970102999144272\n",
      "engine: -1.5260816979077845 -0.7630408489538922\n",
      "manual: -1.5260816979077845 -0.7630408489538922\n",
      "31 0.14555783429306915\n",
      "engine: -1.3734735281170032 -0.6867367640585016\n",
      "manual: -1.3734735281170032 -0.6867367640585016\n",
      "32 0.11790184577738552\n",
      "engine: -1.2361261753053014 -0.6180630876526507\n",
      "manual: -1.2361261753053014 -0.6180630876526507\n",
      "33 0.09550049507968206\n",
      "engine: -1.1125135577747756 -0.5562567788873878\n",
      "manual: -1.1125135577747756 -0.5562567788873878\n",
      "34 0.07735540101454305\n",
      "engine: -1.0012622019972923 -0.5006311009986462\n",
      "manual: -1.0012622019972923 -0.5006311009986462\n",
      "35 0.06265787482177916\n",
      "engine: -0.9011359817975659 -0.45056799089878297\n",
      "manual: -0.9011359817975659 -0.45056799089878297\n",
      "36 0.05075287860564144\n",
      "engine: -0.8110223836178108 -0.4055111918089054\n",
      "manual: -0.8110223836178108 -0.4055111918089054\n",
      "37 0.04110983167056971\n",
      "engine: -0.729920145256024 -0.364960072628012\n",
      "manual: -0.729920145256024 -0.364960072628012\n",
      "38 0.03329896365316095\n",
      "engine: -0.656928130730428 -0.328464065365214\n",
      "manual: -0.656928130730428 -0.328464065365214\n",
      "39 0.026972160559060893\n",
      "engine: -0.5912353176573788 -0.2956176588286894\n",
      "manual: -0.5912353176573788 -0.2956176588286894\n",
      "40 0.021847450052838852\n",
      "engine: -0.5321117858916438 -0.2660558929458219\n",
      "manual: -0.5321117858916438 -0.2660558929458219\n",
      "41 0.01769643454279966\n",
      "engine: -0.47890060730247797 -0.23945030365123898\n",
      "manual: -0.47890060730247797 -0.23945030365123898\n",
      "42 0.014334111979667639\n",
      "engine: -0.4310105465722316 -0.2155052732861158\n",
      "manual: -0.4310105465722316 -0.2155052732861158\n",
      "43 0.011610630703530864\n",
      "engine: -0.38790949191501056 -0.19395474595750528\n",
      "manual: -0.38790949191501056 -0.19395474595750528\n",
      "44 0.009404610869860103\n",
      "engine: -0.3491185427235095 -0.17455927136175475\n",
      "manual: -0.3491185427235095 -0.17455927136175475\n",
      "45 0.007617734804586683\n",
      "engine: -0.31420668845115785 -0.15710334422557892\n",
      "manual: -0.31420668845115785 -0.15710334422557892\n",
      "46 0.006170365191715186\n",
      "engine: -0.28278601960603567 -0.14139300980301783\n",
      "manual: -0.28278601960603567 -0.14139300980301783\n",
      "47 0.004997995805289074\n",
      "engine: -0.25450741764543494 -0.12725370882271747\n",
      "manual: -0.25450741764543494 -0.12725370882271747\n",
      "48 0.004048376602284241\n",
      "engine: -0.22905667588089074 -0.11452833794044537\n",
      "manual: -0.22905667588089074 -0.11452833794044537\n",
      "49 0.0032791850478502147\n",
      "engine: -0.20615100829279953 -0.10307550414639977\n",
      "manual: -0.20615100829279953 -0.10307550414639977\n",
      "50 0.002656139888758619\n",
      "engine: -0.18553590746351745 -0.09276795373175872\n",
      "manual: -0.18553590746351745 -0.09276795373175872\n",
      "51 0.002151473309894432\n",
      "engine: -0.16698231671716712 -0.08349115835858356\n",
      "manual: -0.16698231671716712 -0.08349115835858356\n",
      "52 0.0017426933810145194\n",
      "engine: -0.150284085045449 -0.0751420425227245\n",
      "manual: -0.150284085045449 -0.0751420425227245\n",
      "53 0.0014115816386217341\n",
      "engine: -0.13525567654090764 -0.06762783827045382\n",
      "manual: -0.13525567654090764 -0.06762783827045382\n",
      "54 0.0011433811272836647\n",
      "engine: -0.12173010888681546 -0.06086505444340773\n",
      "manual: -0.12173010888681546 -0.06086505444340773\n",
      "55 0.0009261387130997467\n",
      "engine: -0.10955709799813462 -0.05477854899906731\n",
      "manual: -0.10955709799813462 -0.05477854899906731\n",
      "56 0.0007501723576108046\n",
      "engine: -0.09860138819831832 -0.04930069409915916\n",
      "manual: -0.09860138819831832 -0.04930069409915916\n",
      "57 0.0006076396096647167\n",
      "engine: -0.08874124937848649 -0.04437062468924324\n",
      "manual: -0.08874124937848649 -0.04437062468924324\n",
      "58 0.0004921880838284205\n",
      "engine: -0.07986712444063926 -0.03993356222031963\n",
      "manual: -0.07986712444063926 -0.03993356222031963\n",
      "59 0.0003986723479010348\n",
      "engine: -0.07188041199657391 -0.035940205998286956\n",
      "manual: -0.07188041199657391 -0.035940205998286956\n",
      "60 0.0003229246017998254\n",
      "engine: -0.06469237079691226 -0.03234618539845613\n",
      "manual: -0.06469237079691226 -0.03234618539845613\n",
      "61 0.00026156892745782414\n",
      "engine: -0.05822313371722743 -0.029111566858613713\n",
      "manual: -0.05822313371722743 -0.029111566858613713\n",
      "62 0.00021187083124088408\n",
      "engine: -0.05240082034550397 -0.026200410172751987\n",
      "manual: -0.05240082034550397 -0.026200410172751987\n",
      "63 0.00017161537330511145\n",
      "engine: -0.04716073831095002 -0.02358036915547501\n",
      "manual: -0.04716073831095002 -0.02358036915547501\n",
      "64 0.00013900845237711933\n",
      "engine: -0.04244466447985218 -0.02122233223992609\n",
      "manual: -0.04244466447985218 -0.02122233223992609\n",
      "65 0.00011259684642545158\n",
      "engine: -0.03820019803186625 -0.019100099015933125\n",
      "manual: -0.03820019803186625 -0.019100099015933125\n",
      "66 9.120344560461239e-05\n",
      "engine: -0.034380178228680336 -0.017190089114340168\n",
      "manual: -0.034380178228680336 -0.017190089114340168\n",
      "67 7.387479093973908e-05\n",
      "engine: -0.030942160405814434 -0.015471080202907217\n",
      "manual: -0.030942160405814434 -0.015471080202907217\n",
      "68 5.9838580661196906e-05\n",
      "engine: -0.02784794436523441 -0.013923972182617206\n",
      "manual: -0.02784794436523441 -0.013923972182617206\n",
      "69 4.846925033557444e-05\n",
      "engine: -0.025063149928705286 -0.012531574964352643\n",
      "manual: -0.025063149928705286 -0.012531574964352643\n",
      "70 3.926009277179749e-05\n",
      "engine: -0.022556834935841152 -0.011278417467920576\n",
      "manual: -0.022556834935841152 -0.011278417467920576\n",
      "71 3.1800675145173994e-05\n",
      "engine: -0.020301151442254195 -0.010150575721127097\n",
      "manual: -0.020301151442254195 -0.010150575721127097\n",
      "72 2.5758546867583725e-05\n",
      "engine: -0.018271036298024512 -0.009135518149012256\n",
      "manual: -0.018271036298024512 -0.009135518149012256\n",
      "73 2.086442296273308e-05\n",
      "engine: -0.016443932668224193 -0.008221966334112096\n",
      "manual: -0.016443932668224193 -0.008221966334112096\n",
      "74 1.6900182599818178e-05\n",
      "engine: -0.014799539401401773 -0.007399769700700887\n",
      "manual: -0.014799539401401773 -0.007399769700700887\n",
      "75 1.3689147905852722e-05\n",
      "engine: -0.013319585461260885 -0.006659792730630443\n",
      "manual: -0.013319585461260885 -0.006659792730630443\n",
      "76 1.1088209803739523e-05\n",
      "engine: -0.011987626915136218 -0.005993813457568109\n",
      "manual: -0.011987626915136218 -0.005993813457568109\n",
      "77 8.981449941031142e-06\n",
      "engine: -0.010788864223620465 -0.005394432111810232\n",
      "manual: -0.010788864223620465 -0.005394432111810232\n",
      "78 7.27497445223235e-06\n",
      "engine: -0.009709977801264813 -0.0048549889006324065\n",
      "manual: -0.009709977801264813 -0.0048549889006324065\n",
      "79 5.892729306315966e-06\n",
      "engine: -0.00873898002113549 -0.004369490010567745\n",
      "manual: -0.00873898002113549 -0.004369490010567745\n",
      "80 4.773110738112827e-06\n",
      "engine: -0.007865082019023362 -0.003932541009511681\n",
      "manual: -0.007865082019023362 -0.003932541009511681\n",
      "81 3.866219697872787e-06\n",
      "engine: -0.007078573817118183 -0.0035392869085590917\n",
      "manual: -0.007078573817118183 -0.0035392869085590917\n",
      "82 3.131637955274443e-06\n",
      "engine: -0.0063707164354056545 -0.0031853582177028272\n",
      "manual: -0.0063707164354056545 -0.0031853582177028272\n",
      "83 2.536626743771733e-06\n",
      "engine: -0.005733644791867221 -0.0028668223959336103\n",
      "manual: -0.005733644791867221 -0.0028668223959336103\n",
      "84 2.0546676624566314e-06\n",
      "engine: -0.005160280312679788 -0.002580140156339894\n",
      "manual: -0.005160280312679788 -0.002580140156339894\n",
      "85 1.664280806589413e-06\n",
      "engine: -0.004644252281408967 -0.0023221261407044835\n",
      "manual: -0.004644252281408967 -0.0023221261407044835\n",
      "86 1.3480674533357747e-06\n",
      "engine: -0.004179827053263807 -0.0020899135266319036\n",
      "manual: -0.004179827053263807 -0.0020899135266319036\n",
      "87 1.0919346371997501e-06\n",
      "engine: -0.0037618443479345842 -0.0018809221739672921\n",
      "manual: -0.0037618443479345842 -0.0018809221739672921\n",
      "88 8.844670561304611e-07\n",
      "engine: -0.003385659913142547 -0.0016928299565712734\n",
      "manual: -0.003385659913142547 -0.0016928299565712734\n",
      "89 7.164183154662749e-07\n",
      "engine: -0.0030470939218290027 -0.0015235469609145014\n",
      "manual: -0.0030470939218290027 -0.0015235469609145014\n",
      "90 5.802988355279533e-07\n",
      "engine: -0.002742384529646813 -0.0013711922648234065\n",
      "manual: -0.002742384529646813 -0.0013711922648234065\n",
      "91 4.700420567778857e-07\n",
      "engine: -0.00246814607668 -0.00123407303834\n",
      "manual: -0.00246814607668 -0.00123407303834\n",
      "92 3.807340659894298e-07\n",
      "engine: -0.0022213314690162633 -0.0011106657345081317\n",
      "manual: -0.0022213314690162633 -0.0011106657345081317\n",
      "93 3.083945934526219e-07\n",
      "engine: -0.0019991983221103737 -0.0009995991610551869\n",
      "manual: -0.0019991983221103737 -0.0009995991610551869\n",
      "94 2.4979962069555833e-07\n",
      "engine: -0.0017992784899050207 -0.0008996392449525104\n",
      "manual: -0.0017992784899050207 -0.0008996392449525104\n",
      "95 2.0233769276468074e-07\n",
      "engine: -0.0016193506409152292 -0.0008096753204576146\n",
      "manual: -0.0016193506409152292 -0.0008096753204576146\n",
      "96 1.6389353113953522e-07\n",
      "engine: -0.0014574155768229957 -0.0007287077884114979\n",
      "manual: -0.0014574155768229957 -0.0007287077884114979\n",
      "97 1.3275376022289407e-07\n",
      "engine: -0.0013116740191350118 -0.0006558370095675059\n",
      "manual: -0.0013116740191350118 -0.0006558370095675059\n",
      "98 1.0753054577961221e-07\n",
      "engine: -0.0011805066172243528 -0.0005902533086121764\n",
      "manual: -0.0011805066172243528 -0.0005902533086121764\n",
      "99 8.709974208190529e-08\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w)\n",
    "    zero_grad(b)\n",
    "\n",
    "    y = w*x + b\n",
    "    l = (y-ytrue)**2\n",
    "    \n",
    "    backward(l)\n",
    "    \n",
    "    manual_dw = 2*(y.value - ytrue)*x\n",
    "    manual_db = 2*(y.value - ytrue)\n",
    "\n",
    "    print(\"engine:\", w.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw, manual_db)\n",
    "    \n",
    "    w.value -= n*w.grad\n",
    "    b.value -= n*b.grad\n",
    "    \n",
    "    print(step, l.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fbeae",
   "metadata": {},
   "source": [
    "### What This Confirms\n",
    "\n",
    "The gradients printed by the engine match the manually derived gradients:\n",
    "\n",
    "dL/dw = 2(y − y_true)x  \n",
    "dL/db = 2(y − y_true)\n",
    "\n",
    "This confirms:\n",
    "\n",
    "- Chain rule is implemented correctly\n",
    "- Gradients accumulate via +=\n",
    "- Backward traversal is correct\n",
    "- Parameter updates modify value, not graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137dcdc",
   "metadata": {},
   "source": [
    "## Phase 2 — Multi-Input Neuron\n",
    "\n",
    "Now we extend the neuron to:\n",
    "\n",
    "y = w1·x1 + w2·x2 + b\n",
    "\n",
    "This introduces multiple parents in the computational graph.\n",
    "\n",
    "Graph structure becomes branched:\n",
    "\n",
    "(w1·x1) + (w2·x2) + b\n",
    "\n",
    "Reverse-mode must:\n",
    "\n",
    "- Propagate gradients through multiple branches\n",
    "- Accumulate gradient contributions correctly\n",
    "- Handle shared nodes safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21203234",
   "metadata": {},
   "source": [
    "### Multi-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d0a5b",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5eab5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47a78",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 3\n",
    "ytrue = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96085",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117a12c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -46.08 -69.12 -23.04\n",
      "manual: -46.08 -69.12 -23.04\n",
      "step: 1 Loss: 132.7104\n",
      "engine: -33.1776 -49.7664 -16.5888\n",
      "manual: -33.1776 -49.7664 -16.5888\n",
      "step: 2 Loss: 68.79707135999999\n",
      "engine: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "manual: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "step: 3 Loss: 35.664401793023984\n",
      "engine: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "manual: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "step: 4 Loss: 18.488425889503635\n",
      "engine: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "manual: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "step: 5 Loss: 9.584399981118693\n",
      "engine: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "manual: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "step: 6 Loss: 4.968552950211923\n",
      "engine: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "manual: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "step: 7 Loss: 2.5756978493898646\n",
      "engine: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "manual: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "step: 8 Loss: 1.3352417651237016\n",
      "engine: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "manual: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "step: 9 Loss: 0.6921893310401275\n",
      "engine: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "manual: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "step: 10 Loss: 0.35883094921120207\n",
      "engine: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "manual: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "step: 11 Loss: 0.18601796407108673\n",
      "engine: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "manual: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "step: 12 Loss: 0.09643171257445238\n",
      "engine: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "manual: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "step: 13 Loss: 0.049990199798596015\n",
      "engine: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "manual: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "step: 14 Loss: 0.025914919575591878\n",
      "engine: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "manual: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "step: 15 Loss: 0.013434294307986566\n",
      "engine: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "manual: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "step: 16 Loss: 0.006964338169260342\n",
      "engine: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "manual: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "step: 17 Loss: 0.0036103129069445785\n",
      "engine: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "manual: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "step: 18 Loss: 0.0018715862109599897\n",
      "engine: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "manual: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "step: 19 Loss: 0.0009702302917615258\n",
      "engine: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "manual: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "step: 20 Loss: 0.00050296738324921\n",
      "engine: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "manual: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "step: 21 Loss: 0.000260738291476379\n",
      "engine: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "manual: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "step: 22 Loss: 0.00013516673030137142\n",
      "engine: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "manual: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "step: 23 Loss: 7.00704329882464e-05\n",
      "engine: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "manual: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "step: 24 Loss: 3.6324512461082954e-05\n",
      "engine: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "manual: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "step: 25 Loss: 1.883062725985562e-05\n",
      "engine: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "manual: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "step: 26 Loss: 9.761797171499829e-06\n",
      "engine: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "manual: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "step: 27 Loss: 5.060515653709667e-06\n",
      "engine: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "manual: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "step: 28 Loss: 2.6233713148759562e-06\n",
      "engine: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "manual: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "step: 29 Loss: 1.3599556896305355e-06\n",
      "engine: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "manual: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "step: 30 Loss: 7.050010295060208e-07\n",
      "engine: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "manual: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "step: 31 Loss: 3.654725336985844e-07\n",
      "engine: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "manual: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "step: 32 Loss: 1.894609614669338e-07\n",
      "engine: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "manual: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "step: 33 Loss: 9.821656242414673e-08\n",
      "engine: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "manual: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "step: 34 Loss: 5.0915465961094525e-08\n",
      "engine: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "manual: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "step: 35 Loss: 2.6394577554231402e-08\n",
      "engine: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "manual: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "step: 36 Loss: 1.368294900422992e-08\n",
      "engine: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "manual: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "step: 37 Loss: 7.0932407638047585e-09\n",
      "engine: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "manual: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "step: 38 Loss: 3.6771360120942648e-09\n",
      "engine: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "manual: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "step: 39 Loss: 1.9062273085828036e-09\n",
      "engine: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "manual: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "step: 40 Loss: 9.881882366799804e-10\n",
      "engine: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "manual: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "step: 41 Loss: 5.122767820396407e-10\n",
      "engine: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "manual: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "step: 42 Loss: 2.655642836935587e-10\n",
      "engine: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "manual: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "step: 43 Loss: 1.3766852470509082e-10\n",
      "engine: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "manual: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "step: 44 Loss: 7.136736320591857e-11\n",
      "engine: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "manual: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "step: 45 Loss: 3.699684109113444e-11\n",
      "engine: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "manual: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "step: 46 Loss: 1.9179162423511145e-11\n",
      "engine: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "manual: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "step: 47 Loss: 9.9424777931787e-12\n",
      "engine: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "manual: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "step: 48 Loss: 5.154180495404247e-12\n",
      "engine: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "manual: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "step: 49 Loss: 2.671927166726942e-12\n",
      "engine: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "manual: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "step: 50 Loss: 1.3851270464089885e-12\n",
      "engine: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "manual: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "step: 51 Loss: 7.180498583296061e-13\n",
      "engine: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "manual: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "step: 52 Loss: 3.722370470782809e-13\n",
      "engine: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "manual: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "step: 53 Loss: 1.9296768383201834e-13\n",
      "engine: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "manual: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "step: 54 Loss: 1.0003444828733929e-13\n",
      "engine: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "manual: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "step: 55 Loss: 5.1857858380493653e-14\n",
      "engine: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "manual: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "step: 56 Loss: 2.688311322524268e-14\n",
      "engine: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "manual: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "step: 57 Loss: 1.3936205963070433e-14\n",
      "engine: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "manual: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "step: 58 Loss: 7.224529207492211e-15\n",
      "engine: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "manual: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "step: 59 Loss: 3.7451960194348e-15\n",
      "engine: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "manual: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "step: 60 Loss: 1.941509647783336e-15\n",
      "engine: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "manual: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "step: 61 Loss: 1.0064784120580766e-15\n",
      "engine: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "manual: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "step: 62 Loss: 5.217584380253387e-16\n",
      "engine: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "manual: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "step: 63 Loss: 2.7047962568973864e-16\n",
      "engine: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "manual: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "step: 64 Loss: 1.4021654204074408e-16\n",
      "engine: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "manual: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "step: 65 Loss: 7.268827477921314e-17\n",
      "engine: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "manual: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "step: 66 Loss: 3.768161211360287e-17\n",
      "engine: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "manual: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "step: 67 Loss: 1.953415776903015e-17\n",
      "engine: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "manual: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "step: 68 Loss: 1.0126506030804414e-17\n",
      "engine: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "manual: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "step: 69 Loss: 5.2495735632025836e-18\n",
      "engine: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "manual: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "step: 70 Loss: 2.721383858213691e-18\n",
      "engine: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "manual: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "step: 71 Loss: 1.410763535404805e-18\n",
      "engine: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "manual: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "step: 72 Loss: 7.313403028698023e-19\n",
      "engine: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "manual: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "step: 73 Loss: 3.791296130412209e-19\n",
      "engine: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "manual: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "step: 74 Loss: 1.9653896437957275e-19\n",
      "engine: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "manual: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "step: 75 Loss: 1.0188793108506885e-19\n",
      "engine: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "manual: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "step: 76 Loss: 5.281791964122085e-20\n",
      "engine: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "manual: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "step: 77 Loss: 2.7380715482445175e-20\n",
      "engine: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "manual: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "step: 78 Loss: 1.4194399936986123e-20\n",
      "engine: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "manual: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "step: 79 Loss: 7.358559781631122e-21\n",
      "engine: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "manual: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "step: 80 Loss: 3.8143438697772514e-21\n",
      "engine: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "manual: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "step: 81 Loss: 1.977520164900858e-21\n",
      "engine: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "manual: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "step: 82 Loss: 1.0250918539904007e-21\n",
      "engine: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "manual: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "step: 83 Loss: 5.314665854129708e-22\n",
      "engine: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "manual: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "step: 84 Loss: 2.755028427684428e-22\n",
      "engine: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "manual: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "step: 85 Loss: 1.4283426045583935e-22\n",
      "engine: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "manual: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "step: 86 Loss: 7.40403893494799e-23\n",
      "engine: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "manual: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "step: 87 Loss: 3.836757225630106e-23\n",
      "engine: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "manual: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "step: 88 Loss: 1.9879610355931586e-23\n",
      "engine: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "manual: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "step: 89 Loss: 1.031471605633751e-23\n",
      "engine: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "manual: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "step: 90 Loss: 5.340907028151959e-24\n",
      "engine: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "manual: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "step: 91 Loss: 2.770381680383946e-24\n",
      "engine: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "manual: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "step: 92 Loss: 1.4334423063207214e-24\n",
      "engine: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "manual: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "step: 93 Loss: 7.483734081214486e-25\n",
      "engine: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "manual: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "step: 94 Loss: 3.8433618846729784e-25\n",
      "engine: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "manual: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "step: 95 Loss: 2.0038329170062053e-25\n",
      "engine: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "manual: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "step: 96 Loss: 1.0337548846378227e-25\n",
      "engine: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "manual: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "step: 97 Loss: 5.415056797799113e-26\n",
      "engine: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "manual: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "step: 98 Loss: 2.8477878678478526e-26\n",
      "engine: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "manual: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "step: 99 Loss: 1.4590771302967834e-26\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w1)\n",
    "    zero_grad(w2)\n",
    "    zero_grad(b)\n",
    "    \n",
    "    y = w1*x1 + w2*x2 + b\n",
    "    l = (y-ytrue)**2\n",
    "\n",
    "    backward(l)\n",
    "\n",
    "    delta = 2*(y.value - ytrue)\n",
    "\n",
    "    manual_dw1 = delta * x1\n",
    "    manual_dw2 = delta * x2\n",
    "    manual_db = delta\n",
    "    \n",
    "    print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "    w1.value -= n * w1.grad\n",
    "    w2.value -= n * w2.grad\n",
    "    b.value -= n * b.grad\n",
    "\n",
    "    print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21723baf",
   "metadata": {},
   "source": [
    "Now for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbe8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83bde1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -6.16 -6.16 -6.16\n",
      "manual: -6.16 -6.16 -6.16\n",
      "step: 0 Loss: 9.4864\n",
      "engine: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "manual: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "step: 1 Loss: 124.33142016000002\n",
      "engine: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "manual: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "step: 1 Loss: 2.4247223511039984\n",
      "engine: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "manual: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "step: 2 Loss: 61.48802068101364\n",
      "engine: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "manual: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "step: 2 Loss: 0.2732689308521132\n",
      "engine: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "manual: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "step: 3 Loss: 31.170997460282468\n",
      "engine: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "manual: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "step: 3 Loss: 0.03189284586729491\n",
      "engine: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "manual: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "step: 4 Loss: 16.33179658342692\n",
      "engine: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "manual: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "step: 4 Loss: 0.4261765009868418\n",
      "engine: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "manual: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "step: 5 Loss: 8.92842546475222\n",
      "engine: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "manual: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "step: 5 Loss: 0.9452083970312428\n",
      "engine: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "manual: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "step: 6 Loss: 5.144096642410019\n",
      "engine: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "manual: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "step: 6 Loss: 1.4067203007232514\n",
      "engine: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "manual: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "step: 7 Loss: 3.1517950617409176\n",
      "engine: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "manual: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "step: 7 Loss: 1.7633949131753535\n",
      "engine: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "manual: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "step: 8 Loss: 2.0666613938848766\n",
      "engine: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "manual: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "step: 8 Loss: 2.0185695667441066\n",
      "engine: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "manual: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "step: 9 Loss: 1.4533641651357307\n",
      "engine: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "manual: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "step: 9 Loss: 2.19094635544156\n",
      "engine: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "manual: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "step: 10 Loss: 1.0933256565007086\n",
      "engine: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "manual: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "step: 10 Loss: 2.300828311885715\n",
      "engine: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "manual: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "step: 11 Loss: 0.8739813892243515\n",
      "engine: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "manual: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "step: 11 Loss: 2.3655105271774577\n",
      "engine: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "manual: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "step: 12 Loss: 0.7355957569813151\n",
      "engine: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "manual: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "step: 12 Loss: 2.39834960772937\n",
      "engine: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "manual: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "step: 13 Loss: 0.6453885758378625\n",
      "engine: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "manual: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "step: 13 Loss: 2.409151760465584\n",
      "engine: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "manual: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "step: 14 Loss: 0.5847310457140027\n",
      "engine: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "manual: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "step: 14 Loss: 2.404908812355005\n",
      "engine: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "manual: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "step: 15 Loss: 0.5426690205794957\n",
      "engine: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "manual: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "step: 15 Loss: 2.3905164858697434\n",
      "engine: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "manual: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "step: 16 Loss: 0.5125575727467586\n",
      "engine: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "manual: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "step: 16 Loss: 2.369362902444438\n",
      "engine: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "manual: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "step: 17 Loss: 0.4902566940029108\n",
      "engine: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "manual: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "step: 17 Loss: 2.343774490634568\n",
      "engine: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "manual: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "step: 18 Loss: 0.4731303138646301\n",
      "engine: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "manual: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "step: 18 Loss: 2.3153399170214897\n",
      "engine: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "manual: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "step: 19 Loss: 0.45947122411636654\n",
      "engine: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "manual: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "step: 19 Loss: 2.2851396186686763\n",
      "engine: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "manual: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "step: 20 Loss: 0.44815948369410363\n",
      "engine: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "manual: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "step: 20 Loss: 2.2539059385445808\n",
      "engine: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "manual: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "step: 21 Loss: 0.43845337086104164\n",
      "engine: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "manual: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "step: 21 Loss: 2.222133731465707\n",
      "engine: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "manual: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "step: 22 Loss: 0.42985819753731525\n",
      "engine: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "manual: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "step: 22 Loss: 2.1901562572347317\n",
      "engine: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "manual: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "step: 23 Loss: 0.42204231181996454\n",
      "engine: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "manual: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "step: 23 Loss: 2.15819702616437\n",
      "engine: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "manual: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "step: 24 Loss: 0.41478247554885156\n",
      "engine: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "manual: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "step: 24 Loss: 2.1264051115482006\n",
      "engine: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "manual: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "step: 25 Loss: 0.4079279314824542\n",
      "engine: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "manual: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "step: 25 Loss: 2.0948791527557025\n",
      "engine: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "manual: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "step: 26 Loss: 0.4013765655318351\n",
      "engine: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "manual: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "step: 26 Loss: 2.0636836484305476\n",
      "engine: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "manual: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "step: 27 Loss: 0.39505899726057586\n",
      "engine: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "manual: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "step: 27 Loss: 2.0328600057264787\n",
      "engine: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "manual: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "step: 28 Loss: 0.38892791640667984\n",
      "engine: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "manual: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "step: 28 Loss: 2.002434028386879\n",
      "engine: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "manual: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "step: 29 Loss: 0.38295091418671107\n",
      "engine: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "manual: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "step: 29 Loss: 1.9724209890373758\n",
      "engine: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "manual: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "step: 30 Loss: 0.37710565395421897\n",
      "engine: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "manual: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "step: 30 Loss: 1.9428290638754178\n",
      "engine: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "manual: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "step: 31 Loss: 0.3713766130825078\n",
      "engine: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "manual: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "step: 31 Loss: 1.9136616578256025\n",
      "engine: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "manual: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "step: 32 Loss: 0.3657528826485063\n",
      "engine: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "manual: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "step: 32 Loss: 1.884918978206111\n",
      "engine: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "manual: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "step: 33 Loss: 0.3602266804314682\n",
      "engine: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "manual: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "step: 33 Loss: 1.8565990995326203\n",
      "engine: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "manual: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "step: 34 Loss: 0.3547923454722424\n",
      "engine: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "manual: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "step: 34 Loss: 1.8286986838094759\n",
      "engine: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "manual: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "step: 35 Loss: 0.3494456579904879\n",
      "engine: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "manual: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "step: 35 Loss: 1.8012134676049756\n",
      "engine: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "manual: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "step: 36 Loss: 0.3441833792432918\n",
      "engine: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "manual: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "step: 36 Loss: 1.7741385912669188\n",
      "engine: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "manual: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "step: 37 Loss: 0.33900294011960047\n",
      "engine: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "manual: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "step: 37 Loss: 1.7474688212932457\n",
      "engine: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "manual: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "step: 38 Loss: 0.33390223034363575\n",
      "engine: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "manual: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "step: 38 Loss: 1.7211987003911826\n",
      "engine: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "manual: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "step: 39 Loss: 0.3288794557454377\n",
      "engine: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "manual: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "step: 39 Loss: 1.6953226486000565\n",
      "engine: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "manual: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "step: 40 Loss: 0.3239330415882179\n",
      "engine: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "manual: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "step: 40 Loss: 1.6698350312993278\n",
      "engine: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "manual: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "step: 41 Loss: 0.3190615670625761\n",
      "engine: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "manual: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "step: 41 Loss: 1.6447302048105017\n",
      "engine: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "manual: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "step: 42 Loss: 0.3142637208729977\n",
      "engine: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "manual: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "step: 42 Loss: 1.6200025468407362\n",
      "engine: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "manual: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "step: 43 Loss: 0.30953827109960846\n",
      "engine: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "manual: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "step: 43 Loss: 1.5956464766734495\n",
      "engine: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "manual: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "step: 44 Loss: 0.3048840447220777\n",
      "engine: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "manual: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "step: 44 Loss: 1.5716564684259073\n",
      "engine: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "manual: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "step: 45 Loss: 0.300299913683752\n",
      "engine: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "manual: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "step: 45 Loss: 1.5480270596206327\n",
      "engine: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "manual: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "step: 46 Loss: 0.2957847853833086\n",
      "engine: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "manual: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "step: 46 Loss: 1.5247528565912059\n",
      "engine: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "manual: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "step: 47 Loss: 0.29133759616397464\n",
      "engine: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "manual: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "step: 47 Loss: 1.5018285377515561\n",
      "engine: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "manual: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "step: 48 Loss: 0.2869573068327053\n",
      "engine: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "manual: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "step: 48 Loss: 1.4792488554250944\n",
      "engine: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "manual: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "step: 49 Loss: 0.2826428995542664\n",
      "engine: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "manual: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "step: 49 Loss: 1.4570086367048911\n",
      "engine: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "manual: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "step: 50 Loss: 0.2783933756769821\n",
      "engine: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "manual: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "step: 50 Loss: 1.4351027836637926\n",
      "engine: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "manual: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "step: 51 Loss: 0.2742077541900975\n",
      "engine: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "manual: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "step: 51 Loss: 1.4135262731301397\n",
      "engine: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "manual: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "step: 52 Loss: 0.27008507060972936\n",
      "engine: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "manual: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "step: 52 Loss: 1.3922741561749867\n",
      "engine: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "manual: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "step: 53 Loss: 0.26602437615586866\n",
      "engine: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "manual: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "step: 53 Loss: 1.3713415574095253\n",
      "engine: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "manual: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "step: 54 Loss: 0.2620247371274635\n",
      "engine: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "manual: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "step: 54 Loss: 1.3507236741593498\n",
      "engine: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "manual: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "step: 55 Loss: 0.2580852344125613\n",
      "engine: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "manual: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "step: 55 Loss: 1.3304157755607073\n",
      "engine: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "manual: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "step: 56 Loss: 0.2542049630908597\n",
      "engine: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "manual: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "step: 56 Loss: 1.3104132016090455\n",
      "engine: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "manual: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "step: 57 Loss: 0.250383032099792\n",
      "engine: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "manual: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "step: 57 Loss: 1.2907113621804505\n",
      "engine: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "manual: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "step: 58 Loss: 0.24661856394460346\n",
      "engine: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "manual: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "step: 58 Loss: 1.2713057360397508\n",
      "engine: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "manual: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "step: 59 Loss: 0.2429106944391491\n",
      "engine: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "manual: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "step: 59 Loss: 1.252191869844529\n",
      "engine: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "manual: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "step: 60 Loss: 0.23925857246842042\n",
      "engine: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "manual: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "step: 60 Loss: 1.2333653771512865\n",
      "engine: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "manual: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "step: 61 Loss: 0.23566135976675504\n",
      "engine: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "manual: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "step: 61 Loss: 1.2148219374278124\n",
      "engine: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "manual: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "step: 62 Loss: 0.2321182307075031\n",
      "engine: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "manual: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "step: 62 Loss: 1.1965572950745806\n",
      "engine: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "manual: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "step: 63 Loss: 0.22862837210144146\n",
      "engine: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "manual: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "step: 63 Loss: 1.1785672584568012\n",
      "engine: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "manual: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "step: 64 Loss: 0.22519098300191842\n",
      "engine: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "manual: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "step: 64 Loss: 1.1608476989483916\n",
      "engine: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "manual: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "step: 65 Loss: 0.22180527451550977\n",
      "engine: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "manual: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "step: 65 Loss: 1.1433945499884604\n",
      "engine: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "manual: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "step: 66 Loss: 0.2184704696171666\n",
      "engine: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "manual: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "step: 66 Loss: 1.1262038061507598\n",
      "engine: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "manual: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "step: 67 Loss: 0.21518580296939233\n",
      "engine: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "manual: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "step: 67 Loss: 1.1092715222262894\n",
      "engine: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "manual: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "step: 68 Loss: 0.2119505207448405\n",
      "engine: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "manual: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "step: 68 Loss: 1.092593812319109\n",
      "engine: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "manual: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "step: 69 Loss: 0.2087638804521744\n",
      "engine: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "manual: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "step: 69 Loss: 1.0761668489553933\n",
      "engine: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "manual: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "step: 70 Loss: 0.20562515076485635\n",
      "engine: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "manual: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "step: 70 Loss: 1.0599868622056043\n",
      "engine: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "manual: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "step: 71 Loss: 0.202533611352796\n",
      "engine: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "manual: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "step: 71 Loss: 1.0440501388196815\n",
      "engine: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "manual: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "step: 72 Loss: 0.1994885527166665\n",
      "engine: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "manual: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "step: 72 Loss: 1.0283530213751464\n",
      "engine: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "manual: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "step: 73 Loss: 0.19648927602481972\n",
      "engine: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "manual: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "step: 73 Loss: 1.0128919074378824\n",
      "engine: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "manual: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "step: 74 Loss: 0.19353509295277788\n",
      "engine: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "manual: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "step: 74 Loss: 0.997663248735532\n",
      "engine: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "manual: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "step: 75 Loss: 0.19062532552510686\n",
      "engine: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "manual: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "step: 75 Loss: 0.9826635503432468\n",
      "engine: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "manual: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "step: 76 Loss: 0.18775930595978385\n",
      "engine: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "manual: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "step: 76 Loss: 0.967889369881674\n",
      "engine: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "manual: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "step: 77 Loss: 0.184936376514874\n",
      "engine: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "manual: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "step: 77 Loss: 0.9533373167269823\n",
      "engine: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "manual: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "step: 78 Loss: 0.1821558893375515\n",
      "engine: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "manual: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "step: 78 Loss: 0.9390040512327443\n",
      "engine: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "manual: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "step: 79 Loss: 0.179417206315393\n",
      "engine: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "manual: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "step: 79 Loss: 0.9248862839635181\n",
      "engine: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "manual: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "step: 80 Loss: 0.17671969892992428\n",
      "engine: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "manual: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "step: 80 Loss: 0.9109807749399496\n",
      "engine: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "manual: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "step: 81 Loss: 0.1740627481123579\n",
      "engine: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "manual: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "step: 81 Loss: 0.8972843328952061\n",
      "engine: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "manual: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "step: 82 Loss: 0.1714457441015169\n",
      "engine: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "manual: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "step: 82 Loss: 0.883793814542602\n",
      "engine: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "manual: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "step: 83 Loss: 0.16886808630389047\n",
      "engine: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "manual: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "step: 83 Loss: 0.8705061238542326\n",
      "engine: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "manual: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "step: 84 Loss: 0.1663291831557962\n",
      "engine: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "manual: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "step: 84 Loss: 0.8574182113504414\n",
      "engine: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "manual: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "step: 85 Loss: 0.16382845198761636\n",
      "engine: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "manual: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "step: 85 Loss: 0.8445270733999863\n",
      "engine: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "manual: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "step: 86 Loss: 0.16136531889007713\n",
      "engine: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "manual: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "step: 86 Loss: 0.8318297515306993\n",
      "engine: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "manual: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "step: 87 Loss: 0.15893921858253074\n",
      "engine: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "manual: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "step: 87 Loss: 0.8193233317505548\n",
      "engine: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "manual: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "step: 88 Loss: 0.15654959428323653\n",
      "engine: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "manual: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "step: 88 Loss: 0.8070049438788954\n",
      "engine: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "manual: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "step: 89 Loss: 0.15419589758156857\n",
      "engine: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "manual: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "step: 89 Loss: 0.7948717608877401\n",
      "engine: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "manual: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "step: 90 Loss: 0.1518775883121652\n",
      "engine: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "manual: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "step: 90 Loss: 0.782920998253007\n",
      "engine: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "manual: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "step: 91 Loss: 0.14959413443096498\n",
      "engine: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "manual: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "step: 91 Loss: 0.7711499133154561\n",
      "engine: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "manual: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "step: 92 Loss: 0.1473450118930849\n",
      "engine: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "manual: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "step: 92 Loss: 0.7595558046512645\n",
      "engine: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "manual: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "step: 93 Loss: 0.1451297045325846\n",
      "engine: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "manual: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "step: 93 Loss: 0.7481360114520615\n",
      "engine: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "manual: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "step: 94 Loss: 0.14294770394398368\n",
      "engine: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "manual: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "step: 94 Loss: 0.7368879129142843\n",
      "engine: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "manual: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "step: 95 Loss: 0.14079850936559313\n",
      "engine: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "manual: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "step: 95 Loss: 0.725808927637705\n",
      "engine: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "manual: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "step: 96 Loss: 0.13868162756459101\n",
      "engine: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "manual: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "step: 96 Loss: 0.7148965130330096\n",
      "engine: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "manual: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "step: 97 Loss: 0.13659657272382797\n",
      "engine: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "manual: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "step: 97 Loss: 0.7041481647382901\n",
      "engine: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "manual: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "step: 98 Loss: 0.13454286633033774\n",
      "engine: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "manual: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "step: 98 Loss: 0.6935614160442927\n",
      "engine: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "manual: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "step: 99 Loss: 0.13252003706550594\n",
      "engine: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "manual: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "step: 99 Loss: 0.6831338373283254\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        \n",
    "        zero_grad(w1)\n",
    "        zero_grad(w2)\n",
    "        zero_grad(b)\n",
    "\n",
    "        y = w1*x1 + w2*x2 + b\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "        \n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        w1.value -= n * w1.grad\n",
    "        w2.value -= n * w2.grad\n",
    "        b.value -= n * b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fe5c5",
   "metadata": {},
   "source": [
    "### Observations with Dataset Training\n",
    "\n",
    "Training over multiple examples introduces SGD dynamics.\n",
    "\n",
    "We observe:\n",
    "\n",
    "- Loss oscillates because updates happen per sample\n",
    "- Gradients must be zeroed before backward\n",
    "- Parameters must persist across steps\n",
    "\n",
    "This validates the full training lifecycle:\n",
    "\n",
    "1. zero_grad  \n",
    "2. forward  \n",
    "3. backward  \n",
    "4. update  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e5b5a",
   "metadata": {},
   "source": [
    "## Class Functions of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c765068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4405ab",
   "metadata": {},
   "source": [
    "### Abstraction — Turning Neuron into a Class\n",
    "\n",
    "To scale the system, we abstract the neuron into a reusable class.\n",
    "\n",
    "This introduces:\n",
    "\n",
    "- Weight vector abstraction\n",
    "- Dimension safety checks\n",
    "- Dot-product computation\n",
    "- Random initialization to break symmetry\n",
    "\n",
    "Now the neuron represents a hyperplane in ℝⁿ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d95f",
   "metadata": {},
   "source": [
    "#### For a single input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron for a single input\n",
    "class neuron:\n",
    "    def __init__(self):\n",
    "        self.w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.b = Node(0)\n",
    "    def pred(self, x):\n",
    "        y = self.w * x + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39690",
   "metadata": {},
   "source": [
    "#### For multi-input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated __init___ and pred for multi-inputs\n",
    "def __init__(self, dim=1):\n",
    "    self.w = []\n",
    "    self.b = Node(0)\n",
    "    self.dim = dim\n",
    "    for i in range (0, self.dim):\n",
    "        w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.w.append(w)\n",
    "        i += 1\n",
    "\n",
    "neuron.__init__ = __init__\n",
    "\n",
    "def pred(self, x):\n",
    "    if len(x) != len(self.w):\n",
    "        raise ValueError(\"Input dimension does not match neuron weight dimension\")\n",
    "    else:\n",
    "        wx = Node(0)\n",
    "        for i in range(0, self.dim):\n",
    "            wx += x[i]*self.w[i]\n",
    "        y = wx + self.b\n",
    "        return y\n",
    "\n",
    "neuron.pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd57a4a",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba23d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64.31670124157894 -96.47505186236842 -32.15835062078947\n",
      "manual: -64.31670124157894 -96.47505186236842 -32.15835062078947\n",
      "step: 0 Loss: 258.5398786624076\n",
      "engine: -6.218299199837374 -6.218299199837374 -6.218299199837374\n",
      "manual: -6.218299199837374 -6.218299199837374 -6.218299199837374\n",
      "step: 0 Loss: 9.666811234674531\n",
      "engine: -44.81563308597587 -67.2234496289638 -22.407816542987934\n",
      "manual: -44.81563308597587 -67.2234496289638 -22.407816542987934\n",
      "step: 1 Loss: 125.52756055605093\n",
      "engine: -3.1562632626885794 -3.1562632626885794 -3.1562632626885794\n",
      "manual: -3.1562632626885794 -3.1562632626885794 -3.1562632626885794\n",
      "step: 1 Loss: 2.490499445849389\n",
      "engine: -31.509752638857364 -47.264628958286046 -15.754876319428682\n",
      "manual: -31.509752638857364 -47.264628958286046 -15.754876319428682\n",
      "step: 2 Loss: 62.054031960123666\n",
      "engine: -1.0763023085958228 -1.0763023085958228 -1.0763023085958228\n",
      "manual: -1.0763023085958228 -1.0763023085958228 -1.0763023085958228\n",
      "step: 2 Loss: 0.2896066648721744\n",
      "engine: -22.4287093459143 -33.64306401887145 -11.21435467295715\n",
      "manual: -22.4287093459143 -33.64306401887145 -11.21435467295715\n",
      "step: 3 Loss: 31.440437682718965\n",
      "engine: 0.3339983906747843 0.3339983906747843 0.3339983906747843\n",
      "manual: 0.3339983906747843 0.3339983906747843 0.3339983906747843\n",
      "step: 3 Loss: 0.027888731243336462\n",
      "engine: -16.228830342820252 -24.34324551423038 -8.114415171410126\n",
      "manual: -16.228830342820252 -24.34324551423038 -8.114415171410126\n",
      "step: 4 Loss: 16.460933393502707\n",
      "engine: 1.2876883078035135 1.2876883078035135 1.2876883078035135\n",
      "manual: 1.2876883078035135 1.2876883078035135 1.2876883078035135\n",
      "step: 4 Loss: 0.41453529451346904\n",
      "engine: -11.993803040703419 -17.99070456105513 -5.9969015203517095\n",
      "manual: -11.993803040703419 -17.99070456105513 -5.9969015203517095\n",
      "step: 5 Loss: 8.99070696119916\n",
      "engine: 1.930055191777507 1.930055191777507 1.930055191777507\n",
      "manual: 1.930055191777507 1.930055191777507 1.930055191777507\n",
      "step: 5 Loss: 0.9312782608268274\n",
      "engine: -9.098751435333064 -13.648127152999596 -4.549375717666532\n",
      "manual: -9.098751435333064 -13.648127152999596 -4.549375717666532\n",
      "step: 6 Loss: 5.1742048551234685\n",
      "engine: 2.3601769663908403 2.3601769663908403 2.3601769663908403\n",
      "manual: 2.3601769663908403 2.3601769663908403 2.3601769663908403\n",
      "step: 6 Loss: 1.3926088281704674\n",
      "engine: -7.117543505373604 -10.676315258060406 -3.558771752686802\n",
      "manual: -7.117543505373604 -10.676315258060406 -3.558771752686802\n",
      "step: 7 Loss: 3.166214096930373\n",
      "engine: 2.6456189587298073 2.6456189587298073 2.6456189587298073\n",
      "manual: 2.6456189587298073 2.6456189587298073 2.6456189587298073\n",
      "step: 7 Loss: 1.7498249186976476\n",
      "engine: -5.7595798739641495 -8.639369810946224 -2.8797899369820747\n",
      "manual: -5.7595798739641495 -8.639369810946224 -2.8797899369820747\n",
      "step: 8 Loss: 2.0732975202858057\n",
      "engine: 2.8324566136438687 2.8324566136438687 2.8324566136438687\n",
      "manual: 2.8324566136438687 2.8324566136438687 2.8324566136438687\n",
      "step: 8 Loss: 2.005702617043723\n",
      "engine: -4.826687096528715 -7.240030644793073 -2.4133435482643577\n",
      "manual: -4.826687096528715 -7.240030644793073 -2.4133435482643577\n",
      "step: 9 Loss: 1.4560567704873\n",
      "engine: 2.952110442616961 2.952110442616961 2.952110442616961\n",
      "manual: 2.952110442616961 2.952110442616961 2.952110442616961\n",
      "step: 9 Loss: 2.1787390163520275\n",
      "engine: -4.183721215728738 -6.275581823593107 -2.091860607864369\n",
      "manual: -4.183721215728738 -6.275581823593107 -2.091860607864369\n",
      "step: 10 Loss: 1.093970200683672\n",
      "engine: 3.0260070890036666 3.0260070890036666 3.0260070890036666\n",
      "manual: 3.0260070890036666 3.0260070890036666 3.0260070890036666\n",
      "step: 10 Loss: 2.289179725675111\n",
      "engine: -3.738520976685564 -5.607781465028346 -1.869260488342782\n",
      "manual: -3.738520976685564 -5.607781465028346 -1.869260488342782\n",
      "step: 11 Loss: 0.8735336933198738\n",
      "engine: 3.068757922264581 3.068757922264581 3.068757922264581\n",
      "manual: 3.068757922264581 3.068757922264581 3.068757922264581\n",
      "step: 11 Loss: 2.3543187963654066\n",
      "engine: -3.4282370045571113 -5.142355506835667 -1.7141185022785557\n",
      "manual: -3.4282370045571113 -5.142355506835667 -1.7141185022785557\n",
      "step: 12 Loss: 0.7345505599634197\n",
      "engine: 3.0903266672021346 3.0903266672021346 3.0903266672021346\n",
      "manual: 3.0903266672021346 3.0903266672021346 3.0903266672021346\n",
      "step: 12 Loss: 2.387529727505163\n",
      "engine: -3.2100090434096344 -4.815013565114452 -1.6050045217048172\n",
      "manual: -3.2100090434096344 -4.815013565114452 -1.6050045217048172\n",
      "step: 13 Loss: 0.6440098786732272\n",
      "engine: 3.097507609774585 3.097507609774585 3.097507609774585\n",
      "manual: 3.097507609774585 3.097507609774585 3.097507609774585\n",
      "step: 13 Loss: 2.3986383481528653\n",
      "engine: -3.0546083376008184 -4.581912506401228 -1.5273041688004092\n",
      "manual: -3.0546083376008184 -4.581912506401228 -1.5273041688004092\n",
      "step: 14 Loss: 0.5831645060087772\n",
      "engine: 3.0949336534441603 3.0949336534441603 3.0949336534441603\n",
      "manual: 3.0949336534441603 3.0949336534441603 3.0949336534441603\n",
      "step: 14 Loss: 2.3946535798053046\n",
      "engine: -2.9421020798992004 -4.4131531198488005 -1.4710510399496002\n",
      "manual: -2.9421020798992004 -4.4131531198488005 -1.4710510399496002\n",
      "step: 15 Loss: 0.5409977905342\n",
      "engine: 3.0857637590314617 3.0857637590314617 3.0857637590314617\n",
      "manual: 3.0857637590314617 3.0857637590314617 3.0857637590314617\n",
      "step: 15 Loss: 2.380484494137994\n",
      "engine: -2.8588967996949677 -4.2883451995424515 -1.4294483998474838\n",
      "manual: -2.8588967996949677 -4.2883451995424515 -1.4294483998474838\n",
      "step: 16 Loss: 0.5108306819566331\n",
      "engine: 3.0721517414712736 3.0721517414712736 3.0721517414712736\n",
      "manual: 3.0721517414712736 3.0721517414712736 3.0721517414712736\n",
      "step: 16 Loss: 2.3595290806562446\n",
      "engine: -2.7957221137334756 -4.193583170600213 -1.3978610568667378\n",
      "manual: -2.7957221137334756 -4.193583170600213 -1.3978610568667378\n",
      "step: 17 Loss: 0.48850388357614827\n",
      "engine: 3.055565963807007 3.055565963807007 3.055565963807007\n",
      "manual: 3.055565963807007 3.055565963807007 3.055565963807007\n",
      "step: 17 Loss: 2.334120839793961\n",
      "engine: -2.746255753201787 -4.11938362980268 -1.3731278766008934\n",
      "manual: -2.746255753201787 -4.11938362980268 -1.3731278766008934\n",
      "step: 18 Loss: 0.4713700413746196\n",
      "engine: 3.0370073511706916 3.0370073511706916 3.0370073511706916\n",
      "manual: 3.0370073511706916 3.0370073511706916 3.0370073511706916\n",
      "step: 18 Loss: 2.305853412766205\n",
      "engine: -2.7061859065862492 -4.059278859879374 -1.3530929532931246\n",
      "manual: -2.7061859065862492 -4.059278859879374 -1.3530929532931246\n",
      "step: 19 Loss: 0.4577151350628775\n",
      "engine: 3.017158064495625 3.017158064495625 3.017158064495625\n",
      "manual: 3.017158064495625 3.017158064495625 3.017158064495625\n",
      "step: 19 Loss: 2.2758106965377465\n",
      "engine: -2.672571788221063 -4.008857682331595 -1.3362858941105316\n",
      "manual: -2.672571788221063 -4.008857682331595 -1.3362858941105316\n",
      "step: 20 Loss: 0.44641499769969567\n",
      "engine: 2.996482887919152 2.996482887919152 2.996482887919152\n",
      "manual: 2.996482887919152 2.996482887919152 2.996482887919152\n",
      "step: 20 Loss: 2.244727424398075\n",
      "engine: -2.6434075806197654 -3.965111370929648 -1.3217037903098827\n",
      "manual: -2.6434075806197654 -3.965111370929648 -1.3217037903098827\n",
      "step: 21 Loss: 0.4367252273298776\n",
      "engine: 2.9752983694811874 2.9752983694811874 2.9752983694811874\n",
      "manual: 2.9752983694811874 2.9752983694811874 2.9752983694811874\n",
      "step: 21 Loss: 2.2131000968593533\n",
      "engine: -2.617325066721719 -3.9259876000825784 -1.3086625333608595\n",
      "manual: -2.617325066721719 -3.9259876000825784 -1.3086625333608595\n",
      "step: 22 Loss: 0.4281494065556157\n",
      "engine: 2.9538199713156192 2.9538199713156192 2.9538199713156192\n",
      "manual: 2.9538199713156192 2.9538199713156192 2.9538199713156192\n",
      "step: 22 Loss: 2.181263105735751\n",
      "engine: -2.593390841155376 -3.890086261733064 -1.296695420577688\n",
      "manual: -2.593390841155376 -3.890086261733064 -1.296695420577688\n",
      "step: 23 Loss: 0.42035475343678674\n",
      "engine: 2.932194223506004 2.932194223506004 2.932194223506004\n",
      "manual: 2.932194223506004 2.932194223506004 2.932194223506004\n",
      "step: 23 Loss: 2.149440741090494\n",
      "engine: -2.570968019273316 -3.856452028909974 -1.285484009636658\n",
      "manual: -2.570968019273316 -3.856452028909974 -1.285484009636658\n",
      "step: 24 Loss: 0.41311728475788484\n",
      "engine: 2.9105206512520425 2.9105206512520425 2.9105206512520425\n",
      "manual: 2.9105206512520425 2.9105206512520425 2.9105206512520425\n",
      "step: 24 Loss: 2.1177826153411536\n",
      "engine: -2.549621930177274 -3.824432895265911 -1.274810965088637\n",
      "manual: -2.549621930177274 -3.824432895265911 -1.274810965088637\n",
      "step: 25 Loss: 0.4062857491775555\n",
      "engine: 2.8888667279875584 2.8888667279875584 2.8888667279875584\n",
      "manual: 2.8888667279875584 2.8888667279875584 2.8888667279875584\n",
      "step: 25 Loss: 2.0863877430183853\n",
      "engine: -2.52905580444466 -3.7935837066669897 -1.26452790222233\n",
      "manual: -2.52905580444466 -3.7935837066669897 -1.26452790222233\n",
      "step: 26 Loss: 0.3997577038747016\n",
      "engine: 2.867278072574983 2.867278072574983 2.867278072574983\n",
      "manual: 2.867278072574983 2.867278072574983 2.867278072574983\n",
      "step: 26 Loss: 2.0553208863673276\n",
      "engine: -2.5090669166181456 -3.7636003749272184 -1.2545334583090728\n",
      "manual: -2.5090669166181456 -3.7636003749272184 -1.2545334583090728\n",
      "step: 27 Loss: 0.3934635495042305\n",
      "engine: 2.8457854032175742 2.8457854032175742 2.8457854032175742\n",
      "manual: 2.8457854032175742 2.8457854032175742 2.8457854032175742\n",
      "step: 27 Loss: 2.0246236402915527\n",
      "engine: -2.4895166767372743 -3.7342750151059114 -1.2447583383686371\n",
      "manual: -2.4895166767372743 -3.7342750151059114 -1.2447583383686371\n",
      "step: 28 Loss: 0.38735583023456266\n",
      "engine: 2.8244092796287568 2.8244092796287568 2.8244092796287568\n",
      "manual: 2.8244092796287568 2.8244092796287568 2.8244092796287568\n",
      "step: 28 Loss: 1.994321944713258\n",
      "engine: -2.470310234361733 -3.7054653515425997 -1.2351551171808666\n",
      "manual: -2.470310234361733 -3.7054653515425997 -1.2351551171808666\n",
      "step: 29 Loss: 0.38140204087452007\n",
      "engine: 2.8031633369127356 2.8031633369127356 2.8031633369127356\n",
      "manual: 2.8031633369127356 2.8031633369127356 2.8031633369127356\n",
      "step: 29 Loss: 1.9644311733529356\n",
      "engine: -2.4513825695995095 -3.6770738543992643 -1.2256912847997548\n",
      "manual: -2.4513825695995095 -3.6770738543992643 -1.2256912847997548\n",
      "step: 30 Loss: 0.3755797814085184\n",
      "engine: 2.7820564908739414 2.7820564908739414 2.7820564908739414\n",
      "manual: 2.7820564908739414 2.7820564908739414 2.7820564908739414\n",
      "step: 30 Loss: 1.934959579603457\n",
      "engine: -2.4326890079213968 -3.649033511882095 -1.2163445039606984\n",
      "manual: -2.4326890079213968 -3.649033511882095 -1.2163445039606984\n",
      "step: 31 Loss: 0.36987348807884934\n",
      "engine: 2.7610944418967893 2.7610944418967893 2.7610944418967893\n",
      "manual: 2.7610944418967893 2.7610944418967893 2.7610944418967893\n",
      "step: 31 Loss: 1.9059106292683357\n",
      "engine: -2.414198751758633 -3.6212981276379494 -1.2070993758793165\n",
      "manual: -2.414198751758633 -3.6212981276379494 -1.2070993758793165\n",
      "step: 32 Loss: 0.36427222581205887\n",
      "engine: 2.7402807004884977 2.7402807004884977 2.7402807004884977\n",
      "manual: 2.7402807004884977 2.7402807004884977 2.7402807004884977\n",
      "step: 32 Loss: 1.8772845793674329\n",
      "engine: -2.3958904693834597 -3.5938357040751896 -1.1979452346917299\n",
      "manual: -2.3958904693834597 -3.5938357040751896 -1.1979452346917299\n",
      "step: 33 Loss: 0.35876819633015594\n",
      "engine: 2.7196172866221957 2.7196172866221957 2.7196172866221957\n",
      "manual: 2.7196172866221957 2.7196172866221957 2.7196172866221957\n",
      "step: 33 Loss: 1.8490795464235685\n",
      "engine: -2.377749286745413 -3.5666239301181193 -1.1888746433727064\n",
      "manual: -2.377749286745413 -3.5666239301181193 -1.1888746433727064\n",
      "step: 34 Loss: 0.35335572941364496\n",
      "engine: 2.6991052066295893 2.6991052066295893 2.6991052066295893\n",
      "manual: 2.6991052066295893 2.6991052066295893 2.6991052066295893\n",
      "step: 34 Loss: 1.8212922291137394\n",
      "engine: -2.3597647360478007 -3.539647104071701 -1.1798823680239003\n",
      "manual: -2.3597647360478007 -3.539647104071701 -1.1798823680239003\n",
      "step: 35 Loss: 0.34803060059342167\n",
      "engine: 2.678744778394684 2.678744778394684 2.678744778394684\n",
      "manual: 2.678744778394684 2.678744778394684 2.678744778394684\n",
      "step: 35 Loss: 1.7939183969441959\n",
      "engine: -2.3419293567691355 -3.5128940351537032 -1.1709646783845677\n",
      "manual: -2.3419293567691355 -3.5128940351537032 -1.1709646783845677\n",
      "step: 36 Loss: 0.34278956950606854\n",
      "engine: 2.658535853097149 2.658535853097149 2.658535853097149\n",
      "manual: 2.658535853097149 2.658535853097149 2.658535853097149\n",
      "step: 36 Loss: 1.7669532205507466\n",
      "engine: -2.3242377416170967 -3.486356612425645 -1.1621188708085484\n",
      "manual: -2.3242377416170967 -3.486356612425645 -1.1621188708085484\n",
      "step: 37 Loss: 0.3376300674723339\n",
      "engine: 2.638477966408347 2.638477966408347 2.638477966408347\n",
      "manual: 2.638477966408347 2.638477966408347 2.638477966408347\n",
      "step: 37 Loss: 1.7403914948055816\n",
      "engine: -2.3066858859023114 -3.460028828853467 -1.1533429429511557\n",
      "manual: -2.3066858859023114 -3.460028828853467 -1.1533429429511557\n",
      "step: 38 Loss: 0.3325499860138082\n",
      "engine: 2.618570441577985 2.618570441577985 2.618570441577985\n",
      "manual: 2.618570441577985 2.618570441577985 2.618570441577985\n",
      "step: 38 Loss: 1.7142277893764808\n",
      "engine: -2.289270743828382 -3.433906115742573 -1.144635371914191\n",
      "manual: -2.289270743828382 -3.433906115742573 -1.144635371914191\n",
      "step: 39 Loss: 0.32754753365928463\n",
      "engine: 2.5988124597130096 2.5988124597130096 2.5988124597130096\n",
      "manual: 2.5988124597130096 2.5988124597130096 2.5988124597130096\n",
      "step: 39 Loss: 1.6884565501898958\n",
      "engine: -2.271989925887553 -3.4079848888313293 -1.1359949629437764\n",
      "manual: -2.271989925887553 -3.4079848888313293 -1.1359949629437764\n",
      "step: 40 Loss: 0.322621138958408\n",
      "engine: 2.5792031076834796 2.5792031076834796 2.5792031076834796\n",
      "manual: 2.5792031076834796 2.5792031076834796 2.5792031076834796\n",
      "step: 40 Loss: 1.6630721676710296\n",
      "engine: -2.254841492483081 -3.3822622387246213 -1.1274207462415404\n",
      "manual: -2.254841492483081 -3.3822622387246213 -1.1274207462415404\n",
      "step: 41 Loss: 0.317769384763958\n",
      "engine: 2.5597414107714584 2.5597414107714584 2.5597414107714584\n",
      "manual: 2.5597414107714584 2.5597414107714584 2.5597414107714584\n",
      "step: 41 Loss: 1.638069022504564\n",
      "engine: -2.237823813172973 -3.3567357197594596 -1.1189119065864865\n",
      "manual: -2.237823813172973 -3.3567357197594596 -1.1189119065864865\n",
      "step: 42 Loss: 0.3129909636752516\n",
      "engine: 2.5404263549155477 2.5404263549155477 2.5404263549155477\n",
      "manual: 2.5404263549155477 2.5404263549155477 2.5404263549155477\n",
      "step: 42 Loss: 1.613441516187374\n",
      "engine: -2.2209354706642657 -3.3314032059963985 -1.1104677353321328\n",
      "manual: -2.2209354706642657 -3.3314032059963985 -1.1104677353321328\n",
      "step: 43 Loss: 0.30828464780341897\n",
      "engine: 2.521256901860472 2.521256901860472 2.521256901860472\n",
      "manual: 2.521256901860472 2.521256901860472 2.521256901860472\n",
      "step: 43 Loss: 1.5891840912947661\n",
      "engine: -2.204175195324787 -3.30626279298718 -1.1020875976623934\n",
      "manual: -2.204175195324787 -3.30626279298718 -1.1020875976623934\n",
      "step: 44 Loss: 0.30364926823031635\n",
      "engine: 2.5022319994683304 2.5022319994683304 2.5022319994683304\n",
      "manual: 2.5022319994683304 2.5022319994683304 2.5022319994683304\n",
      "step: 44 Loss: 1.5652912447908196\n",
      "engine: -2.1875418205062473 -3.281312730759371 -1.0937709102531237\n",
      "manual: -2.1875418205062473 -3.281312730759371 -1.0937709102531237\n",
      "step: 45 Loss: 0.2990837010289867\n",
      "engine: 2.4833505887306053 2.4833505887306053 2.4833505887306053\n",
      "manual: 2.4833505887306053 2.4833505887306053 2.4833505887306053\n",
      "step: 45 Loss: 1.541757536637161\n",
      "engine: -2.1710342520598402 -3.2565513780897604 -1.0855171260299201\n",
      "manual: -2.1710342520598402 -3.2565513780897604 -1.0855171260299201\n",
      "step: 46 Loss: 0.29458685772606436\n",
      "engine: 2.464611608530358 2.464611608530358 2.464611608530358\n",
      "manual: 2.464611608530358 2.464611608530358 2.464611608530358\n",
      "step: 46 Loss: 1.51857759522565\n",
      "engine: -2.1546514475303695 -3.2319771712955543 -1.0773257237651848\n",
      "manual: -2.1546514475303695 -3.2319771712955543 -1.0773257237651848\n",
      "step: 47 Loss: 0.2901576787715448\n",
      "engine: 2.4460139988703595 2.4460139988703595 2.4460139988703595\n",
      "manual: 2.4460139988703595 2.4460139988703595 2.4460139988703595\n",
      "step: 47 Loss: 1.4957461206674418\n",
      "engine: -2.1383924019507603 -3.2075886029261405 -1.0691962009753802\n",
      "manual: -2.1383924019507603 -3.2075886029261405 -1.0691962009753802\n",
      "step: 48 Loss: 0.2857951290450464\n",
      "engine: 2.4275567030551812 2.4275567030551812 2.4275567030551812\n",
      "manual: 2.4275567030551812 2.4275567030551812 2.4275567030551812\n",
      "step: 48 Loss: 1.4732578866370354\n",
      "engine: -2.1222561381377787 -3.183384207206668 -1.0611280690688893\n",
      "manual: -2.1222561381377787 -3.183384207206668 -1.0611280690688893\n",
      "step: 49 Loss: 0.2814981947414674\n",
      "engine: 2.409238669160139 2.409238669160139 2.409238669160139\n",
      "manual: 2.409238669160139 2.409238669160139 2.409238669160139\n",
      "step: 49 Loss: 1.4511077412441296\n",
      "engine: -2.1062417000576374 -3.159362550086456 -1.0531208500288187\n",
      "manual: -2.1062417000576374 -3.159362550086456 -1.0531208500288187\n",
      "step: 50 Loss: 0.2772658811913554\n",
      "engine: 2.3910588510139874 2.3910588510139874 2.3910588510139874\n",
      "manual: 2.3910588510139874 2.3910588510139874 2.3910588510139874\n",
      "step: 50 Loss: 1.4292906072530824\n",
      "engine: -2.0903481482848605 -3.1355222224272907 -1.0451740741424302\n",
      "manual: -2.0903481482848605 -3.1355222224272907 -1.0451740741424302\n",
      "step: 51 Loss: 0.2730972113148716\n",
      "engine: 2.37301620885024 2.37301620885024 2.37301620885024\n",
      "manual: 2.37301620885024 2.37301620885024 2.37301620885024\n",
      "step: 51 Loss: 1.4078014818664915\n",
      "engine: -2.074574556889168 -3.1118618353337517 -1.037287278444584\n",
      "manual: -2.074574556889168 -3.1118618353337517 -1.037287278444584\n",
      "step: 52 Loss: 0.2689912245057429\n",
      "engine: 2.3551097097325773 2.3551097097325773 2.3551097097325773\n",
      "manual: 2.3551097097325773 2.3551097097325773 2.3551097097325773\n",
      "step: 52 Loss: 1.3866354362191662\n",
      "engine: -2.058920011296003 -3.0883800169440043 -1.0294600056480014\n",
      "manual: -2.058920011296003 -3.0883800169440043 -1.0294600056480014\n",
      "step: 53 Loss: 0.2649469758071958\n",
      "engine: 2.3373383278263837 2.3373383278263837 2.3373383278263837\n",
      "manual: 2.3373383278263837 2.3373383278263837 2.3373383278263837\n",
      "step: 53 Loss: 1.3657876146815588\n",
      "engine: -2.043383606811453 -3.0650754102171796 -1.0216918034057265\n",
      "manual: -2.043383606811453 -3.0650754102171796 -1.0216918034057265\n",
      "step: 54 Loss: 0.2609635352866114\n",
      "engine: 2.3197010445654893 2.3197010445654893 2.3197010445654893\n",
      "manual: 2.3197010445654893 2.3197010445654893 2.3197010445654893\n",
      "step: 54 Loss: 1.3452532340395555\n",
      "engine: -2.027964447599956 -3.041946671399934 -1.013982223799978\n",
      "manual: -2.027964447599956 -3.041946671399934 -1.013982223799978\n",
      "step: 55 Loss: 0.2570399875455872\n",
      "engine: 2.302196848747558 2.302196848747558 2.302196848747558\n",
      "manual: 2.302196848747558 2.302196848747558 2.302196848747558\n",
      "step: 55 Loss: 1.3250275825957964\n",
      "engine: -2.0126616459713844 -3.0189924689570766 -1.0063308229856922\n",
      "manual: -2.0126616459713844 -3.0189924689570766 -1.0063308229856922\n",
      "step: 56 Loss: 0.25317543132276515\n",
      "engine: 2.284824736580987 2.284824736580987 2.284824736580987\n",
      "manual: 2.284824736580987 2.284824736580987 2.284824736580987\n",
      "step: 56 Loss: 1.3051060192230939\n",
      "engine: -1.9974743218788333 -2.99621148281825 -0.9987371609394167\n",
      "manual: -1.9974743218788333 -2.99621148281825 -0.9987371609394167\n",
      "step: 57 Loss: 0.24936897916033157\n",
      "engine: 2.267583711698858 2.267583711698858 2.267583711698858\n",
      "manual: 2.267583711698858 2.267583711698858 2.267583711698858\n",
      "step: 57 Loss: 1.2854839723904925\n",
      "engine: -1.9824016025604863 -2.9736024038407294 -0.9912008012802431\n",
      "manual: -1.9824016025604863 -2.9736024038407294 -0.9912008012802431\n",
      "step: 58 Loss: 0.245619757114649\n",
      "engine: 2.2504727851505564 2.2504727851505564 2.2504727851505564\n",
      "manual: 2.2504727851505564 2.2504727851505564 2.2504727851505564\n",
      "step: 58 Loss: 1.2661569391758256\n",
      "engine: -1.9674426222796768 -2.951163933419515 -0.9837213111398384\n",
      "manual: -1.9674426222796768 -2.951163933419515 -0.9837213111398384\n",
      "step: 59 Loss: 0.24192690449767068\n",
      "engine: 2.2334909753783023 2.2334909753783023 2.2334909753783023\n",
      "manual: 2.2334909753783023 2.2334909753783023 2.2334909753783023\n",
      "step: 59 Loss: 1.2471204842740802\n",
      "engine: -1.9525965221321684 -2.9288947831982526 -0.9762982610660842\n",
      "manual: -1.9525965221321684 -2.9288947831982526 -0.9762982610660842\n",
      "step: 60 Loss: 0.23828957364016498\n",
      "engine: 2.216637308183534 2.216637308183534 2.216637308183534\n",
      "manual: 2.216637308183534 2.216637308183534 2.216637308183534\n",
      "step: 60 Loss: 1.2283702390077862\n",
      "engine: -1.9378624498992139 -2.906793674848821 -0.9689312249496069\n",
      "manual: -1.9378624498992139 -2.906793674848821 -0.9689312249496069\n",
      "step: 61 Loss: 0.23470692967058646\n",
      "engine: 2.199910816686474 2.199910816686474 2.199910816686474\n",
      "manual: 2.199910816686474 2.199910816686474 2.199910816686474\n",
      "step: 61 Loss: 1.2099019003435374\n",
      "engine: -1.9232395599321848 -2.884859339898277 -0.9616197799660924\n",
      "manual: -1.9232395599321848 -2.884859339898277 -0.9616197799660924\n",
      "step: 62 Loss: 0.23117815030550898\n",
      "engine: 2.183310541281216 2.183310541281216 2.183310541281216\n",
      "manual: 2.183310541281216 2.183310541281216 2.183310541281216\n",
      "step: 62 Loss: 1.191711229917419\n",
      "engine: -1.908727013058666 -2.863090519587999 -0.954363506529333\n",
      "manual: -1.908727013058666 -2.863090519587999 -0.954363506529333\n",
      "step: 63 Loss: 0.22770242564874105\n",
      "engine: 2.166835529587864 2.166835529587864 2.166835529587864\n",
      "manual: 2.166835529587864 2.166835529587864 2.166835529587864\n",
      "step: 63 Loss: 1.1737940530710798\n",
      "engine: -1.8943239765033297 -2.8414859647549946 -0.9471619882516649\n",
      "manual: -1.8943239765033297 -2.8414859647549946 -0.9471619882516649\n",
      "step: 64 Loss: 0.22427895799721173\n",
      "engine: 2.150484836402791 2.150484836402791 2.150484836402791\n",
      "manual: 2.150484836402791 2.150484836402791 2.150484836402791\n",
      "step: 64 Loss: 1.1561462578995847\n",
      "engine: -1.8800296238190697 -2.8200444357286045 -0.9400148119095348\n",
      "manual: -1.8800296238190697 -2.8200444357286045 -0.9400148119095348\n",
      "step: 65 Loss: 0.22090696165232954\n",
      "engine: 2.1342575236477686 2.1342575236477686 2.1342575236477686\n",
      "manual: 2.1342575236477686 2.1342575236477686 2.1342575236477686\n",
      "step: 65 Loss: 1.1387637943117763\n",
      "engine: -1.865843134825198 -2.798764702237797 -0.932921567412599\n",
      "manual: -1.865843134825198 -2.798764702237797 -0.932921567412599\n",
      "step: 66 Loss: 0.21758566273589514\n",
      "engine: 2.1181526603184135 2.1181526603184135 2.1181526603184135\n",
      "manual: 2.1181526603184135 2.1181526603184135 2.1181526603184135\n",
      "step: 66 Loss: 1.1216426731034932\n",
      "engine: -1.8517636955505594 -2.777645543325839 -0.9258818477752797\n",
      "manual: -1.8517636955505594 -2.777645543325839 -0.9258818477752797\n",
      "step: 67 Loss: 0.21431429900994156\n",
      "engine: 2.102169322432342 2.102169322432342 2.102169322432342\n",
      "manual: 2.102169322432342 2.102169322432342 2.102169322432342\n",
      "step: 67 Loss: 1.104778965043913\n",
      "engine: -1.8377904981801692 -2.7566857472702537 -0.9188952490900846\n",
      "manual: -1.8377904981801692 -2.7566857472702537 -0.9188952490900846\n",
      "step: 68 Loss: 0.21109211970008215\n",
      "engine: 2.086306592977211 2.086306592977211 2.086306592977211\n",
      "manual: 2.086306592977211 2.086306592977211 2.086306592977211\n",
      "step: 68 Loss: 1.0881687999750445\n",
      "engine: -1.8239227410042602 -2.7358841115063903 -0.9119613705021301\n",
      "manual: -1.8239227410042602 -2.7358841115063903 -0.9119613705021301\n",
      "step: 69 Loss: 0.20791838532203086\n",
      "engine: 2.070563561858833 2.070563561858833 2.070563561858833\n",
      "manual: 2.070563561858833 2.070563561858833 2.070563561858833\n",
      "step: 69 Loss: 1.0718083659243844\n",
      "engine: -1.8101596283691848 -2.715239442553777 -0.9050798141845924\n",
      "manual: -1.8101596283691848 -2.715239442553777 -0.9050798141845924\n",
      "step: 70 Loss: 0.20479236751110408\n",
      "engine: 2.054939325849455 2.054939325849455 2.054939325849455\n",
      "manual: 2.054939325849455 2.054939325849455 2.054939325849455\n",
      "step: 70 Loss: 1.0556939082306531\n",
      "engine: -1.7965003706296727 -2.694750555944509 -0.8982501853148364\n",
      "manual: -1.7965003706296727 -2.694750555944509 -0.8982501853148364\n",
      "step: 71 Loss: 0.20171334885453446\n",
      "engine: 2.0394329885362676 2.0394329885362676 2.0394329885362676\n",
      "manual: 2.0394329885362676 2.0394329885362676 2.0394329885362676\n",
      "step: 71 Loss: 1.039821728682493\n",
      "engine: -1.782944184102071 -2.6744162761531065 -0.8914720920510355\n",
      "manual: -1.782944184102071 -2.6744162761531065 -0.8914720920510355\n",
      "step: 72 Loss: 0.19868062272646247\n",
      "engine: 2.024043660270216 2.024043660270216 2.024043660270216\n",
      "manual: 2.024043660270216 2.024043660270216 2.024043660270216\n",
      "step: 72 Loss: 1.0241881846700134\n",
      "engine: -1.769490291018343 -2.6542354365275145 -0.8847451455091715\n",
      "manual: -1.769490291018343 -2.6542354365275145 -0.8847451455091715\n",
      "step: 73 Loss: 0.19569349312551126\n",
      "engine: 2.0087704581151034 2.0087704581151034 2.0087704581151034\n",
      "manual: 2.0087704581151034 2.0087704581151034 2.0087704581151034\n",
      "step: 73 Loss: 1.0087896883489906\n",
      "engine: -1.7561379194808282 -2.6342068792212423 -0.8780689597404141\n",
      "manual: -1.7561379194808282 -2.6342068792212423 -0.8780689597404141\n",
      "step: 74 Loss: 0.19275127451490323\n",
      "engine: 1.9936125057970475 1.9936125057970475 1.9936125057970475\n",
      "manual: 1.9936125057970475 1.9936125057970475 1.9936125057970475\n",
      "step: 74 Loss: 0.9936227058175957\n",
      "engine: -1.7428863034174924 -2.6143294551262386 -0.8714431517087462\n",
      "manual: -1.7428863034174924 -2.6143294551262386 -0.8714431517087462\n",
      "step: 75 Loss: 0.18985329166501821\n",
      "engine: 1.978568933654273 1.978568933654273 1.978568933654273\n",
      "manual: 1.978568933654273 1.978568933654273 1.978568933654273\n",
      "step: 75 Loss: 0.9786837563054519\n",
      "engine: -1.729734682537618 -2.594602023806427 -0.864867341268809\n",
      "manual: -1.729734682537618 -2.594602023806427 -0.864867341268809\n",
      "step: 76 Loss: 0.18699887949834465\n",
      "engine: 1.9636388785872736 1.9636388785872736 1.9636388785872736\n",
      "manual: 1.9636388785872736 1.9636388785872736 1.9636388785872736\n",
      "step: 76 Loss: 0.9639694113748714\n",
      "engine: -1.716682302288035 -2.5750234534320526 -0.8583411511440175\n",
      "manual: -1.716682302288035 -2.5750234534320526 -0.8583411511440175\n",
      "step: 77 Loss: 0.18418738293680928\n",
      "engine: 1.9488214840093203 1.9488214840093203 1.9488214840093203\n",
      "manual: 1.9488214840093203 1.9488214840093203 1.9488214840093203\n",
      "step: 77 Loss: 0.9494762941340724\n",
      "engine: -1.7037284138096211 -2.5555926207144317 -0.8518642069048106\n",
      "manual: -1.7037284138096211 -2.5555926207144317 -0.8518642069048106\n",
      "step: 78 Loss: 0.18141815675139047\n",
      "engine: 1.934115899797339 1.934115899797339 1.934115899797339\n",
      "manual: 1.934115899797339 1.934115899797339 1.934115899797339\n",
      "step: 78 Loss: 0.9352010784622177\n",
      "engine: -1.6908722738942856 -2.5363084108414284 -0.8454361369471428\n",
      "manual: -1.6908722738942856 -2.5363084108414284 -0.8454361369471428\n",
      "step: 79 Loss: 0.178690565414027\n",
      "engine: 1.9195212822431564 1.9195212822431564 1.9195212822431564\n",
      "manual: 1.9195212822431564 1.9195212822431564 1.9195212822431564\n",
      "step: 79 Loss: 0.9211404882461028\n",
      "engine: -1.6781131449422375 -2.5171697174133563 -0.8390565724711188\n",
      "manual: -1.6781131449422375 -2.5171697174133563 -0.8390565724711188\n",
      "step: 80 Loss: 0.17600398295174544\n",
      "engine: 1.9050367940051007 1.9050367940051007 1.9050367940051007\n",
      "manual: 1.9050367940051007 1.9050367940051007 1.9050367940051007\n",
      "step: 80 Loss: 0.9072912966283081\n",
      "engine: -1.6654502949196441 -2.498175442379466 -0.8327251474598221\n",
      "manual: -1.6654502949196441 -2.498175442379466 -0.8327251474598221\n",
      "step: 81 Loss: 0.1733577928029956\n",
      "engine: 1.8906616040599733 1.8906616040599733 1.8906616040599733\n",
      "manual: 1.8906616040599733 1.8906616040599733 1.8906616040599733\n",
      "step: 81 Loss: 0.8936503252666578\n",
      "engine: -1.6528829973165315 -2.4793244959747973 -0.8264414986582658\n",
      "manual: -1.6528829973165315 -2.4793244959747973 -0.8264414986582658\n",
      "step: 82 Loss: 0.17075138767613007\n",
      "engine: 1.8763948876553656 1.8763948876553656 1.8763948876553656\n",
      "manual: 1.8763948876553656 1.8763948876553656 1.8763948876553656\n",
      "step: 82 Loss: 0.880214443604798\n",
      "engine: -1.6404105311051893 -2.460615796657784 -0.8202052655525947\n",
      "manual: -1.6404105311051893 -2.460615796657784 -0.8202052655525947\n",
      "step: 83 Loss: 0.16818416941005057\n",
      "engine: 1.8622358262623564 1.8622358262623564 1.8622358262623564\n",
      "manual: 1.8622358262623564 1.8622358262623564 1.8622358262623564\n",
      "step: 83 Loss: 0.8669805681537603\n",
      "engine: -1.6280321806987033 -2.442048271048055 -0.8140160903493516\n",
      "manual: -1.6280321806987033 -2.442048271048055 -0.8140160903493516\n",
      "step: 84 Loss: 0.16565554883691094\n",
      "engine: 1.8481836075285365 1.8481836075285365 1.8481836075285365\n",
      "manual: 1.8481836075285365 1.8481836075285365 1.8481836075285365\n",
      "step: 84 Loss: 0.8539456617842989\n",
      "engine: -1.6157472359099145 -2.423620853864872 -0.8078736179549573\n",
      "manual: -1.6157472359099145 -2.423620853864872 -0.8078736179549573\n",
      "step: 85 Loss: 0.16316494564690806\n",
      "engine: 1.8342374252314197 1.8342374252314197 1.8342374252314197\n",
      "manual: 1.8342374252314197 1.8342374252314197 1.8342374252314197\n",
      "step: 85 Loss: 0.841106733029897\n",
      "engine: -1.603554991910677 -2.4053324878660156 -0.8017774959553385\n",
      "manual: -1.603554991910677 -2.4053324878660156 -0.8017774959553385\n",
      "step: 86 Loss: 0.16071178825510324\n",
      "engine: 1.8203964792321763 1.8203964792321763 1.8203964792321763\n",
      "manual: 1.8203964792321763 1.8203964792321763 1.8203964792321763\n",
      "step: 86 Loss: 0.8284608354002259\n",
      "engine: -1.5914547491914064 -2.3871821237871096 -0.7957273745957032\n",
      "manual: -1.5914547491914064 -2.3871821237871096 -0.7957273745957032\n",
      "step: 87 Loss: 0.15829551367024264\n",
      "engine: 1.8066599754297297 1.8066599754297297 1.8066599754297297\n",
      "manual: 1.8066599754297297 1.8066599754297297 1.8066599754297297\n",
      "step: 87 Loss: 0.8160050667049379\n",
      "engine: -1.5794458135209553 -2.369168720281433 -0.7897229067604776\n",
      "manual: -1.5794458135209553 -2.369168720281433 -0.7897229067604776\n",
      "step: 88 Loss: 0.1559155673655545\n",
      "engine: 1.7930271257152022 1.7930271257152022 1.7930271257152022\n",
      "manual: 1.7930271257152022 1.7930271257152022 1.7930271257152022\n",
      "step: 88 Loss: 0.8037365683876299\n",
      "engine: -1.567527495906731 -2.3512912438600964 -0.7837637479533655\n",
      "manual: -1.567527495906731 -2.3512912438600964 -0.7837637479533655\n",
      "step: 89 Loss: 0.15357140315147663\n",
      "engine: 1.7794971479266941 1.7794971479266941 1.7794971479266941\n",
      "manual: 1.7794971479266941 1.7794971479266941 1.7794971479266941\n",
      "step: 89 Loss: 0.7916525248698096\n",
      "engine: -1.555699112555267 -2.3335486688329006 -0.7778495562776335\n",
      "manual: -1.555699112555267 -2.3335486688329006 -0.7778495562776335\n",
      "step: 90 Loss: 0.15126248305032783\n",
      "engine: 1.7660692658044077 1.7660692658044077 1.7660692658044077\n",
      "manual: 1.7660692658044077 1.7660692658044077 1.7660692658044077\n",
      "step: 90 Loss: 0.7797501629047299\n",
      "engine: -1.5439599848328385 -2.3159399772492577 -0.7719799924164192\n",
      "manual: -1.5439599848328385 -2.3159399772492577 -0.7719799924164192\n",
      "step: 91 Loss: 0.14898827717281368\n",
      "engine: 1.7527427089461138 1.7527427089461138 1.7527427089461138\n",
      "manual: 1.7527427089461138 1.7527427089461138 1.7527427089461138\n",
      "step: 91 Loss: 0.7680267509409404\n",
      "engine: -1.532309439226708 -2.298464158840062 -0.766154719613354\n",
      "manual: -1.532309439226708 -2.298464158840062 -0.766154719613354\n",
      "step: 92 Loss: 0.14674826359645426\n",
      "engine: 1.7395167127629492 1.7395167127629492 1.7395167127629492\n",
      "manual: 1.7395167127629492 1.7395167127629492 1.7395167127629492\n",
      "step: 92 Loss: 0.7564795984954041\n",
      "engine: -1.5207468073063453 -2.281120210959518 -0.7603734036531726\n",
      "manual: -1.5207468073063453 -2.281120210959518 -0.7603734036531726\n",
      "step: 93 Loss: 0.14454192824577766\n",
      "engine: 1.7263905184355526 1.7263905184355526 1.7263905184355526\n",
      "manual: 1.7263905184355526 1.7263905184355526 1.7263905184355526\n",
      "step: 93 Loss: 0.745106055536044\n",
      "engine: -1.5092714256850925 -2.263907138527639 -0.7546357128425463\n",
      "manual: -1.5092714256850925 -2.263907138527639 -0.7546357128425463\n",
      "step: 94 Loss: 0.1423687647743445\n",
      "engine: 1.7133633728705249 1.7133633728705249 1.7133633728705249\n",
      "manual: 1.7133633728705249 1.7133633728705249 1.7133633728705249\n",
      "step: 94 Loss: 0.7339035118735653\n",
      "engine: -1.497882635982208 -2.246823953973312 -0.748941317991104\n",
      "manual: -1.497882635982208 -2.246823953973312 -0.748941317991104\n",
      "step: 95 Loss: 0.14022827444856298\n",
      "engine: 1.700434528657226 1.700434528657226 1.700434528657226\n",
      "manual: 1.700434528657226 1.700434528657226 1.700434528657226\n",
      "step: 95 Loss: 0.7228693965624305\n",
      "engine: -1.4865797847849223 -2.2298696771773834 -0.7432898923924611\n",
      "manual: -1.4865797847849223 -2.2298696771773834 -0.7432898923924611\n",
      "step: 96 Loss: 0.13811996603319912\n",
      "engine: 1.6876032440248885 1.6876032440248885 1.6876032440248885\n",
      "manual: 1.6876032440248885 1.6876032440248885 1.6876032440248885\n",
      "step: 96 Loss: 0.7120011773108318\n",
      "engine: -1.4753622236111141 -2.213043335416671 -0.7376811118055571\n",
      "manual: -1.4753622236111141 -2.213043335416671 -0.7376811118055571\n",
      "step: 97 Loss: 0.1360433556786707\n",
      "engine: 1.6748687828000612 1.6748687828000612 1.6748687828000612\n",
      "manual: 1.6748687828000612 1.6748687828000612 1.6748687828000612\n",
      "step: 97 Loss: 0.7012963598995396\n",
      "engine: -1.4642293088720209 -2.1963439633080313 -0.7321146544360104\n",
      "manual: -1.4642293088720209 -2.1963439633080313 -0.7321146544360104\n",
      "step: 98 Loss: 0.13399796680998974\n",
      "engine: 1.6622304143643785 1.6622304143643785 1.6622304143643785\n",
      "manual: 1.6622304143643785 1.6622304143643785 1.6622304143643785\n",
      "step: 98 Loss: 0.6907524876094934\n",
      "engine: -1.4531804018353114 -2.179770602752967 -0.7265902009176557\n",
      "manual: -1.4531804018353114 -2.179770602752967 -0.7265902009176557\n",
      "step: 99 Loss: 0.13198333001738982\n",
      "engine: 1.649687413612634 1.649687413612634 1.649687413612634\n",
      "manual: 1.649687413612634 1.649687413612634 1.649687413612634\n",
      "step: 99 Loss: 0.6803671406579854\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "model = neuron(2)\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for w in model.w:\n",
    "            zero_grad(w)\n",
    "        zero_grad(model.b)\n",
    "\n",
    "        y = model.pred(x)\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "\n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", model.w[0].grad, model.w[1].grad, model.b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        for w in model.w:\n",
    "            w.value -= n * w.grad\n",
    "        model.b.value -= n* model.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192012b",
   "metadata": {},
   "source": [
    "## Phase 3 — Adding Depth (2 → 3 → 1)\n",
    "\n",
    "Now we build a multi-layer network:\n",
    "\n",
    "Input (2)\n",
    "  ↓\n",
    "Linear (2 → 3)\n",
    "  ↓\n",
    "ReLU\n",
    "  ↓\n",
    "Linear (3 → 1)\n",
    "  ↓\n",
    "Loss\n",
    "\n",
    "This is where real backpropagation complexity begins.\n",
    "\n",
    "We now test:\n",
    "\n",
    "- Gradient flow through depth\n",
    "- Nonlinear gating via ReLU\n",
    "- Reverse topological traversal correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9daae",
   "metadata": {},
   "source": [
    "### Layer Abstraction\n",
    "\n",
    "A layer is simply a collection of neurons sharing the same input.\n",
    "\n",
    "Each neuron corresponds to one row of a weight matrix.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "y = W·x + b\n",
    "\n",
    "Even though everything is scalar-based,\n",
    "this reproduces matrix multiplication behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f700aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class for the different layers\n",
    "class layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.neurons = [neuron(dim_in) for _ in range(dim_out)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        yout = []\n",
    "        if len(x) != self.dim_in:\n",
    "            raise ValueError(f\"Layer expected input dimension {self.dim_in}, got {len(x)}\")\n",
    "        else:\n",
    "            for neuron in self.neurons:\n",
    "                yout.append(neuron.pred(x))\n",
    "        return yout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5a0c3",
   "metadata": {},
   "source": [
    "### ReLU — First Nonlinearity\n",
    "\n",
    "ReLU(z) = max(0, z)\n",
    "\n",
    "Backward rule:\n",
    "\n",
    "If z > 0:\n",
    "    dL/dz = dL/da\n",
    "Else:\n",
    "    dL/dz = 0\n",
    "\n",
    "This introduces gradient gating.\n",
    "\n",
    "If a neuron’s pre-activation is negative,\n",
    "it receives zero gradient and does not learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3789416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(node):\n",
    "    out = Node(\n",
    "        value = max(0, node.value),\n",
    "        parents = (node,),\n",
    "    )\n",
    "    def backward():\n",
    "        if node.value > 0:\n",
    "            node.grad += out.grad\n",
    "        else:\n",
    "            node.grad += 0\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13776fb",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e5b2b",
   "metadata": {},
   "source": [
    "### Full Forward Pass\n",
    "\n",
    "For each example:\n",
    "\n",
    "1. h = hidden.forward(x)\n",
    "2. a = relu(h)  (element-wise)\n",
    "3. y = output.forward(a)\n",
    "4. L = (y − y_true)²\n",
    "\n",
    "Backward then computes gradients for all 13 parameters\n",
    "in a single reverse traversal.\n",
    "\n",
    "This is where reverse-mode autodiff becomes powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d49dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layer(2,3)\n",
    "output = layer(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d81b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d511dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.562636464761438, -6.8439546971421565] -2.281318232380719\n",
      "[0, 0] 0\n",
      "[-1.0357360622975873, -1.5536040934463808] -0.5178680311487937\n",
      "step: 0 Loss: 255.81379330662722\n",
      "[-0.8720959475727935, -0.8720959475727935] -0.8720959475727935\n",
      "[0, 0] 0\n",
      "[-0.30947695556799587, -0.30947695556799587] -0.30947695556799587\n",
      "step: 0 Loss: 21.721297479025967\n",
      "[-6.908795374694774, -10.36319306204216] -3.454397687347387\n",
      "[0, 0] 0\n",
      "[-2.3766031142772324, -3.5649046714158485] -1.1883015571386162\n",
      "step: 1 Loss: 241.25295028456873\n",
      "[-2.0656818541192656, -2.0656818541192656] -2.0656818541192656\n",
      "[0, 0] 0\n",
      "[-0.6911130146852104, -0.6911130146852104] -0.6911130146852104\n",
      "step: 1 Loss: 17.306642671357906\n",
      "[-16.850553630186035, -25.275830445279052] -8.425276815093017\n",
      "[0, 0] 0\n",
      "[-5.604826273109139, -8.407239409663708] -2.8024131365545695\n",
      "step: 2 Loss: 220.89659821917667\n",
      "[-3.8593517655878804, -3.8593517655878804] -3.8593517655878804\n",
      "[0, 0] 0\n",
      "[-1.2899200979686873, -1.2899200979686873] -1.2899200979686873\n",
      "step: 2 Loss: 10.511067190336222\n",
      "[-34.3631366089782, -51.5447049134673] -17.1815683044891\n",
      "[0, 0] 0\n",
      "[-11.460629483987532, -17.190944225981298] -5.730314741993766\n",
      "step: 3 Loss: 169.7331372331232\n",
      "[-1.261939199702267, -1.261939199702267] -1.261939199702267\n",
      "[0, 0] 0\n",
      "[-0.42122983068646475, -0.42122983068646475] -0.42122983068646475\n",
      "step: 3 Loss: 0.23544472255014692\n",
      "[-38.60721563103257, -57.91082344654885] -19.303607815516283\n",
      "[0, 0] 0\n",
      "[-12.884907946504216, -19.327361919756324] -6.442453973252108\n",
      "step: 4 Loss: 53.37610609347511\n",
      "[16.951543805261075, 16.951543805261075] 16.951543805261075\n",
      "[0, 0] 0\n",
      "[5.657822801576718, 5.657822801576718] 5.657822801576718\n",
      "step: 4 Loss: 17.21040649309516\n",
      "[-10.502279758913186, -15.75341963836978] -5.251139879456593\n",
      "[0, 0] 0\n",
      "[-3.5087676953840217, -5.263151543076033] -1.7543838476920108\n",
      "step: 5 Loss: 2.20902851416815\n",
      "[13.270394926266565, 13.270394926266565] 13.270394926266565\n",
      "[0, 0] 0\n",
      "[4.433168927037116, 4.433168927037116] 4.433168927037116\n",
      "step: 5 Loss: 11.419317310355336\n",
      "[-12.286899827072487, -18.43034974060873] -6.1434499135362435\n",
      "[0, 0] 0\n",
      "[-4.1083604655725905, -6.162540698358885] -2.0541802327862952\n",
      "step: 6 Loss: 3.0762835999401577\n",
      "[13.259220178131757, 13.259220178131757] 13.259220178131757\n",
      "[0, 0] 0\n",
      "[4.432574804821683, 4.432574804821683] 4.432574804821683\n",
      "step: 6 Loss: 11.192486675924584\n",
      "[-10.608265213195768, -15.912397819793652] -5.304132606597884\n",
      "[0, 0] 0\n",
      "[-3.5497655835696547, -5.3246483753544815] -1.7748827917848273\n",
      "step: 7 Loss: 2.236043264314641\n",
      "[12.228648897003888, 12.228648897003888] 12.228648897003888\n",
      "[0, 0] 0\n",
      "[4.090970092451105, 4.090970092451105] 4.090970092451105\n",
      "step: 7 Loss: 9.615163439832147\n",
      "[-10.222931294318029, -15.334396941477042] -5.111465647159014\n",
      "[0, 0] 0\n",
      "[-3.4232250212252073, -5.134837531837811] -1.7116125106126037\n",
      "step: 8 Loss: 2.052536553241534\n",
      "[11.62195697643191, 11.62195697643191] 11.62195697643191\n",
      "[0, 0] 0\n",
      "[3.890525576847152, 3.890525576847152] 3.890525576847152\n",
      "step: 8 Loss: 8.664648594833013\n",
      "[-9.442559997770651, -14.163839996655977] -4.721279998885326\n",
      "[0, 0] 0\n",
      "[-3.1639500541654866, -4.74592508124823] -1.5819750270827433\n",
      "step: 9 Loss: 1.7213612642837048\n",
      "[10.92490462657922, 10.92490462657922] 10.92490462657922\n",
      "[0, 0] 0\n",
      "[3.6594353124719494, 3.6594353124719494] 3.6594353124719494\n",
      "step: 9 Loss: 7.657848471108502\n",
      "[-8.893440100311041, -13.340160150466563] -4.446720050155521\n",
      "[0, 0] 0\n",
      "[-2.981737439047864, -4.472606158571796] -1.490868719523932\n",
      "step: 10 Loss: 1.5036336881873287\n",
      "[10.323102541213698, 10.323102541213698] 10.323102541213698\n",
      "[0, 0] 0\n",
      "[3.459837811448288, 3.459837811448288] 3.459837811448288\n",
      "step: 10 Loss: 6.819262493384128\n",
      "[-8.31085519202368, -12.466282788035521] -4.15542759601184\n",
      "[0, 0] 0\n",
      "[-2.787953569692109, -4.181930354538164] -1.3939767848460545\n",
      "step: 11 Loss: 1.29181792448914\n",
      "[9.734638075276772, 9.734638075276772] 9.734638075276772\n",
      "[0, 0] 0\n",
      "[3.264373081728068, 3.264373081728068] 3.264373081728068\n",
      "step: 11 Loss: 6.045641975718936\n",
      "[-7.801994930032893, -11.702992395049339] -3.9009974650164465\n",
      "[0, 0] 0\n",
      "[-2.6185963835551695, -3.927894575332754] -1.3092981917775848\n",
      "step: 12 Loss: 1.1204507627674918\n",
      "[9.189179387416488, 9.189179387416488] 9.189179387416488\n",
      "[0, 0] 0\n",
      "[3.0830196233255647, 3.0830196233255647] 3.0830196233255647\n",
      "step: 12 Loss: 5.36441887338505\n",
      "[-7.3109178827549295, -10.966376824132395] -3.6554589413774647\n",
      "[0, 0] 0\n",
      "[-2.4549451723289915, -3.682417758493487] -1.2274725861644957\n",
      "step: 13 Loss: 0.9682142887468939\n",
      "[8.668361096757243, 8.668361096757243] 8.668361096757243\n",
      "[0, 0] 0\n",
      "[2.9096632944062053, 2.9096632944062053] 2.9096632944062053\n",
      "step: 13 Loss: 4.750667602319705\n",
      "[-6.859199590696603, -10.288799386044904] -3.4295997953483015\n",
      "[0, 0] 0\n",
      "[-2.3042824409057814, -3.456423661358672] -1.1521412204528907\n",
      "step: 14 Loss: 0.8389571989101963\n",
      "[8.176946441352172, 8.176946441352172] 8.176946441352172\n",
      "[0, 0] 0\n",
      "[2.74593428799, 2.74593428799] 2.74593428799\n",
      "step: 14 Loss: 4.204172926559007\n",
      "[-6.431135646536443, -9.646703469804665] -3.2155678232682217\n",
      "[0, 0] 0\n",
      "[-2.161367915595232, -3.242051873392848] -1.080683957797616\n",
      "step: 15 Loss: 0.7261529416531552\n",
      "[7.709070675304103, 7.709070675304103] 7.709070675304103\n",
      "[0, 0] 0\n",
      "[2.5898954263461587, 2.5898954263461587] 2.5898954263461587\n",
      "step: 15 Loss: 3.7147285284649\n",
      "[-6.0310204566178856, -9.046530684926829] -3.0155102283089428\n",
      "[0, 0] 0\n",
      "[-2.0276735427353225, -3.041510314102984] -1.0138367713676613\n",
      "step: 16 Loss: 0.6289873480575656\n",
      "[7.264883376777119, 7.264883376777119] 7.264883376777119\n",
      "[0, 0] 0\n",
      "[2.4416230021105165, 2.4416230021105165] 2.4416230021105165\n",
      "step: 16 Loss: 3.278179691873803\n",
      "[-5.653012942971028, -8.479519414456542] -2.826506471485514\n",
      "[0, 0] 0\n",
      "[-1.901260884236038, -2.851891326354057] -0.950630442118019\n",
      "step: 17 Loss: 0.5444797397220289\n",
      "[6.84180353014447, 6.84180353014447] 6.84180353014447\n",
      "[0, 0] 0\n",
      "[2.300273899802531, 2.300273899802531] 2.300273899802531\n",
      "step: 17 Loss: 2.888368601819271\n",
      "[-5.297464580285366, -7.946196870428049] -2.648732290142683\n",
      "[0, 0] 0\n",
      "[-1.7822697226590516, -2.673404583988577] -0.8911348613295258\n",
      "step: 18 Loss: 0.4713022842355377\n",
      "[6.439170708831308, 6.439170708831308] 6.439170708831308\n",
      "[0, 0] 0\n",
      "[2.1656470393352834, 2.1656470393352834] 2.1656470393352834\n",
      "step: 18 Loss: 2.5410883781720344\n",
      "[-4.961576211658456, -7.442364317487684] -2.480788105829228\n",
      "[0, 0] 0\n",
      "[-1.6697772151208785, -2.504665822681318] -0.8348886075604393\n",
      "step: 19 Loss: 0.40769427192980323\n",
      "[6.05555723970808, 6.05555723970808] 6.05555723970808\n",
      "[0, 0] 0\n",
      "[2.0372813465953965, 2.0372813465953965] 2.0372813465953965\n",
      "step: 19 Loss: 2.231859472544454\n",
      "[-4.644798035525233, -6.967197053287849] -2.3223990177626166\n",
      "[0, 0] 0\n",
      "[-1.5636147146312198, -2.34542207194683] -0.7818073573156099\n",
      "step: 20 Loss: 0.35250559213135735\n",
      "[5.690270761353462, 5.690270761353462] 5.690270761353462\n",
      "[0, 0] 0\n",
      "[1.914960753071974, 1.914960753071974] 1.914960753071974\n",
      "step: 20 Loss: 1.9570399810304986\n",
      "[-4.3454968506379235, -6.518245275956885] -2.1727484253189617\n",
      "[0, 0] 0\n",
      "[-1.4632467991795106, -2.194870198769266] -0.7316233995897553\n",
      "step: 21 Loss: 0.3045519161816424\n",
      "[5.34239649579876, 5.34239649579876] 5.34239649579876\n",
      "[0, 0] 0\n",
      "[1.7983926536304609, 1.7983926536304609] 1.7983926536304609\n",
      "step: 21 Loss: 1.7131094282966215\n",
      "[-4.062977102622585, -6.094465653933877] -2.0314885513112926\n",
      "[0, 0] 0\n",
      "[-1.3684518350203851, -2.0526777525305775] -0.6842259175101926\n",
      "step: 22 Loss: 0.2629316122191244\n",
      "[5.011343064312024, 5.011343064312024] 5.011343064312024\n",
      "[0, 0] 0\n",
      "[1.6873915849447239, 1.6873915849447239] 1.6873915849447239\n",
      "step: 22 Loss: 1.4970112039194055\n",
      "[-3.7961410750590265, -5.69421161258854] -1.8980705375295133\n",
      "[0, 0] 0\n",
      "[-1.2788711654446674, -1.918306748167001] -0.6394355827223337\n",
      "step: 23 Loss: 0.22679646945432524\n",
      "[4.69645419314459, 4.69645419314459] 4.69645419314459\n",
      "[0, 0] 0\n",
      "[1.5817488430042166, 1.5817488430042166] 1.5817488430042166\n",
      "step: 23 Loss: 1.3058869541541345\n",
      "[-3.5443282766471724, -5.316492414970758] -1.7721641383235862\n",
      "[0, 0] 0\n",
      "[-1.1942920063693367, -1.791438009554005] -0.5971460031846684\n",
      "step: 24 Loss: 0.1954549595912936\n",
      "[4.39722695729077, 4.39722695729077] 4.39722695729077\n",
      "[0, 0] 0\n",
      "[1.4813060388876427, 1.4813060388876427] 1.4813060388876427\n",
      "step: 24 Loss: 1.1371878051346203\n",
      "[-3.3067188098799685, -4.960078214819953] -1.6533594049399842\n",
      "[0, 0] 0\n",
      "[-1.114446656610125, -1.6716699849151875] -0.5572233283050625\n",
      "step: 25 Loss: 0.1682795548819824\n",
      "[4.11313607115273, 4.11313607115273] 4.11313607115273\n",
      "[0, 0] 0\n",
      "[1.385896108818099, 1.385896108818099] 1.385896108818099\n",
      "step: 25 Loss: 0.9885619664585552\n",
      "[-3.0827144963713344, -4.624071744557002] -1.5413572481856672\n",
      "[0, 0] 0\n",
      "[-1.0391410622324395, -1.5587115933486593] -0.5195705311162198\n",
      "step: 26 Loss: 0.14474123646769949\n",
      "[3.843728871433031, 3.843728871433031] 3.843728871433031\n",
      "[0, 0] 0\n",
      "[1.2953753904245677, 1.2953753904245677] 1.2953753904245677\n",
      "step: 26 Loss: 0.857890138739533\n",
      "[-2.871649219143618, -4.307473828715427] -1.435824609571809\n",
      "[0, 0] 0\n",
      "[-0.9681573345679235, -1.4522360018518854] -0.48407866728396176\n",
      "step: 27 Loss: 0.12436733568125746\n",
      "[3.5885386969120256, 3.5885386969120256] 3.5885386969120256\n",
      "[0, 0] 0\n",
      "[1.2095945599666038, 1.2095945599666038] 1.2095945599666038\n",
      "step: 27 Loss: 0.7432313712990624\n",
      "[-2.67297618625153, -4.009464279377295] -1.336488093125765\n",
      "[0, 0] 0\n",
      "[-0.9013169813950813, -1.351975472092622] -0.45065849069754066\n",
      "step: 28 Loss: 0.10675244136949835\n",
      "[3.347130095565141, 3.347130095565141] 3.347130095565141\n",
      "[0, 0] 0\n",
      "[1.1284139404291191, 1.1284139404291191] 1.1284139404291191\n",
      "step: 28 Loss: 0.6428318183871136\n",
      "[-2.486116994508329, -3.7291754917624935] -1.2430584972541645\n",
      "[0, 0] 0\n",
      "[-0.8384301203358047, -1.257645180503707] -0.41921506016790233\n",
      "step: 29 Loss: 0.09153703846876395\n",
      "[3.1190537039534583, 3.1190537039534583] 3.1190537039534583\n",
      "[0, 0] 0\n",
      "[1.0516883824552095, 1.0516883824552095] 1.0516883824552095\n",
      "step: 29 Loss: 0.5550948690206037\n",
      "[-2.3105606092230304, -3.465840913834546] -1.1552803046115152\n",
      "[0, 0] 0\n",
      "[-0.7793289369375624, -1.1689934054063436] -0.3896644684687812\n",
      "step: 30 Loss: 0.07841001387044826\n",
      "[2.903870294395692, 2.903870294395692] 2.903870294395692\n",
      "[0, 0] 0\n",
      "[0.9792754357642453, 0.9792754357642453] 0.9792754357642453\n",
      "step: 30 Loss: 0.4785799267588911\n",
      "[-2.1457802228219403, -3.21867033423291] -1.0728901114109701\n",
      "[0, 0] 0\n",
      "[-0.7238397214307245, -1.0857595821460868] -0.36191986071536225\n",
      "step: 31 Loss: 0.0670967246880716\n",
      "[2.7011268000699915, 2.7011268000699915] 2.7011268000699915\n",
      "[0, 0] 0\n",
      "[0.9110273358637635, 0.9110273358637635] 0.9110273358637635\n",
      "step: 31 Loss: 0.4119834168313201\n",
      "[-1.99128934369586, -2.98693401554379] -0.99564467184793\n",
      "[0, 0] 0\n",
      "[-0.6718018516592991, -1.0077027774889487] -0.33590092582964953\n",
      "step: 32 Loss: 0.05735852596172166\n",
      "[2.5103709448487597, 2.5103709448487597] 2.5103709448487597\n",
      "[0, 0] 0\n",
      "[0.84679599434371, 0.84679599434371] 0.84679599434371\n",
      "step: 32 Loss: 0.3541336319063906\n",
      "[-1.846594038474332, -2.769891057711498] -0.923297019237166\n",
      "[0, 0] 0\n",
      "[-0.6230517418412413, -0.934577612761862] -0.3115258709206207\n",
      "step: 33 Loss: 0.04898550640759551\n",
      "[2.3311387470536875, 2.3311387470536875] 2.3311387470536875\n",
      "[0, 0] 0\n",
      "[0.7864288424139888, 0.7864288424139888] 0.7864288424139888\n",
      "step: 33 Loss: 0.3039769677542241\n",
      "[-1.7112268714396686, -2.5668403071595027] -0.8556134357198343\n",
      "[0, 0] 0\n",
      "[-0.5774343442723773, -0.8661515164085659] -0.28871717213618864\n",
      "step: 34 Loss: 0.04179505315441008\n",
      "[2.1629646536884835, 2.1629646536884835] 2.1629646536884835\n",
      "[0, 0] 0\n",
      "[0.729772299053953, 0.729772299053953] 0.729772299053953\n",
      "step: 34 Loss: 0.26057123709524393\n",
      "[-1.5847185896623543, -2.3770778844935316] -0.7923592948311772\n",
      "[0, 0] 0\n",
      "[-0.5347936425006178, -0.8021904637509267] -0.2673968212503089\n",
      "step: 35 Loss: 0.035627094379762185\n",
      "[2.005375451636177, 2.005375451636177] 2.005375451636177\n",
      "[0, 0] 0\n",
      "[0.6766697660014678, 0.6766697660014678] 0.6766697660014678\n",
      "step: 35 Loss: 0.22307468672929645\n",
      "[-1.4666198837734201, -2.19992982566013] -0.7333099418867101\n",
      "[0, 0] 0\n",
      "[-0.4949800356549125, -0.7424700534823687] -0.24749001782745625\n",
      "step: 36 Loss: 0.03034246658012665\n",
      "[1.8578979486576885, 1.8578979486576885] 1.8578979486576885\n",
      "[0, 0] 0\n",
      "[0.6269642624727508, 0.6269642624727508] 0.6269642624727508\n",
      "step: 36 Loss: 0.19073888261004956\n",
      "[-1.3564841520022042, -2.034726228003306] -0.6782420760011021\n",
      "[0, 0] 0\n",
      "[-0.45784455926656964, -0.6867668388998545] -0.22892227963328482\n",
      "step: 37 Loss: 0.02581961376371416\n",
      "[1.7200564131674292, 1.7200564131674292] 1.7200564131674292\n",
      "[0, 0] 0\n",
      "[0.580497602823764, 0.580497602823764] 0.580497602823764\n",
      "step: 37 Loss: 0.16289946692158652\n",
      "[-1.2538821012887387, -1.880823151933108] -0.6269410506443693\n",
      "[0, 0] 0\n",
      "[-0.423243846383204, -0.634865769574806] -0.211621923191602\n",
      "step: 38 Loss: 0.021953046074002912\n",
      "[1.5913786262558227, 1.5913786262558227] 1.5913786262558227\n",
      "[0, 0] 0\n",
      "[0.5371124758918856, 0.5371124758918856] 0.5371124758918856\n",
      "step: 38 Loss: 0.138969210474693\n",
      "[-1.1583909659144118, -1.7375864488716177] -0.5791954829572059\n",
      "[0, 0] 0\n",
      "[-0.39103651855810573, -0.5865547778371586] -0.19551825927905286\n",
      "step: 39 Loss: 0.018650951929349086\n",
      "[1.471395141757277, 1.471395141757277] 1.471395141757277\n",
      "[0, 0] 0\n",
      "[0.4966522301803146, 0.4966522301803146] 0.4966522301803146\n",
      "step: 39 Loss: 0.1184300879284834\n",
      "[-1.0696046022235057, -1.6044069033352586] -0.5348023011117529\n",
      "[0, 0] 0\n",
      "[-0.36108661992159363, -0.5416299298823904] -0.18054330996079682\n",
      "step: 40 Loss: 0.015833848599480907\n",
      "[1.359644046298303, 1.359644046298303] 1.359644046298303\n",
      "[0, 0] 0\n",
      "[0.4589625116420478, 0.4589625116420478] 0.4589625116420478\n",
      "step: 40 Loss: 0.10082685036267679\n",
      "[-0.9871265091159045, -1.4806897636738567] -0.49356325455795225\n",
      "[0, 0] 0\n",
      "[-0.33326128495208734, -0.49989192742813104] -0.16663064247604367\n",
      "step: 41 Loss: 0.013432807537970536\n",
      "[1.255671022295066, 1.255671022295066] 1.255671022295066\n",
      "[0, 0] 0\n",
      "[0.42389131386300843, 0.42389131386300843] 0.42389131386300843\n",
      "step: 41 Loss: 0.0857602496310607\n",
      "[-0.9105769049288688, -1.3658653573933033] -0.4552884524644344\n",
      "[0, 0] 0\n",
      "[-0.3074331486870213, -0.461149723030532] -0.15371657434351066\n",
      "step: 42 Loss: 0.011388320594866871\n",
      "[1.1590329859867259, 1.1590329859867259] 1.1590329859867259\n",
      "[0, 0] 0\n",
      "[0.3912902319171518, 0.3912902319171518] 0.3912902319171518\n",
      "step: 42 Loss: 0.0728813312852547\n",
      "[-0.8395879967722344, -1.2593819951583516] -0.4197939983861172\n",
      "[0, 0] 0\n",
      "[-0.283478768416166, -0.425218152624249] -0.141739384208083\n",
      "step: 43 Loss: 0.009648961242296061\n",
      "[1.0692983705646397, 1.0692983705646397] 1.0692983705646397\n",
      "[0, 0] 0\n",
      "[0.36101458152165306, 0.36101458152165306] 0.36101458152165306\n",
      "step: 43 Loss: 0.06188572039560306\n",
      "[-0.7738089372478189, -1.1607134058717283] -0.3869044686239094\n",
      "[0, 0] 0\n",
      "[-0.26128031379205446, -0.39192047068808167] -0.13064015689602723\n",
      "step: 44 Loss: 0.00817045779474915\n",
      "[0.986049775037277, 0.986049775037277] 0.986049775037277\n",
      "[0, 0] 0\n",
      "[0.33292431422900903, 0.33292431422900903] 0.33292431422900903\n",
      "step: 44 Loss: 0.05250872279167738\n",
      "[-0.7129024288030481, -1.0693536432045723] -0.35645121440152405\n",
      "[0, 0] 0\n",
      "[-0.24072443499920093, -0.3610866524988014] -0.12036221749960047\n",
      "step: 45 Loss: 0.006914677304860095\n",
      "[0.9088841630716669, 0.9088841630716669] 0.9088841630716669\n",
      "[0, 0] 0\n",
      "[0.3068841034609122, 0.3068841034609122] 0.3068841034609122\n",
      "step: 45 Loss: 0.04452059301752911\n",
      "[-0.6565481454888283, -0.9848222182332425] -0.32827407274441417\n",
      "[0, 0] 0\n",
      "[-0.22170343102152953, -0.3325551465322943] -0.11085171551076477\n",
      "step: 46 Loss: 0.005848883780841138\n",
      "[0.837414660343395, 0.837414660343395] 0.837414660343395\n",
      "[0, 0] 0\n",
      "[0.2827639680432786, 0.2827639680432786] 0.2827639680432786\n",
      "step: 46 Loss: 0.037722448743021164\n",
      "[-0.6044401387028536, -0.9066602080542805] -0.3022200693514268\n",
      "[0, 0] 0\n",
      "[-0.20411438497618523, -0.3061715774642778] -0.10205719248809261\n",
      "step: 47 Loss: 0.004944966857903326\n",
      "[0.7712705390852271, 0.7712705390852271] 0.7712705390852271\n",
      "[0, 0] 0\n",
      "[0.2604392819010981, 0.2604392819010981] 0.2604392819010981\n",
      "step: 47 Loss: 0.03194242655309423\n",
      "[-0.5562891329542728, -0.8344336994314092] -0.2781445664771364\n",
      "[0, 0] 0\n",
      "[-0.18785994925365496, -0.2817899238804824] -0.09392997462682748\n",
      "step: 48 Loss: 0.004178858336782221\n",
      "[0.7100983106052756, 0.7100983106052756] 0.7100983106052756\n",
      "[0, 0] 0\n",
      "[0.239791156049707, 0.239791156049707] 0.239791156049707\n",
      "step: 48 Loss: 0.027032357705853455\n",
      "[-0.511820426721853, -0.7677306400827795] -0.2559102133609265\n",
      "[0, 0] 0\n",
      "[-0.1728476453108641, -0.2592714679662962] -0.08642382265543205\n",
      "step: 49 Loss: 0.0035299486860019243\n",
      "[0.6535614686019147, 0.6535614686019147] 0.6535614686019147\n",
      "[0, 0] 0\n",
      "[0.2207063635097754, 0.2207063635097754] 0.2207063635097754\n",
      "step: 49 Loss: 0.02286470222926239\n",
      "[-0.4707753626025392, -0.7061630439038088] -0.2353876813012696\n",
      "[0, 0] 0\n",
      "[-0.15899036767268546, -0.23848555150902817] -0.07949518383634273\n",
      "step: 50 Loss: 0.002980634916893647\n",
      "[0.6013410239003555, 0.6013410239003555] 0.6013410239003555\n",
      "[0, 0] 0\n",
      "[0.20307752999721201, 0.20307752999721201] 0.20307752999721201\n",
      "step: 50 Loss: 0.019329900957599795\n",
      "[-0.432909551589152, -0.649364327383728] -0.216454775794576\n",
      "[0, 0] 0\n",
      "[-0.14620579079556315, -0.2193086861933447] -0.07310289539778157\n",
      "step: 51 Loss: 0.002515881569805138\n",
      "[0.5531350344359459, 0.5531350344359459] 0.5531350344359459\n",
      "[0, 0] 0\n",
      "[0.18680298408993806, 0.18680298408993806] 0.18680298408993806\n",
      "step: 51 Loss: 0.016333971313593525\n",
      "[-0.39799374429971607, -0.5969906164495741] -0.19899687214985803\n",
      "[0, 0] 0\n",
      "[-0.13441666896719787, -0.2016250034507968] -0.06720833448359893\n",
      "step: 52 Loss: 0.00212287488580488\n",
      "[0.5086587195001122, 0.5086587195001122] 0.5086587195001122\n",
      "[0, 0] 0\n",
      "[0.17178680352818992, 0.17178680352818992] 0.17178680352818992\n",
      "step: 52 Loss: 0.013796436093057321\n",
      "[-0.3658122847745712, -0.5487184271618568] -0.1829061423872856\n",
      "[0, 0] 0\n",
      "[-0.12355031901445282, -0.18532547852167924] -0.06177515950722641\n",
      "step: 53 Loss: 0.0017906945813022457\n",
      "[0.46764382801773224, 0.46764382801773224] 0.46764382801773224\n",
      "[0, 0] 0\n",
      "[0.1579386086098867, 0.1579386086098867] 0.1579386086098867\n",
      "step: 53 Loss: 0.011648466315318844\n",
      "[-0.336163555357336, -0.5042453330360039] -0.168081777678668\n",
      "[0, 0] 0\n",
      "[-0.11353877478484324, -0.17030816217726485] -0.05676938739242162\n",
      "step: 54 Loss: 0.0015100523073505265\n",
      "[0.4298384525927786, 0.4298384525927786] 0.4298384525927786\n",
      "[0, 0] 0\n",
      "[0.14517350521217956, 0.14517350521217956] 0.14517350521217956\n",
      "step: 54 Loss: 0.009831286171066283\n",
      "[-0.30885860946270977, -0.4632879141940647] -0.15442930473135488\n",
      "[0, 0] 0\n",
      "[-0.10431832901872544, -0.15647749352808815] -0.05215916450936272\n",
      "step: 55 Loss: 0.0012730478480458465\n",
      "[0.3950062927398827, 0.3950062927398827] 0.3950062927398827\n",
      "[0, 0] 0\n",
      "[0.13341184101578682, 0.13341184101578682] 0.13341184101578682\n",
      "step: 55 Loss: 0.00829475728828979\n",
      "[-0.2837213209384069, -0.4255819814076104] -0.14186066046920345\n",
      "[0, 0] 0\n",
      "[-0.09582958692745798, -0.14374438039118698] -0.04791479346372899\n",
      "step: 56 Loss: 0.0010729732666367256\n",
      "[0.36292626851287124, 0.36292626851287124] 0.36292626851287124\n",
      "[0, 0] 0\n",
      "[0.12257907936209532, 0.12257907936209532] 0.12257907936209532\n",
      "step: 56 Loss: 0.0069961665920544045\n",
      "[-0.2605871681921815, -0.39088075228827224] -0.13029358409609074\n",
      "[0, 0] 0\n",
      "[-0.08801705821564695, -0.13202558732347042] -0.04400852910782348\n",
      "step: 57 Loss: 0.000904132950093627\n",
      "[0.333391730315006, 0.333391730315006] 0.333391730315006\n",
      "[0, 0] 0\n",
      "[0.11260553612788533, 0.11260553612788533] 0.11260553612788533\n",
      "step: 57 Loss: 0.005899159369317842\n",
      "[-0.23930318602775902, -0.3589547790416385] -0.11965159301387951\n",
      "[0, 0] 0\n",
      "[-0.08082914311590335, -0.12124371467385503] -0.040414571557951674\n",
      "step: 58 Loss: 0.0007616981944220002\n",
      "[0.3062099509811739, 0.3062099509811739] 0.3062099509811739\n",
      "[0, 0] 0\n",
      "[0.103426211397431, 0.103426211397431] 0.103426211397431\n",
      "step: 58 Loss: 0.004972828481590094\n",
      "[-0.21972688479300317, -0.32959032718950476] -0.10986344239650159\n",
      "[0, 0] 0\n",
      "[-0.07421776928643452, -0.11132665392965177] -0.03710888464321726\n",
      "step: 59 Loss: 0.0006415751514906352\n",
      "[0.28120132429706535, 0.28120132429706535] 0.28120132429706535\n",
      "[0, 0] 0\n",
      "[0.0949805215289161, 0.0949805215289161] 0.0949805215289161\n",
      "step: 59 Loss: 0.004190918191981889\n",
      "[-0.20172607684258903, -0.30258911526388355] -0.10086303842129452\n",
      "[0, 0] 0\n",
      "[-0.06813833487871633, -0.10220750231807449] -0.034069167439358165\n",
      "step: 60 Loss: 0.0005402976587624625\n",
      "[0.2581987951401553, 0.2581987951401553] 0.2581987951401553\n",
      "[0, 0] 0\n",
      "[0.08721210904721774, 0.08721210904721774] 0.08721210904721774\n",
      "step: 60 Loss: 0.0035311463583849326\n",
      "[-0.1851779193188268, -0.2777668789782402] -0.0925889596594134\n",
      "[0, 0] 0\n",
      "[-0.0625493866515745, -0.09382407997736175] -0.03127469332578725\n",
      "step: 61 Loss: 0.0004549308505904391\n",
      "[0.23704707826289917, 0.23704707826289917] 0.23704707826289917\n",
      "[0, 0] 0\n",
      "[0.08006858080645952, 0.08006858080645952] 0.08006858080645952\n",
      "step: 61 Loss: 0.0029746153118278365\n",
      "[-0.16996866727023, -0.254953000905345] -0.084984333635115\n",
      "[0, 0] 0\n",
      "[-0.05741253780445423, -0.08611880670668134] -0.028706268902227116\n",
      "step: 62 Loss: 0.00038299267459854144\n",
      "[0.21760206955678715, 0.21760206955678715] 0.21760206955678715\n",
      "[0, 0] 0\n",
      "[0.07350131086392048, 0.07350131086392048] 0.07350131086392048\n",
      "step: 62 Loss: 0.002505311530064064\n",
      "[-0.155992830459285, -0.23398924568892748] -0.0779964152296425\n",
      "[0, 0] 0\n",
      "[-0.05269218420634961, -0.07903827630952441] -0.026346092103174804\n",
      "step: 63 Loss: 0.00032238386727657795\n",
      "[0.1997301068261081, 0.1997301068261081] 0.1997301068261081\n",
      "[0, 0] 0\n",
      "[0.06746519226841086, 0.06746519226841086] 0.06746519226841086\n",
      "step: 63 Loss: 0.0021096728547788735\n",
      "[-0.14315288973953155, -0.21472933460929733] -0.07157644486976578\n",
      "[0, 0] 0\n",
      "[-0.04835540947897944, -0.07253311421846917] -0.02417770473948972\n",
      "step: 64 Loss: 0.0002713307907585715\n",
      "[0.1833073917794589, 0.1833073917794589] 0.1833073917794589\n",
      "[0, 0] 0\n",
      "[0.06191844308052512, 0.06191844308052512] 0.06191844308052512\n",
      "step: 64 Loss: 0.0017762217152089749\n",
      "[-0.1313585586783121, -0.19703783801746813] -0.06567927933915604\n",
      "[0, 0] 0\n",
      "[-0.04437173633649025, -0.06655760450473537] -0.022185868168245124\n",
      "step: 65 Loss: 0.00022833477261396438\n",
      "[0.1682193062614903, 0.1682193062614903] 0.1682193062614903\n",
      "[0, 0] 0\n",
      "[0.056822376481807886, 0.056822376481807886] 0.056822376481807886\n",
      "step: 65 Loss: 0.001495249140208026\n",
      "[-0.12052648818288839, -0.1807897322743326] -0.06026324409144419\n",
      "[0, 0] 0\n",
      "[-0.04071302745071308, -0.061069541176069615] -0.02035651372535654\n",
      "step: 66 Loss: 0.0001921306647271446\n",
      "[0.1543598640941815, 0.1543598640941815] 0.1543598640941815\n",
      "[0, 0] 0\n",
      "[0.05214121652636783, 0.05214121652636783] 0.05214121652636783\n",
      "step: 66 Loss: 0.0012585474503457768\n",
      "[-0.11057962367299908, -0.16586943550949862] -0.05528981183649954\n",
      "[0, 0] 0\n",
      "[-0.03735326885214242, -0.05602990327821363] -0.01867663442607121\n",
      "step: 67 Loss: 0.00016165032486361783\n",
      "[0.1416310897693778, 0.1416310897693778] 0.1416310897693778\n",
      "[0, 0] 0\n",
      "[0.04784188904635435, 0.04784188904635435] 0.04784188904635435\n",
      "step: 67 Loss: 0.0010591807435417265\n",
      "[-0.1014469143727625, -0.15217037155914376] -0.05072345718638125\n",
      "[0, 0] 0\n",
      "[-0.03426847219024223, -0.05140270828536335] -0.017134236095121116\n",
      "step: 68 Loss: 0.00013599269251227182\n",
      "[0.12994251140909557, 0.12994251140909557] 0.12994251140909557\n",
      "[0, 0] 0\n",
      "[0.04389385103604215, 0.04389385103604215] 0.04389385103604215\n",
      "step: 68 Loss: 0.0008912910396956414\n",
      "[-0.09306275671483884, -0.13959413507225826] -0.04653137835741942\n",
      "[0, 0] 0\n",
      "[-0.03143648710733037, -0.047154730660995556] -0.015718243553665186\n",
      "step: 69 Loss: 0.00011439754681441444\n",
      "[0.11921060415145451, 0.11921060415145451] 0.11921060415145451\n",
      "[0, 0] 0\n",
      "[0.040268903189902575, 0.040268903189902575] 0.040268903189902575\n",
      "step: 69 Loss: 0.0007499323090403901\n",
      "[-0.08536671851345358, -0.12805007777018038] -0.04268335925672679\n",
      "[0, 0] 0\n",
      "[-0.02883690839448377, -0.04325536259172565] -0.014418454197241885\n",
      "step: 70 Loss: 9.622397152811573e-05\n",
      "[0.10935832983065187, 0.10935832983065187] 0.10935832983065187\n",
      "[0, 0] 0\n",
      "[0.03694103488125319, 0.03694103488125319] 0.03694103488125319\n",
      "step: 70 Loss: 0.0006309304704709693\n",
      "[-0.07830305940558231, -0.11745458910837347] -0.039151529702791156\n",
      "[0, 0] 0\n",
      "[-0.026450914275911855, -0.03967637141386778] -0.013225457137955927\n",
      "step: 71 Loss: 8.093154819122718e-05\n",
      "[0.10031464396752905, 0.10031464396752905] 0.10031464396752905\n",
      "[0, 0] 0\n",
      "[0.03388625802588508, 0.03388625802588508] 0.03388625802588508\n",
      "step: 71 Loss: 0.0005307638172449762\n",
      "[-0.07182047573070605, -0.10773071359605907] -0.03591023786535302\n",
      "[0, 0] 0\n",
      "[-0.024261180464895234, -0.036391770697342854] -0.012130590232447617\n",
      "step: 72 Loss: 6.806490272951427e-05\n",
      "[0.09201408383351692, 0.09201408383351692] 0.09201408383351692\n",
      "[0, 0] 0\n",
      "[0.03108246826925341, 0.03108246826925341] 0.03108246826925341\n",
      "step: 72 Loss: 0.0004464622633123232\n",
      "[-0.06587168914716598, -0.09880753372074898] -0.03293584457358299\n",
      "[0, 0] 0\n",
      "[-0.022251741396833117, -0.03337761209524968] -0.011125870698416558\n",
      "step: 73 Loss: 5.724025820042732e-05\n",
      "[0.08439633579395998, 0.08439633579395998] 0.08439633579395998\n",
      "[0, 0] 0\n",
      "[0.028509299121765565, 0.028509299121765565] 0.028509299121765565\n",
      "step: 73 Loss: 0.000375521467068983\n",
      "[-0.06041321501635529, -0.09061982252453293] -0.030206607508177646\n",
      "[0, 0] 0\n",
      "[-0.020407912156921157, -0.030611868235381735] -0.010203956078460578\n",
      "step: 74 Loss: 4.813437247984654e-05\n",
      "[0.07740587081049878, 0.07740587081049878] 0.07740587081049878\n",
      "[0, 0] 0\n",
      "[0.026147999073019488, 0.026147999073019488] 0.026147999073019488\n",
      "step: 74 Loss: 0.0003158305376910346\n",
      "[-0.05540501084242623, -0.08310751626363935] -0.027702505421213115\n",
      "[0, 0] 0\n",
      "[-0.018716169863625284, -0.028074254795437924] -0.009358084931812642\n",
      "step: 75 Loss: 4.047494206201314e-05\n",
      "[0.07099156760286547, 0.07099156760286547] 0.07099156760286547\n",
      "[0, 0] 0\n",
      "[0.023981304499586357, 0.023981304499586357] 0.023981304499586357\n",
      "step: 75 Loss: 0.00026561052410143277\n",
      "[-0.05081026893012504, -0.07621540339518756] -0.02540513446506252\n",
      "[0, 0] 0\n",
      "[-0.017164083747254727, -0.02574612562088209] -0.008582041873627363\n",
      "step: 76 Loss: 3.4032699393909366e-05\n",
      "[0.06510639301524553, 0.06510639301524553] 0.06510639301524553\n",
      "[0, 0] 0\n",
      "[0.021993331864228752, 0.021993331864228752] 0.021993331864228752\n",
      "step: 76 Loss: 0.0002233626717089941\n",
      "[-0.04659511693204488, -0.06989267539806732] -0.02329755846602244\n",
      "[0, 0] 0\n",
      "[-0.015740214094057674, -0.02361032114108651] -0.007870107047028837\n",
      "step: 77 Loss: 2.8614576271676622e-05\n",
      "[0.05970707582142808, 0.05970707582142808] 0.05970707582142808\n",
      "[0, 0] 0\n",
      "[0.020169467671602864, 0.020169467671602864] 0.020169467671602864\n",
      "step: 77 Loss: 0.00018782446230369794\n",
      "[-0.04272843424432355, -0.06409265136648533] -0.021364217122161775\n",
      "[0, 0] 0\n",
      "[-0.014434050308646843, -0.021651075462970265] -0.007217025154323421\n",
      "step: 78 Loss: 2.4058068187384686e-05\n",
      "[0.05475382845695986, 0.05475382845695986] 0.05475382845695986\n",
      "[0, 0] 0\n",
      "[0.01849627459073574, 0.01849627459073574] 0.01849627459073574\n",
      "step: 78 Loss: 0.0001579326574137908\n",
      "[-0.039181597651330925, -0.05877239647699639] -0.019590798825665463\n",
      "[0, 0] 0\n",
      "[-0.013235925062627989, -0.019853887593941983] -0.006617962531313994\n",
      "step: 79 Loss: 2.0226370623673915e-05\n",
      "[0.050210066098765375, 0.050210066098765375] 0.050210066098765375\n",
      "[0, 0] 0\n",
      "[0.01696139666205559, 0.01696139666205559] 0.01696139666205559\n",
      "step: 79 Loss: 0.00013279194066987055\n",
      "[-0.03592832012990983, -0.05389248019486475] -0.017964160064954916\n",
      "[0, 0] 0\n",
      "[-0.01213695990185928, -0.01820543985278892] -0.00606847995092964\n",
      "step: 80 Loss: 1.7004366002500106e-05\n",
      "[0.04604216583169016, 0.04604216583169016] 0.04604216583169016\n",
      "[0, 0] 0\n",
      "[0.015553478029488775, 0.015553478029488775] 0.015553478029488775\n",
      "step: 80 Loss: 0.0001116485690112911\n",
      "[-0.032944435312888945, -0.04941665296933342] -0.016472217656444472\n",
      "[0, 0] 0\n",
      "[-0.011128992487239758, -0.016693488730859636] -0.005564496243619879\n",
      "step: 81 Loss: 1.4295167680278228e-05\n",
      "[0.042219225737069786, 0.042219225737069786] 0.042219225737069786\n",
      "[0, 0] 0\n",
      "[0.014262081632357464, 0.014262081632357464] 0.014262081632357464\n",
      "step: 81 Loss: 9.38680402413906e-05\n",
      "[-0.030207756934214423, -0.04531163540132163] -0.015103878467107212\n",
      "[0, 0] 0\n",
      "[-0.010204529156872918, -0.015306793735309377] -0.005102264578436459\n",
      "step: 82 Loss: 1.2017265881827148e-05\n",
      "[0.038712857459100365, 0.038712857459100365] 0.038712857459100365\n",
      "[0, 0] 0\n",
      "[0.013077619194550232, 0.013077619194550232] 0.013077619194550232\n",
      "step: 82 Loss: 7.891633530152377e-05\n",
      "[-0.027697896552392978, -0.04154684482858947] -0.013848948276196489\n",
      "[0, 0] 0\n",
      "[-0.009356683387318016, -0.014035025080977023] -0.004678341693659008\n",
      "step: 83 Loss: 1.010207444652762e-05\n",
      "[0.03549698032494754, 0.03549698032494754] 0.03549698032494754\n",
      "[0, 0] 0\n",
      "[0.011991281728679581, 0.011991281728679581] 0.011991281728679581\n",
      "step: 83 Loss: 6.634403474415245e-05\n",
      "[-0.02539614167243509, -0.038094212508652636] -0.012698070836217545\n",
      "[0, 0] 0\n",
      "[-0.008579134652323572, -0.012868701978485358] -0.004289567326161786\n",
      "step: 84 Loss: 8.491903164525742e-06\n",
      "[0.03254764338528975, 0.03254764338528975] 0.03254764338528975\n",
      "[0, 0] 0\n",
      "[0.010994979463630138, 0.010994979463630138] 0.010994979463630138\n",
      "step: 84 Loss: 5.577298187699378e-05\n",
      "[-0.02328530185536903, -0.034927952783053544] -0.011642650927684515\n",
      "[0, 0] 0\n",
      "[-0.007866076462492844, -0.011799114693739267] -0.003933038231246422\n",
      "step: 85 Loss: 7.138217811538581e-06\n",
      "[0.029842849981660313, 0.029842849981660313] 0.029842849981660313\n",
      "[0, 0] 0\n",
      "[0.010081282618652767, 0.010081282618652767] 0.010081282618652767\n",
      "step: 85 Loss: 4.688499864592786e-05\n",
      "[-0.021349603513187723, -0.032024405269781585] -0.010674801756593862\n",
      "[0, 0] 0\n",
      "[-0.0072121808473571206, -0.01081827127103568] -0.0036060904236785603\n",
      "step: 86 Loss: 6.000200881257917e-06\n",
      "[0.02736240557189819, 0.02736240557189819] 0.02736240557189819\n",
      "[0, 0] 0\n",
      "[0.00924337002832831, 0.00924337002832831] 0.00924337002832831\n",
      "step: 86 Loss: 3.941241308241721e-05\n",
      "[-0.01957456016635181, -0.02936184024952771] -0.009787280083175904\n",
      "[0, 0] 0\n",
      "[-0.006612554544913688, -0.009918831817370532] -0.003306277272456844\n",
      "step: 87 Loss: 5.043518404248114e-06\n",
      "[0.02508776860194998, 0.02508776860194998] 0.02508776860194998\n",
      "[0, 0] 0\n",
      "[0.008474978791623768, 0.008474978791623768] 0.008474978791623768\n",
      "step: 87 Loss: 3.313005034605027e-05\n",
      "[-0.017946881969801088, -0.026920322954701634] -0.008973440984900544\n",
      "[0, 0] 0\n",
      "[-0.006062708453650566, -0.00909406268047585] -0.003031354226825283\n",
      "step: 88 Loss: 4.239299100900717e-06\n",
      "[0.02300192073318818, 0.02300192073318818] 0.02300192073318818\n",
      "[0, 0] 0\n",
      "[0.007770360455231379, 0.007770360455231379] 0.007770360455231379\n",
      "step: 88 Loss: 2.7848510800134132e-05\n",
      "[-0.016454366455084586, -0.02468154968262688] -0.008227183227542293\n",
      "[0, 0] 0\n",
      "[-0.0055585207366567, -0.008337781104985051] -0.00277926036832835\n",
      "step: 89 Loss: 3.563260904556697e-06\n",
      "[0.021089240328206266, 0.021089240328206266] 0.021089240328206266\n",
      "[0, 0] 0\n",
      "[0.007124238294110729, 0.007124238294110729] 0.007124238294110729\n",
      "step: 89 Loss: 2.340849014945393e-05\n",
      "[-0.015085820966785278, -0.02262873145017792] -0.007542910483392639\n",
      "[0, 0] 0\n",
      "[-0.005096210630424994, -0.007644315945637491] -0.002548105315212497\n",
      "step: 90 Loss: 2.994987353131898e-06\n",
      "[0.019335392032047507, 0.019335392032047507] 0.019335392032047507\n",
      "[0, 0] 0\n",
      "[0.006531770026265367, 0.006531770026265367] 0.006531770026265367\n",
      "step: 90 Loss: 1.967601299580565e-05\n",
      "[-0.013830970742735628, -0.020746456114103442] -0.006915485371367814\n",
      "[0, 0] 0\n",
      "[-0.004672307402176089, -0.007008461103264134] -0.0023361537010880445\n",
      "step: 91 Loss: 2.517309208221997e-06\n",
      "[0.017727219621888744, 0.017727219621888744] 0.017727219621888744\n",
      "[0, 0] 0\n",
      "[0.005988511629127194, 0.005988511629127194] 0.005988511629127194\n",
      "step: 91 Loss: 1.6538407446125948e-05\n",
      "[-0.012680392590113837, -0.019020588885170754] -0.006340196295056918\n",
      "[0, 0] 0\n",
      "[-0.004283627952367161, -0.006425441928550741] -0.0021418139761835804\n",
      "step: 92 Loss: 2.115791813969866e-06\n",
      "[0.016252652232526428, 0.016252652232526428] 0.016252652232526428\n",
      "[0, 0] 0\n",
      "[0.005490385672334798, 0.005490385672334798] 0.005490385672334798\n",
      "step: 92 Loss: 1.390092740957888e-05\n",
      "[-0.01162543761323559, -0.017438156419853384] -0.005812718806617795\n",
      "[0, 0] 0\n",
      "[-0.003927250717463294, -0.005890876076194941] -0.001963625358731647\n",
      "step: 93 Loss: 1.7782974983521955e-06\n",
      "[0.01490061373232141, 0.01490061373232141] 0.01490061373232141\n",
      "[0, 0] 0\n",
      "[0.005033650712994153, 0.005033650712994153] 0.005033650712994153\n",
      "step: 93 Loss: 1.1683901359011299e-05\n",
      "[-0.01065817462365323, -0.015987261935479842] -0.005329087311826615\n",
      "[0, 0] 0\n",
      "[-0.00360049655845799, -0.005400744837686985] -0.001800248279228995\n",
      "step: 94 Loss: 1.4946225567961012e-06\n",
      "[0.013660943209363004, 0.013660943209363004] 0.013660943209363004\n",
      "[0, 0] 0\n",
      "[0.004614874442617562, 0.004614874442617562] 0.004614874442617562\n",
      "step: 94 Loss: 9.820340019282984e-06\n",
      "[-0.009771325222782713, -0.01465698783417407] -0.004885662611391357\n",
      "[0, 0] 0\n",
      "[-0.003300906835369582, -0.004951360253054373] -0.001650453417684791\n",
      "step: 95 Loss: 1.2561876946290784e-06\n",
      "[0.012524318413540621, 0.012524318413540621] 0.012524318413540621\n",
      "[0, 0] 0\n",
      "[0.004230907831345143, 0.004230907831345143] 0.004230907831345143\n",
      "step: 95 Loss: 8.253917754169078e-06\n",
      "[-0.008958215605554216, -0.013437323408331325] -0.004479107802777108\n",
      "[0, 0] 0\n",
      "[-0.0030262271296177274, -0.004539340694426591] -0.0015131135648088637\n",
      "step: 96 Loss: 1.0557810630001342e-06\n",
      "[0.011482188423527032, 0.011482188423527032] 0.011482188423527032\n",
      "[0, 0] 0\n",
      "[0.0038788623874070104, 0.0038788623874070104] 0.0038788623874070104\n",
      "step: 96 Loss: 6.937278930485608e-06\n",
      "[-0.008212722050679463, -0.012319083076019194] -0.004106361025339731\n",
      "[0, 0] 0\n",
      "[-0.0027743888330194547, -0.004161583249529182] -0.0013871944165097273\n",
      "step: 97 Loss: 8.873393429988016e-07\n",
      "[0.010526709034201302, 0.010526709034201302] 0.010526709034201302\n",
      "[0, 0] 0\n",
      "[0.0035560883346942218, 0.0035560883346942218] 0.0035560883346942218\n",
      "step: 97 Loss: 5.830609354275683e-06\n",
      "[-0.007529229936719161, -0.011293844905078742] -0.0037646149683595804\n",
      "[0, 0] 0\n",
      "[-0.002543495305490455, -0.0038152429582356826] -0.0012717476527452276\n",
      "step: 98 Loss: 7.457659106386219e-07\n",
      "[0.009650685801589872, 0.009650685801589872] 0.009650685801589872\n",
      "[0, 0] 0\n",
      "[0.0032601553763980376, 0.0032601553763980376] 0.0032601553763980376\n",
      "step: 98 Loss: 4.90043771289807e-06\n",
      "[-0.006902587989853212, -0.010353881984779819] -0.003451293994926606\n",
      "[0, 0] 0\n",
      "[-0.002331806421500088, -0.003497709632250132] -0.001165903210750044\n",
      "step: 99 Loss: 6.267760602924804e-07\n",
      "[0.0088475195554266, 0.0088475195554266] 0.0088475195554266\n",
      "[0, 0] 0\n",
      "[0.0029888342916142003, 0.0029888342916142003] 0.0029888342916142003\n",
      "step: 99 Loss: 4.118624924427808e-06\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        h = hidden.forward(x)\n",
    "        a = [relu(node) for node in h]\n",
    "        y = output.forward(a)[0]\n",
    "        l = (y - ytrue)**2\n",
    "\n",
    "        backward(l)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            print([w.grad for w in neuron.w], neuron.b.grad)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187deef",
   "metadata": {},
   "source": [
    "#### Dead Neurons and Gradient Flow\n",
    "\n",
    "If a hidden neuron has negative pre-activation:\n",
    "\n",
    "ReLU'(z) = 0\n",
    "\n",
    "Then:\n",
    "\n",
    "dL/dw_hidden = 0\n",
    "\n",
    "This demonstrates how nonlinearities gate gradients.\n",
    "\n",
    "Only active neurons update.\n",
    "\n",
    "This is also why initialization matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a821d5",
   "metadata": {},
   "source": [
    "## Key Engineering Insights\n",
    "\n",
    "- Reverse-mode autodiff computes all gradients in one backward pass.\n",
    "- Gradients accumulate via +=, not assignment.\n",
    "- zero_grad is mandatory before backward.\n",
    "- Random initialization breaks symmetry.\n",
    "- ReLU introduces gradient gating.\n",
    "- Depth multiplies derivative terms via chain rule.\n",
    "- Even multi-layer networks can collapse to simpler representations.\n",
    "\n",
    "This project transformed backpropagation\n",
    "from an abstract formula into a concrete computational system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
