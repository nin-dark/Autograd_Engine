{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135423c2",
   "metadata": {},
   "source": [
    "# Tiny Neural Network using The Autograd Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfab81",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, I build a tiny neural network entirely on top of my own reverse-mode autodiff engine.\n",
    "\n",
    "There is:\n",
    "- No NumPy\n",
    "- No PyTorch\n",
    "- No vectorization\n",
    "- Only scalar Nodes and a computational graph\n",
    "\n",
    "The goal is not performance — it is understanding gradient flow deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6146a85",
   "metadata": {},
   "source": [
    "Importing our autograd engine as a library to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c3561fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de77e43",
   "metadata": {},
   "source": [
    "## Phase 1 — Single Input, Single Neuron\n",
    "\n",
    "We start with the simplest possible model:\n",
    "\n",
    "y = w·x + b\n",
    "\n",
    "Loss:\n",
    "L = (y − y_true)²\n",
    "\n",
    "This phase validates:\n",
    "\n",
    "- Parameter nodes store gradients correctly\n",
    "- Multiplication and addition propagate gradients\n",
    "- reverse-mode traversal works\n",
    "- zero_grad lifecycle is correct\n",
    "- manual derivative matches engine output\n",
    "\n",
    "If this fails, the engine is broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50610c",
   "metadata": {},
   "source": [
    "### Single-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e648c",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e394a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba937",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5697f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "ytrue = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75b56e",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9f1adb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -40 -20\n",
      "manual: -40 -20\n",
      "0 100\n",
      "engine: -36.0 -18.0\n",
      "manual: -36.0 -18.0\n",
      "1 81.0\n",
      "engine: -32.4 -16.2\n",
      "manual: -32.4 -16.2\n",
      "2 65.61\n",
      "engine: -29.16 -14.58\n",
      "manual: -29.16 -14.58\n",
      "3 53.1441\n",
      "engine: -26.244 -13.122\n",
      "manual: -26.244 -13.122\n",
      "4 43.046721\n",
      "engine: -23.6196 -11.8098\n",
      "manual: -23.6196 -11.8098\n",
      "5 34.86784400999999\n",
      "engine: -21.25764 -10.62882\n",
      "manual: -21.25764 -10.62882\n",
      "6 28.242953648099995\n",
      "engine: -19.131876 -9.565938\n",
      "manual: -19.131876 -9.565938\n",
      "7 22.876792454960995\n",
      "engine: -17.218688399999994 -8.609344199999997\n",
      "manual: -17.218688399999994 -8.609344199999997\n",
      "8 18.5302018885184\n",
      "engine: -15.496819559999999 -7.748409779999999\n",
      "manual: -15.496819559999999 -7.748409779999999\n",
      "9 15.009463529699909\n",
      "engine: -13.947137603999998 -6.973568801999999\n",
      "manual: -13.947137603999998 -6.973568801999999\n",
      "10 12.157665459056926\n",
      "engine: -12.552423843599996 -6.276211921799998\n",
      "manual: -12.552423843599996 -6.276211921799998\n",
      "11 9.847709021836106\n",
      "engine: -11.29718145924 -5.64859072962\n",
      "manual: -11.29718145924 -5.64859072962\n",
      "12 7.976644307687252\n",
      "engine: -10.167463313315999 -5.083731656657999\n",
      "manual: -10.167463313315999 -5.083731656657999\n",
      "13 6.461081889226672\n",
      "engine: -9.150716981984395 -4.575358490992198\n",
      "manual: -9.150716981984395 -4.575358490992198\n",
      "14 5.2334763302736\n",
      "engine: -8.235645283785956 -4.117822641892978\n",
      "manual: -8.235645283785956 -4.117822641892978\n",
      "15 4.2391158275216165\n",
      "engine: -7.412080755407359 -3.7060403777036797\n",
      "manual: -7.412080755407359 -3.7060403777036797\n",
      "16 3.433683820292508\n",
      "engine: -6.670872679866626 -3.335436339933313\n",
      "manual: -6.670872679866626 -3.335436339933313\n",
      "17 2.781283894436934\n",
      "engine: -6.00378541187996 -3.00189270593998\n",
      "manual: -6.00378541187996 -3.00189270593998\n",
      "18 2.252839954493914\n",
      "engine: -5.403406870691967 -2.7017034353459835\n",
      "manual: -5.403406870691967 -2.7017034353459835\n",
      "19 1.8248003631400722\n",
      "engine: -4.86306618362277 -2.431533091811385\n",
      "manual: -4.86306618362277 -2.431533091811385\n",
      "20 1.4780882941434585\n",
      "engine: -4.3767595652604925 -2.1883797826302462\n",
      "manual: -4.3767595652604925 -2.1883797826302462\n",
      "21 1.197251518256201\n",
      "engine: -3.9390836087344425 -1.9695418043672213\n",
      "manual: -3.9390836087344425 -1.9695418043672213\n",
      "22 0.9697737297875224\n",
      "engine: -3.545175247860996 -1.772587623930498\n",
      "manual: -3.545175247860996 -1.772587623930498\n",
      "23 0.7855167211278922\n",
      "engine: -3.1906577230748994 -1.5953288615374497\n",
      "manual: -3.1906577230748994 -1.5953288615374497\n",
      "24 0.6362685441135938\n",
      "engine: -2.87159195076741 -1.435795975383705\n",
      "manual: -2.87159195076741 -1.435795975383705\n",
      "25 0.5153775207320113\n",
      "engine: -2.5844327556906705 -1.2922163778453353\n",
      "manual: -2.5844327556906705 -1.2922163778453353\n",
      "26 0.4174557917929296\n",
      "engine: -2.3259894801215992 -1.1629947400607996\n",
      "manual: -2.3259894801215992 -1.1629947400607996\n",
      "27 0.3381391913522717\n",
      "engine: -2.0933905321094386 -1.0466952660547193\n",
      "manual: -2.0933905321094386 -1.0466952660547193\n",
      "28 0.2738927449953399\n",
      "engine: -1.884051478898499 -0.9420257394492495\n",
      "manual: -1.884051478898499 -0.9420257394492495\n",
      "29 0.22185312344622632\n",
      "engine: -1.6956463310086463 -0.8478231655043231\n",
      "manual: -1.6956463310086463 -0.8478231655043231\n",
      "30 0.17970102999144272\n",
      "engine: -1.5260816979077845 -0.7630408489538922\n",
      "manual: -1.5260816979077845 -0.7630408489538922\n",
      "31 0.14555783429306915\n",
      "engine: -1.3734735281170032 -0.6867367640585016\n",
      "manual: -1.3734735281170032 -0.6867367640585016\n",
      "32 0.11790184577738552\n",
      "engine: -1.2361261753053014 -0.6180630876526507\n",
      "manual: -1.2361261753053014 -0.6180630876526507\n",
      "33 0.09550049507968206\n",
      "engine: -1.1125135577747756 -0.5562567788873878\n",
      "manual: -1.1125135577747756 -0.5562567788873878\n",
      "34 0.07735540101454305\n",
      "engine: -1.0012622019972923 -0.5006311009986462\n",
      "manual: -1.0012622019972923 -0.5006311009986462\n",
      "35 0.06265787482177916\n",
      "engine: -0.9011359817975659 -0.45056799089878297\n",
      "manual: -0.9011359817975659 -0.45056799089878297\n",
      "36 0.05075287860564144\n",
      "engine: -0.8110223836178108 -0.4055111918089054\n",
      "manual: -0.8110223836178108 -0.4055111918089054\n",
      "37 0.04110983167056971\n",
      "engine: -0.729920145256024 -0.364960072628012\n",
      "manual: -0.729920145256024 -0.364960072628012\n",
      "38 0.03329896365316095\n",
      "engine: -0.656928130730428 -0.328464065365214\n",
      "manual: -0.656928130730428 -0.328464065365214\n",
      "39 0.026972160559060893\n",
      "engine: -0.5912353176573788 -0.2956176588286894\n",
      "manual: -0.5912353176573788 -0.2956176588286894\n",
      "40 0.021847450052838852\n",
      "engine: -0.5321117858916438 -0.2660558929458219\n",
      "manual: -0.5321117858916438 -0.2660558929458219\n",
      "41 0.01769643454279966\n",
      "engine: -0.47890060730247797 -0.23945030365123898\n",
      "manual: -0.47890060730247797 -0.23945030365123898\n",
      "42 0.014334111979667639\n",
      "engine: -0.4310105465722316 -0.2155052732861158\n",
      "manual: -0.4310105465722316 -0.2155052732861158\n",
      "43 0.011610630703530864\n",
      "engine: -0.38790949191501056 -0.19395474595750528\n",
      "manual: -0.38790949191501056 -0.19395474595750528\n",
      "44 0.009404610869860103\n",
      "engine: -0.3491185427235095 -0.17455927136175475\n",
      "manual: -0.3491185427235095 -0.17455927136175475\n",
      "45 0.007617734804586683\n",
      "engine: -0.31420668845115785 -0.15710334422557892\n",
      "manual: -0.31420668845115785 -0.15710334422557892\n",
      "46 0.006170365191715186\n",
      "engine: -0.28278601960603567 -0.14139300980301783\n",
      "manual: -0.28278601960603567 -0.14139300980301783\n",
      "47 0.004997995805289074\n",
      "engine: -0.25450741764543494 -0.12725370882271747\n",
      "manual: -0.25450741764543494 -0.12725370882271747\n",
      "48 0.004048376602284241\n",
      "engine: -0.22905667588089074 -0.11452833794044537\n",
      "manual: -0.22905667588089074 -0.11452833794044537\n",
      "49 0.0032791850478502147\n",
      "engine: -0.20615100829279953 -0.10307550414639977\n",
      "manual: -0.20615100829279953 -0.10307550414639977\n",
      "50 0.002656139888758619\n",
      "engine: -0.18553590746351745 -0.09276795373175872\n",
      "manual: -0.18553590746351745 -0.09276795373175872\n",
      "51 0.002151473309894432\n",
      "engine: -0.16698231671716712 -0.08349115835858356\n",
      "manual: -0.16698231671716712 -0.08349115835858356\n",
      "52 0.0017426933810145194\n",
      "engine: -0.150284085045449 -0.0751420425227245\n",
      "manual: -0.150284085045449 -0.0751420425227245\n",
      "53 0.0014115816386217341\n",
      "engine: -0.13525567654090764 -0.06762783827045382\n",
      "manual: -0.13525567654090764 -0.06762783827045382\n",
      "54 0.0011433811272836647\n",
      "engine: -0.12173010888681546 -0.06086505444340773\n",
      "manual: -0.12173010888681546 -0.06086505444340773\n",
      "55 0.0009261387130997467\n",
      "engine: -0.10955709799813462 -0.05477854899906731\n",
      "manual: -0.10955709799813462 -0.05477854899906731\n",
      "56 0.0007501723576108046\n",
      "engine: -0.09860138819831832 -0.04930069409915916\n",
      "manual: -0.09860138819831832 -0.04930069409915916\n",
      "57 0.0006076396096647167\n",
      "engine: -0.08874124937848649 -0.04437062468924324\n",
      "manual: -0.08874124937848649 -0.04437062468924324\n",
      "58 0.0004921880838284205\n",
      "engine: -0.07986712444063926 -0.03993356222031963\n",
      "manual: -0.07986712444063926 -0.03993356222031963\n",
      "59 0.0003986723479010348\n",
      "engine: -0.07188041199657391 -0.035940205998286956\n",
      "manual: -0.07188041199657391 -0.035940205998286956\n",
      "60 0.0003229246017998254\n",
      "engine: -0.06469237079691226 -0.03234618539845613\n",
      "manual: -0.06469237079691226 -0.03234618539845613\n",
      "61 0.00026156892745782414\n",
      "engine: -0.05822313371722743 -0.029111566858613713\n",
      "manual: -0.05822313371722743 -0.029111566858613713\n",
      "62 0.00021187083124088408\n",
      "engine: -0.05240082034550397 -0.026200410172751987\n",
      "manual: -0.05240082034550397 -0.026200410172751987\n",
      "63 0.00017161537330511145\n",
      "engine: -0.04716073831095002 -0.02358036915547501\n",
      "manual: -0.04716073831095002 -0.02358036915547501\n",
      "64 0.00013900845237711933\n",
      "engine: -0.04244466447985218 -0.02122233223992609\n",
      "manual: -0.04244466447985218 -0.02122233223992609\n",
      "65 0.00011259684642545158\n",
      "engine: -0.03820019803186625 -0.019100099015933125\n",
      "manual: -0.03820019803186625 -0.019100099015933125\n",
      "66 9.120344560461239e-05\n",
      "engine: -0.034380178228680336 -0.017190089114340168\n",
      "manual: -0.034380178228680336 -0.017190089114340168\n",
      "67 7.387479093973908e-05\n",
      "engine: -0.030942160405814434 -0.015471080202907217\n",
      "manual: -0.030942160405814434 -0.015471080202907217\n",
      "68 5.9838580661196906e-05\n",
      "engine: -0.02784794436523441 -0.013923972182617206\n",
      "manual: -0.02784794436523441 -0.013923972182617206\n",
      "69 4.846925033557444e-05\n",
      "engine: -0.025063149928705286 -0.012531574964352643\n",
      "manual: -0.025063149928705286 -0.012531574964352643\n",
      "70 3.926009277179749e-05\n",
      "engine: -0.022556834935841152 -0.011278417467920576\n",
      "manual: -0.022556834935841152 -0.011278417467920576\n",
      "71 3.1800675145173994e-05\n",
      "engine: -0.020301151442254195 -0.010150575721127097\n",
      "manual: -0.020301151442254195 -0.010150575721127097\n",
      "72 2.5758546867583725e-05\n",
      "engine: -0.018271036298024512 -0.009135518149012256\n",
      "manual: -0.018271036298024512 -0.009135518149012256\n",
      "73 2.086442296273308e-05\n",
      "engine: -0.016443932668224193 -0.008221966334112096\n",
      "manual: -0.016443932668224193 -0.008221966334112096\n",
      "74 1.6900182599818178e-05\n",
      "engine: -0.014799539401401773 -0.007399769700700887\n",
      "manual: -0.014799539401401773 -0.007399769700700887\n",
      "75 1.3689147905852722e-05\n",
      "engine: -0.013319585461260885 -0.006659792730630443\n",
      "manual: -0.013319585461260885 -0.006659792730630443\n",
      "76 1.1088209803739523e-05\n",
      "engine: -0.011987626915136218 -0.005993813457568109\n",
      "manual: -0.011987626915136218 -0.005993813457568109\n",
      "77 8.981449941031142e-06\n",
      "engine: -0.010788864223620465 -0.005394432111810232\n",
      "manual: -0.010788864223620465 -0.005394432111810232\n",
      "78 7.27497445223235e-06\n",
      "engine: -0.009709977801264813 -0.0048549889006324065\n",
      "manual: -0.009709977801264813 -0.0048549889006324065\n",
      "79 5.892729306315966e-06\n",
      "engine: -0.00873898002113549 -0.004369490010567745\n",
      "manual: -0.00873898002113549 -0.004369490010567745\n",
      "80 4.773110738112827e-06\n",
      "engine: -0.007865082019023362 -0.003932541009511681\n",
      "manual: -0.007865082019023362 -0.003932541009511681\n",
      "81 3.866219697872787e-06\n",
      "engine: -0.007078573817118183 -0.0035392869085590917\n",
      "manual: -0.007078573817118183 -0.0035392869085590917\n",
      "82 3.131637955274443e-06\n",
      "engine: -0.0063707164354056545 -0.0031853582177028272\n",
      "manual: -0.0063707164354056545 -0.0031853582177028272\n",
      "83 2.536626743771733e-06\n",
      "engine: -0.005733644791867221 -0.0028668223959336103\n",
      "manual: -0.005733644791867221 -0.0028668223959336103\n",
      "84 2.0546676624566314e-06\n",
      "engine: -0.005160280312679788 -0.002580140156339894\n",
      "manual: -0.005160280312679788 -0.002580140156339894\n",
      "85 1.664280806589413e-06\n",
      "engine: -0.004644252281408967 -0.0023221261407044835\n",
      "manual: -0.004644252281408967 -0.0023221261407044835\n",
      "86 1.3480674533357747e-06\n",
      "engine: -0.004179827053263807 -0.0020899135266319036\n",
      "manual: -0.004179827053263807 -0.0020899135266319036\n",
      "87 1.0919346371997501e-06\n",
      "engine: -0.0037618443479345842 -0.0018809221739672921\n",
      "manual: -0.0037618443479345842 -0.0018809221739672921\n",
      "88 8.844670561304611e-07\n",
      "engine: -0.003385659913142547 -0.0016928299565712734\n",
      "manual: -0.003385659913142547 -0.0016928299565712734\n",
      "89 7.164183154662749e-07\n",
      "engine: -0.0030470939218290027 -0.0015235469609145014\n",
      "manual: -0.0030470939218290027 -0.0015235469609145014\n",
      "90 5.802988355279533e-07\n",
      "engine: -0.002742384529646813 -0.0013711922648234065\n",
      "manual: -0.002742384529646813 -0.0013711922648234065\n",
      "91 4.700420567778857e-07\n",
      "engine: -0.00246814607668 -0.00123407303834\n",
      "manual: -0.00246814607668 -0.00123407303834\n",
      "92 3.807340659894298e-07\n",
      "engine: -0.0022213314690162633 -0.0011106657345081317\n",
      "manual: -0.0022213314690162633 -0.0011106657345081317\n",
      "93 3.083945934526219e-07\n",
      "engine: -0.0019991983221103737 -0.0009995991610551869\n",
      "manual: -0.0019991983221103737 -0.0009995991610551869\n",
      "94 2.4979962069555833e-07\n",
      "engine: -0.0017992784899050207 -0.0008996392449525104\n",
      "manual: -0.0017992784899050207 -0.0008996392449525104\n",
      "95 2.0233769276468074e-07\n",
      "engine: -0.0016193506409152292 -0.0008096753204576146\n",
      "manual: -0.0016193506409152292 -0.0008096753204576146\n",
      "96 1.6389353113953522e-07\n",
      "engine: -0.0014574155768229957 -0.0007287077884114979\n",
      "manual: -0.0014574155768229957 -0.0007287077884114979\n",
      "97 1.3275376022289407e-07\n",
      "engine: -0.0013116740191350118 -0.0006558370095675059\n",
      "manual: -0.0013116740191350118 -0.0006558370095675059\n",
      "98 1.0753054577961221e-07\n",
      "engine: -0.0011805066172243528 -0.0005902533086121764\n",
      "manual: -0.0011805066172243528 -0.0005902533086121764\n",
      "99 8.709974208190529e-08\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w)\n",
    "    zero_grad(b)\n",
    "\n",
    "    y = w*x + b\n",
    "    l = (y-ytrue)**2\n",
    "    \n",
    "    backward(l)\n",
    "    \n",
    "    manual_dw = 2*(y.value - ytrue)*x\n",
    "    manual_db = 2*(y.value - ytrue)\n",
    "\n",
    "    print(\"engine:\", w.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw, manual_db)\n",
    "    \n",
    "    w.value -= n*w.grad\n",
    "    b.value -= n*b.grad\n",
    "    \n",
    "    print(step, l.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fbeae",
   "metadata": {},
   "source": [
    "### What This Confirms\n",
    "\n",
    "The gradients printed by the engine match the manually derived gradients:\n",
    "\n",
    "dL/dw = 2(y − y_true)x  \n",
    "dL/db = 2(y − y_true)\n",
    "\n",
    "This confirms:\n",
    "\n",
    "- Chain rule is implemented correctly\n",
    "- Gradients accumulate via +=\n",
    "- Backward traversal is correct\n",
    "- Parameter updates modify value, not graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137dcdc",
   "metadata": {},
   "source": [
    "## Phase 2 — Multi-Input Neuron\n",
    "\n",
    "Now we extend the neuron to:\n",
    "\n",
    "y = w1·x1 + w2·x2 + b\n",
    "\n",
    "This introduces multiple parents in the computational graph.\n",
    "\n",
    "Graph structure becomes branched:\n",
    "\n",
    "(w1·x1) + (w2·x2) + b\n",
    "\n",
    "Reverse-mode must:\n",
    "\n",
    "- Propagate gradients through multiple branches\n",
    "- Accumulate gradient contributions correctly\n",
    "- Handle shared nodes safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21203234",
   "metadata": {},
   "source": [
    "### Multi-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d0a5b",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e5eab5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47a78",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5df071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 3\n",
    "ytrue = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96085",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "117a12c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -46.08 -69.12 -23.04\n",
      "manual: -46.08 -69.12 -23.04\n",
      "step: 1 Loss: 132.7104\n",
      "engine: -33.1776 -49.7664 -16.5888\n",
      "manual: -33.1776 -49.7664 -16.5888\n",
      "step: 2 Loss: 68.79707135999999\n",
      "engine: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "manual: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "step: 3 Loss: 35.664401793023984\n",
      "engine: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "manual: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "step: 4 Loss: 18.488425889503635\n",
      "engine: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "manual: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "step: 5 Loss: 9.584399981118693\n",
      "engine: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "manual: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "step: 6 Loss: 4.968552950211923\n",
      "engine: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "manual: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "step: 7 Loss: 2.5756978493898646\n",
      "engine: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "manual: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "step: 8 Loss: 1.3352417651237016\n",
      "engine: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "manual: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "step: 9 Loss: 0.6921893310401275\n",
      "engine: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "manual: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "step: 10 Loss: 0.35883094921120207\n",
      "engine: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "manual: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "step: 11 Loss: 0.18601796407108673\n",
      "engine: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "manual: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "step: 12 Loss: 0.09643171257445238\n",
      "engine: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "manual: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "step: 13 Loss: 0.049990199798596015\n",
      "engine: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "manual: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "step: 14 Loss: 0.025914919575591878\n",
      "engine: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "manual: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "step: 15 Loss: 0.013434294307986566\n",
      "engine: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "manual: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "step: 16 Loss: 0.006964338169260342\n",
      "engine: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "manual: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "step: 17 Loss: 0.0036103129069445785\n",
      "engine: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "manual: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "step: 18 Loss: 0.0018715862109599897\n",
      "engine: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "manual: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "step: 19 Loss: 0.0009702302917615258\n",
      "engine: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "manual: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "step: 20 Loss: 0.00050296738324921\n",
      "engine: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "manual: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "step: 21 Loss: 0.000260738291476379\n",
      "engine: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "manual: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "step: 22 Loss: 0.00013516673030137142\n",
      "engine: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "manual: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "step: 23 Loss: 7.00704329882464e-05\n",
      "engine: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "manual: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "step: 24 Loss: 3.6324512461082954e-05\n",
      "engine: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "manual: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "step: 25 Loss: 1.883062725985562e-05\n",
      "engine: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "manual: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "step: 26 Loss: 9.761797171499829e-06\n",
      "engine: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "manual: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "step: 27 Loss: 5.060515653709667e-06\n",
      "engine: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "manual: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "step: 28 Loss: 2.6233713148759562e-06\n",
      "engine: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "manual: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "step: 29 Loss: 1.3599556896305355e-06\n",
      "engine: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "manual: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "step: 30 Loss: 7.050010295060208e-07\n",
      "engine: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "manual: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "step: 31 Loss: 3.654725336985844e-07\n",
      "engine: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "manual: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "step: 32 Loss: 1.894609614669338e-07\n",
      "engine: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "manual: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "step: 33 Loss: 9.821656242414673e-08\n",
      "engine: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "manual: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "step: 34 Loss: 5.0915465961094525e-08\n",
      "engine: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "manual: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "step: 35 Loss: 2.6394577554231402e-08\n",
      "engine: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "manual: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "step: 36 Loss: 1.368294900422992e-08\n",
      "engine: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "manual: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "step: 37 Loss: 7.0932407638047585e-09\n",
      "engine: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "manual: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "step: 38 Loss: 3.6771360120942648e-09\n",
      "engine: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "manual: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "step: 39 Loss: 1.9062273085828036e-09\n",
      "engine: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "manual: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "step: 40 Loss: 9.881882366799804e-10\n",
      "engine: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "manual: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "step: 41 Loss: 5.122767820396407e-10\n",
      "engine: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "manual: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "step: 42 Loss: 2.655642836935587e-10\n",
      "engine: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "manual: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "step: 43 Loss: 1.3766852470509082e-10\n",
      "engine: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "manual: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "step: 44 Loss: 7.136736320591857e-11\n",
      "engine: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "manual: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "step: 45 Loss: 3.699684109113444e-11\n",
      "engine: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "manual: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "step: 46 Loss: 1.9179162423511145e-11\n",
      "engine: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "manual: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "step: 47 Loss: 9.9424777931787e-12\n",
      "engine: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "manual: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "step: 48 Loss: 5.154180495404247e-12\n",
      "engine: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "manual: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "step: 49 Loss: 2.671927166726942e-12\n",
      "engine: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "manual: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "step: 50 Loss: 1.3851270464089885e-12\n",
      "engine: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "manual: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "step: 51 Loss: 7.180498583296061e-13\n",
      "engine: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "manual: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "step: 52 Loss: 3.722370470782809e-13\n",
      "engine: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "manual: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "step: 53 Loss: 1.9296768383201834e-13\n",
      "engine: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "manual: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "step: 54 Loss: 1.0003444828733929e-13\n",
      "engine: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "manual: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "step: 55 Loss: 5.1857858380493653e-14\n",
      "engine: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "manual: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "step: 56 Loss: 2.688311322524268e-14\n",
      "engine: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "manual: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "step: 57 Loss: 1.3936205963070433e-14\n",
      "engine: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "manual: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "step: 58 Loss: 7.224529207492211e-15\n",
      "engine: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "manual: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "step: 59 Loss: 3.7451960194348e-15\n",
      "engine: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "manual: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "step: 60 Loss: 1.941509647783336e-15\n",
      "engine: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "manual: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "step: 61 Loss: 1.0064784120580766e-15\n",
      "engine: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "manual: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "step: 62 Loss: 5.217584380253387e-16\n",
      "engine: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "manual: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "step: 63 Loss: 2.7047962568973864e-16\n",
      "engine: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "manual: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "step: 64 Loss: 1.4021654204074408e-16\n",
      "engine: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "manual: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "step: 65 Loss: 7.268827477921314e-17\n",
      "engine: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "manual: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "step: 66 Loss: 3.768161211360287e-17\n",
      "engine: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "manual: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "step: 67 Loss: 1.953415776903015e-17\n",
      "engine: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "manual: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "step: 68 Loss: 1.0126506030804414e-17\n",
      "engine: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "manual: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "step: 69 Loss: 5.2495735632025836e-18\n",
      "engine: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "manual: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "step: 70 Loss: 2.721383858213691e-18\n",
      "engine: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "manual: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "step: 71 Loss: 1.410763535404805e-18\n",
      "engine: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "manual: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "step: 72 Loss: 7.313403028698023e-19\n",
      "engine: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "manual: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "step: 73 Loss: 3.791296130412209e-19\n",
      "engine: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "manual: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "step: 74 Loss: 1.9653896437957275e-19\n",
      "engine: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "manual: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "step: 75 Loss: 1.0188793108506885e-19\n",
      "engine: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "manual: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "step: 76 Loss: 5.281791964122085e-20\n",
      "engine: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "manual: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "step: 77 Loss: 2.7380715482445175e-20\n",
      "engine: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "manual: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "step: 78 Loss: 1.4194399936986123e-20\n",
      "engine: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "manual: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "step: 79 Loss: 7.358559781631122e-21\n",
      "engine: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "manual: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "step: 80 Loss: 3.8143438697772514e-21\n",
      "engine: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "manual: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "step: 81 Loss: 1.977520164900858e-21\n",
      "engine: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "manual: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "step: 82 Loss: 1.0250918539904007e-21\n",
      "engine: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "manual: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "step: 83 Loss: 5.314665854129708e-22\n",
      "engine: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "manual: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "step: 84 Loss: 2.755028427684428e-22\n",
      "engine: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "manual: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "step: 85 Loss: 1.4283426045583935e-22\n",
      "engine: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "manual: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "step: 86 Loss: 7.40403893494799e-23\n",
      "engine: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "manual: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "step: 87 Loss: 3.836757225630106e-23\n",
      "engine: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "manual: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "step: 88 Loss: 1.9879610355931586e-23\n",
      "engine: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "manual: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "step: 89 Loss: 1.031471605633751e-23\n",
      "engine: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "manual: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "step: 90 Loss: 5.340907028151959e-24\n",
      "engine: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "manual: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "step: 91 Loss: 2.770381680383946e-24\n",
      "engine: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "manual: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "step: 92 Loss: 1.4334423063207214e-24\n",
      "engine: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "manual: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "step: 93 Loss: 7.483734081214486e-25\n",
      "engine: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "manual: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "step: 94 Loss: 3.8433618846729784e-25\n",
      "engine: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "manual: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "step: 95 Loss: 2.0038329170062053e-25\n",
      "engine: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "manual: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "step: 96 Loss: 1.0337548846378227e-25\n",
      "engine: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "manual: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "step: 97 Loss: 5.415056797799113e-26\n",
      "engine: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "manual: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "step: 98 Loss: 2.8477878678478526e-26\n",
      "engine: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "manual: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "step: 99 Loss: 1.4590771302967834e-26\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w1)\n",
    "    zero_grad(w2)\n",
    "    zero_grad(b)\n",
    "    \n",
    "    y = w1*x1 + w2*x2 + b\n",
    "    l = (y-ytrue)**2\n",
    "\n",
    "    backward(l)\n",
    "\n",
    "    delta = 2*(y.value - ytrue)\n",
    "\n",
    "    manual_dw1 = delta * x1\n",
    "    manual_dw2 = delta * x2\n",
    "    manual_db = delta\n",
    "    \n",
    "    print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "    w1.value -= n * w1.grad\n",
    "    w2.value -= n * w2.grad\n",
    "    b.value -= n * b.grad\n",
    "\n",
    "    print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21723baf",
   "metadata": {},
   "source": [
    "Now for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "afbe8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "375d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "83bde1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -6.16 -6.16 -6.16\n",
      "manual: -6.16 -6.16 -6.16\n",
      "step: 0 Loss: 9.4864\n",
      "engine: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "manual: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "step: 1 Loss: 124.33142016000002\n",
      "engine: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "manual: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "step: 1 Loss: 2.4247223511039984\n",
      "engine: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "manual: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "step: 2 Loss: 61.48802068101364\n",
      "engine: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "manual: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "step: 2 Loss: 0.2732689308521132\n",
      "engine: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "manual: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "step: 3 Loss: 31.170997460282468\n",
      "engine: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "manual: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "step: 3 Loss: 0.03189284586729491\n",
      "engine: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "manual: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "step: 4 Loss: 16.33179658342692\n",
      "engine: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "manual: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "step: 4 Loss: 0.4261765009868418\n",
      "engine: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "manual: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "step: 5 Loss: 8.92842546475222\n",
      "engine: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "manual: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "step: 5 Loss: 0.9452083970312428\n",
      "engine: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "manual: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "step: 6 Loss: 5.144096642410019\n",
      "engine: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "manual: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "step: 6 Loss: 1.4067203007232514\n",
      "engine: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "manual: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "step: 7 Loss: 3.1517950617409176\n",
      "engine: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "manual: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "step: 7 Loss: 1.7633949131753535\n",
      "engine: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "manual: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "step: 8 Loss: 2.0666613938848766\n",
      "engine: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "manual: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "step: 8 Loss: 2.0185695667441066\n",
      "engine: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "manual: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "step: 9 Loss: 1.4533641651357307\n",
      "engine: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "manual: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "step: 9 Loss: 2.19094635544156\n",
      "engine: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "manual: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "step: 10 Loss: 1.0933256565007086\n",
      "engine: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "manual: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "step: 10 Loss: 2.300828311885715\n",
      "engine: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "manual: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "step: 11 Loss: 0.8739813892243515\n",
      "engine: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "manual: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "step: 11 Loss: 2.3655105271774577\n",
      "engine: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "manual: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "step: 12 Loss: 0.7355957569813151\n",
      "engine: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "manual: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "step: 12 Loss: 2.39834960772937\n",
      "engine: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "manual: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "step: 13 Loss: 0.6453885758378625\n",
      "engine: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "manual: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "step: 13 Loss: 2.409151760465584\n",
      "engine: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "manual: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "step: 14 Loss: 0.5847310457140027\n",
      "engine: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "manual: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "step: 14 Loss: 2.404908812355005\n",
      "engine: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "manual: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "step: 15 Loss: 0.5426690205794957\n",
      "engine: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "manual: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "step: 15 Loss: 2.3905164858697434\n",
      "engine: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "manual: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "step: 16 Loss: 0.5125575727467586\n",
      "engine: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "manual: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "step: 16 Loss: 2.369362902444438\n",
      "engine: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "manual: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "step: 17 Loss: 0.4902566940029108\n",
      "engine: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "manual: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "step: 17 Loss: 2.343774490634568\n",
      "engine: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "manual: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "step: 18 Loss: 0.4731303138646301\n",
      "engine: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "manual: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "step: 18 Loss: 2.3153399170214897\n",
      "engine: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "manual: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "step: 19 Loss: 0.45947122411636654\n",
      "engine: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "manual: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "step: 19 Loss: 2.2851396186686763\n",
      "engine: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "manual: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "step: 20 Loss: 0.44815948369410363\n",
      "engine: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "manual: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "step: 20 Loss: 2.2539059385445808\n",
      "engine: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "manual: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "step: 21 Loss: 0.43845337086104164\n",
      "engine: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "manual: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "step: 21 Loss: 2.222133731465707\n",
      "engine: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "manual: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "step: 22 Loss: 0.42985819753731525\n",
      "engine: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "manual: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "step: 22 Loss: 2.1901562572347317\n",
      "engine: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "manual: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "step: 23 Loss: 0.42204231181996454\n",
      "engine: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "manual: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "step: 23 Loss: 2.15819702616437\n",
      "engine: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "manual: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "step: 24 Loss: 0.41478247554885156\n",
      "engine: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "manual: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "step: 24 Loss: 2.1264051115482006\n",
      "engine: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "manual: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "step: 25 Loss: 0.4079279314824542\n",
      "engine: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "manual: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "step: 25 Loss: 2.0948791527557025\n",
      "engine: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "manual: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "step: 26 Loss: 0.4013765655318351\n",
      "engine: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "manual: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "step: 26 Loss: 2.0636836484305476\n",
      "engine: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "manual: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "step: 27 Loss: 0.39505899726057586\n",
      "engine: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "manual: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "step: 27 Loss: 2.0328600057264787\n",
      "engine: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "manual: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "step: 28 Loss: 0.38892791640667984\n",
      "engine: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "manual: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "step: 28 Loss: 2.002434028386879\n",
      "engine: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "manual: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "step: 29 Loss: 0.38295091418671107\n",
      "engine: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "manual: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "step: 29 Loss: 1.9724209890373758\n",
      "engine: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "manual: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "step: 30 Loss: 0.37710565395421897\n",
      "engine: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "manual: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "step: 30 Loss: 1.9428290638754178\n",
      "engine: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "manual: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "step: 31 Loss: 0.3713766130825078\n",
      "engine: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "manual: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "step: 31 Loss: 1.9136616578256025\n",
      "engine: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "manual: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "step: 32 Loss: 0.3657528826485063\n",
      "engine: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "manual: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "step: 32 Loss: 1.884918978206111\n",
      "engine: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "manual: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "step: 33 Loss: 0.3602266804314682\n",
      "engine: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "manual: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "step: 33 Loss: 1.8565990995326203\n",
      "engine: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "manual: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "step: 34 Loss: 0.3547923454722424\n",
      "engine: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "manual: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "step: 34 Loss: 1.8286986838094759\n",
      "engine: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "manual: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "step: 35 Loss: 0.3494456579904879\n",
      "engine: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "manual: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "step: 35 Loss: 1.8012134676049756\n",
      "engine: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "manual: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "step: 36 Loss: 0.3441833792432918\n",
      "engine: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "manual: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "step: 36 Loss: 1.7741385912669188\n",
      "engine: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "manual: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "step: 37 Loss: 0.33900294011960047\n",
      "engine: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "manual: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "step: 37 Loss: 1.7474688212932457\n",
      "engine: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "manual: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "step: 38 Loss: 0.33390223034363575\n",
      "engine: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "manual: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "step: 38 Loss: 1.7211987003911826\n",
      "engine: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "manual: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "step: 39 Loss: 0.3288794557454377\n",
      "engine: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "manual: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "step: 39 Loss: 1.6953226486000565\n",
      "engine: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "manual: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "step: 40 Loss: 0.3239330415882179\n",
      "engine: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "manual: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "step: 40 Loss: 1.6698350312993278\n",
      "engine: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "manual: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "step: 41 Loss: 0.3190615670625761\n",
      "engine: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "manual: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "step: 41 Loss: 1.6447302048105017\n",
      "engine: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "manual: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "step: 42 Loss: 0.3142637208729977\n",
      "engine: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "manual: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "step: 42 Loss: 1.6200025468407362\n",
      "engine: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "manual: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "step: 43 Loss: 0.30953827109960846\n",
      "engine: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "manual: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "step: 43 Loss: 1.5956464766734495\n",
      "engine: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "manual: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "step: 44 Loss: 0.3048840447220777\n",
      "engine: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "manual: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "step: 44 Loss: 1.5716564684259073\n",
      "engine: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "manual: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "step: 45 Loss: 0.300299913683752\n",
      "engine: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "manual: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "step: 45 Loss: 1.5480270596206327\n",
      "engine: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "manual: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "step: 46 Loss: 0.2957847853833086\n",
      "engine: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "manual: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "step: 46 Loss: 1.5247528565912059\n",
      "engine: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "manual: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "step: 47 Loss: 0.29133759616397464\n",
      "engine: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "manual: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "step: 47 Loss: 1.5018285377515561\n",
      "engine: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "manual: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "step: 48 Loss: 0.2869573068327053\n",
      "engine: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "manual: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "step: 48 Loss: 1.4792488554250944\n",
      "engine: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "manual: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "step: 49 Loss: 0.2826428995542664\n",
      "engine: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "manual: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "step: 49 Loss: 1.4570086367048911\n",
      "engine: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "manual: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "step: 50 Loss: 0.2783933756769821\n",
      "engine: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "manual: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "step: 50 Loss: 1.4351027836637926\n",
      "engine: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "manual: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "step: 51 Loss: 0.2742077541900975\n",
      "engine: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "manual: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "step: 51 Loss: 1.4135262731301397\n",
      "engine: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "manual: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "step: 52 Loss: 0.27008507060972936\n",
      "engine: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "manual: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "step: 52 Loss: 1.3922741561749867\n",
      "engine: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "manual: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "step: 53 Loss: 0.26602437615586866\n",
      "engine: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "manual: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "step: 53 Loss: 1.3713415574095253\n",
      "engine: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "manual: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "step: 54 Loss: 0.2620247371274635\n",
      "engine: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "manual: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "step: 54 Loss: 1.3507236741593498\n",
      "engine: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "manual: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "step: 55 Loss: 0.2580852344125613\n",
      "engine: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "manual: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "step: 55 Loss: 1.3304157755607073\n",
      "engine: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "manual: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "step: 56 Loss: 0.2542049630908597\n",
      "engine: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "manual: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "step: 56 Loss: 1.3104132016090455\n",
      "engine: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "manual: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "step: 57 Loss: 0.250383032099792\n",
      "engine: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "manual: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "step: 57 Loss: 1.2907113621804505\n",
      "engine: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "manual: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "step: 58 Loss: 0.24661856394460346\n",
      "engine: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "manual: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "step: 58 Loss: 1.2713057360397508\n",
      "engine: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "manual: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "step: 59 Loss: 0.2429106944391491\n",
      "engine: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "manual: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "step: 59 Loss: 1.252191869844529\n",
      "engine: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "manual: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "step: 60 Loss: 0.23925857246842042\n",
      "engine: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "manual: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "step: 60 Loss: 1.2333653771512865\n",
      "engine: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "manual: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "step: 61 Loss: 0.23566135976675504\n",
      "engine: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "manual: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "step: 61 Loss: 1.2148219374278124\n",
      "engine: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "manual: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "step: 62 Loss: 0.2321182307075031\n",
      "engine: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "manual: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "step: 62 Loss: 1.1965572950745806\n",
      "engine: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "manual: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "step: 63 Loss: 0.22862837210144146\n",
      "engine: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "manual: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "step: 63 Loss: 1.1785672584568012\n",
      "engine: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "manual: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "step: 64 Loss: 0.22519098300191842\n",
      "engine: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "manual: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "step: 64 Loss: 1.1608476989483916\n",
      "engine: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "manual: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "step: 65 Loss: 0.22180527451550977\n",
      "engine: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "manual: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "step: 65 Loss: 1.1433945499884604\n",
      "engine: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "manual: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "step: 66 Loss: 0.2184704696171666\n",
      "engine: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "manual: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "step: 66 Loss: 1.1262038061507598\n",
      "engine: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "manual: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "step: 67 Loss: 0.21518580296939233\n",
      "engine: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "manual: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "step: 67 Loss: 1.1092715222262894\n",
      "engine: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "manual: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "step: 68 Loss: 0.2119505207448405\n",
      "engine: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "manual: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "step: 68 Loss: 1.092593812319109\n",
      "engine: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "manual: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "step: 69 Loss: 0.2087638804521744\n",
      "engine: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "manual: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "step: 69 Loss: 1.0761668489553933\n",
      "engine: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "manual: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "step: 70 Loss: 0.20562515076485635\n",
      "engine: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "manual: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "step: 70 Loss: 1.0599868622056043\n",
      "engine: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "manual: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "step: 71 Loss: 0.202533611352796\n",
      "engine: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "manual: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "step: 71 Loss: 1.0440501388196815\n",
      "engine: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "manual: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "step: 72 Loss: 0.1994885527166665\n",
      "engine: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "manual: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "step: 72 Loss: 1.0283530213751464\n",
      "engine: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "manual: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "step: 73 Loss: 0.19648927602481972\n",
      "engine: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "manual: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "step: 73 Loss: 1.0128919074378824\n",
      "engine: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "manual: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "step: 74 Loss: 0.19353509295277788\n",
      "engine: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "manual: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "step: 74 Loss: 0.997663248735532\n",
      "engine: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "manual: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "step: 75 Loss: 0.19062532552510686\n",
      "engine: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "manual: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "step: 75 Loss: 0.9826635503432468\n",
      "engine: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "manual: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "step: 76 Loss: 0.18775930595978385\n",
      "engine: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "manual: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "step: 76 Loss: 0.967889369881674\n",
      "engine: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "manual: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "step: 77 Loss: 0.184936376514874\n",
      "engine: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "manual: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "step: 77 Loss: 0.9533373167269823\n",
      "engine: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "manual: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "step: 78 Loss: 0.1821558893375515\n",
      "engine: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "manual: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "step: 78 Loss: 0.9390040512327443\n",
      "engine: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "manual: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "step: 79 Loss: 0.179417206315393\n",
      "engine: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "manual: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "step: 79 Loss: 0.9248862839635181\n",
      "engine: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "manual: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "step: 80 Loss: 0.17671969892992428\n",
      "engine: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "manual: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "step: 80 Loss: 0.9109807749399496\n",
      "engine: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "manual: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "step: 81 Loss: 0.1740627481123579\n",
      "engine: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "manual: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "step: 81 Loss: 0.8972843328952061\n",
      "engine: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "manual: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "step: 82 Loss: 0.1714457441015169\n",
      "engine: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "manual: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "step: 82 Loss: 0.883793814542602\n",
      "engine: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "manual: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "step: 83 Loss: 0.16886808630389047\n",
      "engine: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "manual: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "step: 83 Loss: 0.8705061238542326\n",
      "engine: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "manual: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "step: 84 Loss: 0.1663291831557962\n",
      "engine: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "manual: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "step: 84 Loss: 0.8574182113504414\n",
      "engine: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "manual: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "step: 85 Loss: 0.16382845198761636\n",
      "engine: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "manual: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "step: 85 Loss: 0.8445270733999863\n",
      "engine: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "manual: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "step: 86 Loss: 0.16136531889007713\n",
      "engine: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "manual: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "step: 86 Loss: 0.8318297515306993\n",
      "engine: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "manual: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "step: 87 Loss: 0.15893921858253074\n",
      "engine: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "manual: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "step: 87 Loss: 0.8193233317505548\n",
      "engine: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "manual: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "step: 88 Loss: 0.15654959428323653\n",
      "engine: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "manual: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "step: 88 Loss: 0.8070049438788954\n",
      "engine: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "manual: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "step: 89 Loss: 0.15419589758156857\n",
      "engine: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "manual: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "step: 89 Loss: 0.7948717608877401\n",
      "engine: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "manual: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "step: 90 Loss: 0.1518775883121652\n",
      "engine: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "manual: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "step: 90 Loss: 0.782920998253007\n",
      "engine: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "manual: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "step: 91 Loss: 0.14959413443096498\n",
      "engine: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "manual: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "step: 91 Loss: 0.7711499133154561\n",
      "engine: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "manual: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "step: 92 Loss: 0.1473450118930849\n",
      "engine: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "manual: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "step: 92 Loss: 0.7595558046512645\n",
      "engine: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "manual: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "step: 93 Loss: 0.1451297045325846\n",
      "engine: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "manual: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "step: 93 Loss: 0.7481360114520615\n",
      "engine: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "manual: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "step: 94 Loss: 0.14294770394398368\n",
      "engine: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "manual: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "step: 94 Loss: 0.7368879129142843\n",
      "engine: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "manual: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "step: 95 Loss: 0.14079850936559313\n",
      "engine: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "manual: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "step: 95 Loss: 0.725808927637705\n",
      "engine: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "manual: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "step: 96 Loss: 0.13868162756459101\n",
      "engine: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "manual: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "step: 96 Loss: 0.7148965130330096\n",
      "engine: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "manual: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "step: 97 Loss: 0.13659657272382797\n",
      "engine: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "manual: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "step: 97 Loss: 0.7041481647382901\n",
      "engine: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "manual: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "step: 98 Loss: 0.13454286633033774\n",
      "engine: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "manual: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "step: 98 Loss: 0.6935614160442927\n",
      "engine: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "manual: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "step: 99 Loss: 0.13252003706550594\n",
      "engine: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "manual: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "step: 99 Loss: 0.6831338373283254\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        \n",
    "        zero_grad(w1)\n",
    "        zero_grad(w2)\n",
    "        zero_grad(b)\n",
    "\n",
    "        y = w1*x1 + w2*x2 + b\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "        \n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        w1.value -= n * w1.grad\n",
    "        w2.value -= n * w2.grad\n",
    "        b.value -= n * b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fe5c5",
   "metadata": {},
   "source": [
    "### Observations with Dataset Training\n",
    "\n",
    "Training over multiple examples introduces SGD dynamics.\n",
    "\n",
    "We observe:\n",
    "\n",
    "- Loss oscillates because updates happen per sample\n",
    "- Gradients must be zeroed before backward\n",
    "- Parameters must persist across steps\n",
    "\n",
    "This validates the full training lifecycle:\n",
    "\n",
    "1. zero_grad  \n",
    "2. forward  \n",
    "3. backward  \n",
    "4. update  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e5b5a",
   "metadata": {},
   "source": [
    "## Class Functions of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c765068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4405ab",
   "metadata": {},
   "source": [
    "### Abstraction — Turning Neuron into a Class\n",
    "\n",
    "To scale the system, we abstract the neuron into a reusable class.\n",
    "\n",
    "This introduces:\n",
    "\n",
    "- Weight vector abstraction\n",
    "- Dimension safety checks\n",
    "- Dot-product computation\n",
    "- Random initialization to break symmetry\n",
    "\n",
    "Now the neuron represents a hyperplane in ℝⁿ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d95f",
   "metadata": {},
   "source": [
    "#### For a single input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "11203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron for a single input\n",
    "class neuron:\n",
    "    def __init__(self):\n",
    "        self.w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.b = Node(0)\n",
    "    def pred(self, x):\n",
    "        y = self.w * x + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39690",
   "metadata": {},
   "source": [
    "#### For multi-input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4ee2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated __init___ and pred for multi-inputs\n",
    "def __init__(self, dim=1):\n",
    "    self.w = []\n",
    "    self.b = Node(0)\n",
    "    self.dim = dim\n",
    "    for i in range (0, self.dim):\n",
    "        w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.w.append(w)\n",
    "        i += 1\n",
    "\n",
    "neuron.__init__ = __init__\n",
    "\n",
    "def pred(self, x):\n",
    "    if len(x) != len(self.w):\n",
    "        raise ValueError(\"Input dimension does not match neuron weight dimension\")\n",
    "    else:\n",
    "        wx = Node(0)\n",
    "        for i in range(0, self.dim):\n",
    "            wx += x[i]*self.w[i]\n",
    "        y = wx + self.b\n",
    "        return y\n",
    "\n",
    "neuron.pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd57a4a",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e04fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ba23d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64.37048576725331 -96.55572865087997 -32.18524288362666\n",
      "manual: -64.37048576725331 -96.55572865087997 -32.18524288362666\n",
      "step: 0 Loss: 258.9724648695101\n",
      "engine: -6.237448499825584 -6.237448499825584 -6.237448499825584\n",
      "manual: -6.237448499825584 -6.237448499825584 -6.237448499825584\n",
      "step: 0 Loss: 9.726440946994108\n",
      "engine: -44.84976211246424 -67.27464316869636 -22.42488105623212\n",
      "manual: -44.84976211246424 -67.27464316869636 -22.42488105623212\n",
      "step: 1 Loss: 125.71882259653955\n",
      "engine: -3.1722158630881934 -3.1722158630881934 -3.1722158630881934\n",
      "manual: -3.1722158630881934 -3.1722158630881934 -3.1722158630881934\n",
      "step: 1 Loss: 2.515738370507093\n",
      "engine: -31.530496913833083 -47.29574537074963 -15.765248456916542\n",
      "manual: -31.530496913833083 -47.29574537074963 -15.765248456916542\n",
      "step: 2 Loss: 62.13576472707735\n",
      "engine: -1.0900530964729178 -1.0900530964729178 -1.0900530964729178\n",
      "manual: -1.0900530964729178 -1.0900530964729178 -1.0900530964729178\n",
      "step: 2 Loss: 0.29705393828254906\n",
      "engine: -22.440345034806327 -33.66051755220949 -11.220172517403164\n",
      "manual: -22.440345034806327 -33.66051755220949 -11.220172517403164\n",
      "step: 3 Loss: 31.473067830072313\n",
      "engine: 0.3217707914038357 0.3217707914038357 0.3217707914038357\n",
      "manual: 0.3217707914038357 0.3217707914038357 0.3217707914038357\n",
      "step: 3 Loss: 0.025884110550162685\n",
      "engine: -16.234273414997467 -24.3514101224962 -8.117136707498734\n",
      "manual: -16.234273414997467 -24.3514101224962 -8.117136707498734\n",
      "step: 4 Loss: 16.471977082055847\n",
      "engine: 1.2765209488194564 1.2765209488194564 1.2765209488194564\n",
      "manual: 1.2765209488194564 1.2765209488194564 1.2765209488194564\n",
      "step: 4 Loss: 0.40737643319373135\n",
      "engine: -11.995041886514848 -17.99256282977227 -5.997520943257424\n",
      "manual: -11.995041886514848 -17.99256282977227 -5.997520943257424\n",
      "step: 5 Loss: 8.992564366202854\n",
      "engine: 1.9196322050811787 1.9196322050811787 1.9196322050811787\n",
      "manual: 1.9196322050811787 1.9196322050811787 1.9196322050811787\n",
      "step: 5 Loss: 0.9212469506962071\n",
      "engine: -9.097141887510176 -13.645712831265264 -4.548570943755088\n",
      "manual: -9.097141887510176 -13.645712831265264 -4.548570943755088\n",
      "step: 6 Loss: 5.172374407593263\n",
      "engine: 2.350282786026918 2.350282786026918 2.350282786026918\n",
      "manual: 2.350282786026918 2.350282786026918 2.350282786026918\n",
      "step: 6 Loss: 1.3809572935736132\n",
      "engine: -7.114010027653791 -10.671015041480686 -3.5570050138268954\n",
      "manual: -7.114010027653791 -10.671015041480686 -3.5570050138268954\n",
      "step: 7 Loss: 3.163071167097418\n",
      "engine: 2.6361064205245306 2.6361064205245306 2.6361064205245306\n",
      "manual: 2.6361064205245306 2.6361064205245306 2.6361064205245306\n",
      "step: 7 Loss: 1.7372642650826633\n",
      "engine: -5.754752760836617 -8.632129141254925 -2.8773763804183083\n",
      "manual: -5.754752760836617 -8.632129141254925 -2.8773763804183083\n",
      "step: 8 Loss: 2.0698237086472915\n",
      "engine: 2.8232252009432557 2.8232252009432557 2.8232252009432557\n",
      "manual: 2.8232252009432557 2.8232252009432557 2.8232252009432557\n",
      "step: 8 Loss: 1.9926501338102716\n",
      "engine: -4.820996036028738 -7.231494054043107 -2.410498018014369\n",
      "manual: -4.820996036028738 -7.231494054043107 -2.410498018014369\n",
      "step: 9 Loss: 1.4526251737128004\n",
      "engine: 2.9430914510483834 2.9430914510483834 2.9430914510483834\n",
      "manual: 2.9430914510483834 2.9430914510483834 2.9430914510483834\n",
      "step: 9 Loss: 2.1654468223085197\n",
      "engine: -4.177459094192308 -6.266188641288462 -2.088729547096154\n",
      "manual: -4.177459094192308 -6.266188641288462 -2.088729547096154\n",
      "step: 10 Loss: 1.0906977802281261\n",
      "engine: 3.0171535096370192 3.0171535096370192 3.0171535096370192\n",
      "manual: 3.0171535096370192 3.0171535096370192 3.0171535096370192\n",
      "step: 10 Loss: 2.2758038251787456\n",
      "engine: -3.731887390131341 -5.5978310851970114 -1.8659436950656705\n",
      "manual: -3.731887390131341 -5.5978310851970114 -1.8659436950656705\n",
      "step: 11 Loss: 0.8704364682888319\n",
      "engine: 3.060037542466681 3.060037542466681 3.060037542466681\n",
      "manual: 3.060037542466681 3.060037542466681 3.060037542466681\n",
      "step: 11 Loss: 2.3409574403263806\n",
      "engine: -3.4213679310865643 -5.1320518966298465 -1.7106839655432822\n",
      "manual: -3.4213679310865643 -5.1320518966298465 -1.7106839655432822\n",
      "step: 12 Loss: 0.7316099074917224\n",
      "engine: 3.0817173657838737 3.0817173657838737 3.0817173657838737\n",
      "manual: 3.0817173657838737 3.0817173657838737 3.0817173657838737\n",
      "step: 12 Loss: 2.3742454806434745\n",
      "engine: -3.202997078170462 -4.804495617255693 -1.601498539085231\n",
      "manual: -3.202997078170462 -4.804495617255693 -1.601498539085231\n",
      "step: 13 Loss: 0.6411993926730323\n",
      "engine: 3.0889941485270693 3.0889941485270693 3.0889941485270693\n",
      "manual: 3.0889941485270693 3.0889941485270693 3.0889941485270693\n",
      "step: 13 Loss: 2.3854712124086186\n",
      "engine: -3.0475164919292297 -4.5712747378938445 -1.5237582459646148\n",
      "manual: -3.0475164919292297 -4.5712747378938445 -1.5237582459646148\n",
      "step: 14 Loss: 0.5804597980362899\n",
      "engine: 3.0865054891311985 3.0865054891311985 3.0865054891311985\n",
      "manual: 3.0865054891311985 3.0865054891311985 3.0865054891311985\n",
      "step: 14 Loss: 2.3816290336092547\n",
      "engine: -2.934973191580532 -4.402459787370798 -1.467486595790266\n",
      "manual: -2.934973191580532 -4.402459787370798 -1.467486595790266\n",
      "step: 15 Loss: 0.5383792272060259\n",
      "engine: 3.077413551278159 3.077413551278159 3.077413551278159\n",
      "manual: 3.077413551278159 3.077413551278159 3.077413551278159\n",
      "step: 15 Loss: 2.3676185413976127\n",
      "engine: -2.851759950244734 -4.277639925367101 -1.425879975122367\n",
      "manual: -2.851759950244734 -4.277639925367101 -1.425879975122367\n",
      "step: 16 Loss: 0.5082834258637404\n",
      "engine: 3.063874335216152 3.063874335216152 3.063874335216152\n",
      "manual: 3.063874335216152 3.063874335216152 3.063874335216152\n",
      "step: 16 Loss: 2.346831485499054\n",
      "engine: -2.7885970046280875 -4.182895506942131 -1.3942985023140437\n",
      "manual: -2.7885970046280875 -4.182895506942131 -1.3942985023140437\n",
      "step: 17 Loss: 0.4860170783887964\n",
      "engine: 3.047357695380871 3.047357695380871 3.047357695380871\n",
      "manual: 3.047357695380871 3.047357695380871 3.047357695380871\n",
      "step: 17 Loss: 2.321597230899253\n",
      "engine: -2.739155690223633 -4.10873353533545 -1.3695778451118166\n",
      "manual: -2.739155690223633 -4.10873353533545 -1.3695778451118166\n",
      "step: 18 Loss: 0.46893586845528173\n",
      "engine: 3.0288655750714373 3.0288655750714373 3.0288655750714373\n",
      "manual: 3.0288655750714373 3.0288655750714373 3.0288655750714373\n",
      "step: 18 Loss: 2.2935066679632072\n",
      "engine: -2.6991198349781556 -4.048679752467233 -1.3495599174890778\n",
      "manual: -2.6991198349781556 -4.048679752467233 -1.3495599174890778\n",
      "step: 19 Loss: 0.4553279927232816\n",
      "engine: 3.009080830665841 3.009080830665841 3.009080830665841\n",
      "manual: 3.009080830665841 3.009080830665841 3.009080830665841\n",
      "step: 19 Loss: 2.2636418613701568\n",
      "engine: -2.6655456805440707 -3.998318520816106 -1.3327728402720354\n",
      "manual: -2.6655456805440707 -3.998318520816106 -1.3327728402720354\n",
      "step: 20 Loss: 0.44407086094169707\n",
      "engine: 2.988468721658535 2.988468721658535 2.988468721658535\n",
      "manual: 2.988468721658535 2.988468721658535 2.988468721658535\n",
      "step: 20 Loss: 2.232736325082849\n",
      "engine: -2.6364253831897813 -3.954638074784672 -1.3182126915948906\n",
      "manual: -2.6364253831897813 -3.954638074784672 -1.3182126915948906\n",
      "step: 21 Loss: 0.4344211750704616\n",
      "engine: 2.967346121350408 2.967346121350408 2.967346121350408\n",
      "manual: 2.967346121350408 2.967346121350408 2.967346121350408\n",
      "step: 21 Loss: 2.2012857509733275\n",
      "engine: -2.6103893450207494 -3.915584017531124 -1.3051946725103747\n",
      "manual: -2.6103893450207494 -3.915584017531124 -1.3051946725103747\n",
      "step: 22 Loss: 0.4258832832873661\n",
      "engine: 2.9459287147706306 2.9459287147706306 2.9459287147706306\n",
      "manual: 2.9459287147706306 2.9459287147706306 2.9459287147706306\n",
      "step: 22 Loss: 2.169623998127535\n",
      "engine: -2.58650321995988 -3.87975482993982 -1.29325160997994\n",
      "manual: -2.58650321995988 -3.87975482993982 -1.29325160997994\n",
      "step: 23 Loss: 0.4181249316789267\n",
      "engine: 2.924363185081985 2.924363185081985 2.924363185081985\n",
      "manual: 2.924363185081985 2.924363185081985 2.924363185081985\n",
      "step: 23 Loss: 2.137975009565713\n",
      "engine: -2.56412948279079 -3.846194224186185 -1.282064741395395\n",
      "manual: -2.56412948279079 -3.846194224186185 -1.282064741395395\n",
      "step: 24 Loss: 0.41092250028231025\n",
      "engine: 2.902749162944515 2.902749162944515 2.902749162944515\n",
      "manual: 2.902749162944515 2.902749162944515 2.902749162944515\n",
      "step: 24 Loss: 2.1064881757437703\n",
      "engine: -2.5428330267160533 -3.81424954007408 -1.2714165133580266\n",
      "manual: -2.5428330267160533 -3.81424954007408 -1.2714165133580266\n",
      "step: 25 Loss: 0.40412498760987026\n",
      "engine: 2.881154194770806 2.881154194770806 2.881154194770806\n",
      "manual: 2.881154194770806 2.881154194770806 2.881154194770806\n",
      "step: 25 Loss: 2.075262373511353\n",
      "engine: -2.52231678598055 -3.783475178970825 -1.261158392990275\n",
      "manual: -2.52231678598055 -3.783475178970825 -1.261158392990275\n",
      "step: 26 Loss: 0.3976301230524532\n",
      "engine: 2.8596239502433907 2.8596239502433907 2.8596239502433907\n",
      "manual: 2.8596239502433907 2.8596239502433907 2.8596239502433907\n",
      "step: 26 Loss: 2.0443622842014033\n",
      "engine: -2.5023778339644096 -3.7535667509466144 -1.2511889169822048\n",
      "manual: -2.5023778339644096 -3.7535667509466144 -1.2511889169822048\n",
      "step: 27 Loss: 0.39136842649477566\n",
      "engine: 2.8381891832666533 2.8381891832666533 2.8381891832666533\n",
      "manual: 2.8381891832666533 2.8381891832666533 2.8381891832666533\n",
      "step: 27 Loss: 2.0138294600029583\n",
      "engine: -2.4828774444383583 -3.7243161666575375 -1.2414387222191792\n",
      "manual: -2.4828774444383583 -3.7243161666575375 -1.2414387222191792\n",
      "step: 28 Loss: 0.3852925252562971\n",
      "engine: 2.816870478936954 2.816870478936954 2.816870478936954\n",
      "manual: 2.816870478936954 2.816870478936954 2.816870478936954\n",
      "step: 28 Loss: 1.983689823776626\n",
      "engine: -2.4637206749404967 -3.695581012410745 -1.2318603374702484\n",
      "manual: -2.4637206749404967 -3.695581012410745 -1.2318603374702484\n",
      "step: 29 Loss: 0.37936997275807854\n",
      "engine: 2.7956814906971683 2.7956814906971683 2.7956814906971683\n",
      "manual: 2.7956814906971683 2.7956814906971683 2.7956814906971683\n",
      "step: 29 Loss: 1.9539587493566852\n",
      "engine: -2.4448424437244753 -3.667263665586713 -1.2224212218622377\n",
      "manual: -2.4448424437244753 -3.667263665586713 -1.2224212218622377\n",
      "step: 30 Loss: 0.37357841091479155\n",
      "engine: 2.7746311478788055 2.7746311478788055 2.7746311478788055\n",
      "manual: 2.7746311478788055 2.7746311478788055 2.7746311478788055\n",
      "step: 30 Loss: 1.9246445016948144\n",
      "engine: -2.426198034972529 -3.6392970524587938 -1.2130990174862646\n",
      "manual: -2.426198034972529 -3.6392970524587938 -1.2130990174862646\n",
      "step: 31 Loss: 0.3679023065565351\n",
      "engine: 2.7537251611044304 2.7537251611044304 2.7537251611044304\n",
      "manual: 2.7537251611044304 2.7537251611044304 2.7537251611044304\n",
      "step: 31 Loss: 1.8957505657249054\n",
      "engine: -2.4077566238452874 -3.611634935767931 -1.2038783119226437\n",
      "manual: -2.4077566238452874 -3.611634935767931 -1.2038783119226437\n",
      "step: 32 Loss: 0.36233074747942856\n",
      "engine: 2.7329670488688826 2.7329670488688826 2.7329670488688826\n",
      "manual: 2.7329670488688826 2.7329670488688826 2.7329670488688826\n",
      "step: 32 Loss: 1.8672772225507723\n",
      "engine: -2.3894968608971396 -3.5842452913457095 -1.1947484304485698\n",
      "manual: -2.3894968608971396 -3.5842452913457095 -1.1947484304485698\n",
      "step: 33 Loss: 0.35685595301483025\n",
      "engine: 2.7123588375905765 2.7123588375905765 2.7123588375905765\n",
      "manual: 2.7123588375905765 2.7123588375905765 2.7123588375905765\n",
      "step: 33 Loss: 1.8392226159639258\n",
      "engine: -2.371403860867673 -3.5571057913015096 -1.1857019304338365\n",
      "manual: -2.371403860867673 -3.5571057913015096 -1.1857019304338365\n",
      "step: 34 Loss: 0.35147226695863165\n",
      "engine: 2.691901538987203 2.691901538987203 2.691901538987203\n",
      "manual: 2.691901538987203 2.691901538987203 2.691901538987203\n",
      "step: 34 Loss: 1.811583473900418\n",
      "engine: -2.353467149181654 -3.530200723772481 -1.176733574590827\n",
      "manual: -2.353467149181654 -3.530200723772481 -1.176733574590827\n",
      "step: 35 Loss: 0.3461754763923263\n",
      "engine: 2.6715954755988705 2.6715954755988705 2.6715954755988705\n",
      "manual: 2.6715954755988705 2.6715954755988705 2.6715954755988705\n",
      "step: 35 Loss: 1.7843555963100888\n",
      "engine: -2.3356792615545245 -3.503518892331787 -1.1678396307772623\n",
      "manual: -2.3356792615545245 -3.503518892331787 -1.1678396307772623\n",
      "step: 36 Loss: 0.3409623508034931\n",
      "engine: 2.651440502756211 2.651440502756211 2.651440502756211\n",
      "manual: 2.651440502756211 2.651440502756211 2.651440502756211\n",
      "step: 36 Loss: 1.7575341849140271\n",
      "engine: -2.3180347889807393 -3.477052183471109 -1.1590173944903697\n",
      "manual: -2.3180347889807393 -3.477052183471109 -1.1590173944903697\n",
      "step: 37 Loss: 0.3358303301828113\n",
      "engine: 2.631436159929681 2.631436159929681 2.631436159929681\n",
      "manual: 2.631436159929681 2.631436159929681 2.631436159929681\n",
      "step: 37 Loss: 1.7311140659463664\n",
      "engine: -2.3005297264492697 -3.4507945896739045 -1.1502648632246348\n",
      "manual: -2.3005297264492697 -3.4507945896739045 -1.1502648632246348\n",
      "step: 38 Loss: 0.330777313892297\n",
      "engine: 2.6115817739208556 2.6115817739208556 2.6115817739208556\n",
      "manual: 2.6115817739208556 2.6115817739208556 2.6115817739208556\n",
      "step: 38 Loss: 1.7050898404689008\n",
      "engine: -2.283161028784477 -3.424741543176715 -1.1415805143922384\n",
      "manual: -2.283161028784477 -3.424741543176715 -1.1415805143922384\n",
      "step: 39 Loss: 0.3258015177100119\n",
      "engine: 2.591876529212673 2.591876529212673 2.591876529212673\n",
      "manual: 2.591876529212673 2.591876529212673 2.591876529212673\n",
      "step: 39 Loss: 1.6794559856708828\n",
      "engine: -2.2659263077358673 -3.398889461603801 -1.1329631538679337\n",
      "manual: -2.2659263077358673 -3.398889461603801 -1.1329631538679337\n",
      "step: 40 Loss: 0.3209013770055938\n",
      "engine: 2.572319515924063 2.572319515924063 2.572319515924063\n",
      "manual: 2.572319515924063 2.572319515924063 2.572319515924063\n",
      "step: 40 Loss: 1.6542069230009513\n",
      "engine: -2.2488236253915943 -3.3732354380873915 -1.1244118126957972\n",
      "manual: -2.2488236253915943 -3.3732354380873915 -1.1244118126957972\n",
      "step: 41 Loss: 0.3160754811324621\n",
      "engine: 2.552909762492117 2.552909762492117 2.552909762492117\n",
      "manual: 2.552909762492117 2.552909762492117 2.552909762492117\n",
      "step: 41 Loss: 1.6293370638568894\n",
      "engine: -2.2318513532800566 -3.347777029920085 -1.1159256766400283\n",
      "manual: -2.2318513532800566 -3.347777029920085 -1.1159256766400283\n",
      "step: 42 Loss: 0.3113225289461262\n",
      "engine: 2.5336462579393935 2.5336462579393935 2.5336462579393935\n",
      "manual: 2.5336462579393935 2.5336462579393935 2.5336462579393935\n",
      "step: 42 Loss: 1.6048408400925729\n",
      "engine: -2.2150080762670967 -3.322512114400645 -1.1075040381335484\n",
      "manual: -2.2150080762670967 -3.322512114400645 -1.1075040381335484\n",
      "step: 43 Loss: 0.306641298620529\n",
      "engine: 2.514527967039056 2.514527967039056 2.514527967039056\n",
      "manual: 2.514527967039056 2.514527967039056 2.514527967039056\n",
      "step: 43 Loss: 1.5807127242553922\n",
      "engine: -2.1982925270016764 -3.2974387905025146 -1.0991462635008382\n",
      "manual: -2.1982925270016764 -3.2974387905025146 -1.0991462635008382\n",
      "step: 44 Loss: 0.3020306271419635\n",
      "engine: 2.495553840636811 2.495553840636811 2.495553840636811\n",
      "manual: 2.495553840636811 2.495553840636811 2.495553840636811\n",
      "step: 44 Loss: 1.5569472428792843\n",
      "engine: -2.1817035411940466 -3.27255531179107 -1.0908517705970233\n",
      "manual: -2.1817035411940466 -3.27255531179107 -1.0908517705970233\n",
      "step: 45 Loss: 0.29748939635366517\n",
      "engine: 2.4767228226702453 2.4767228226702453 2.4767228226702453\n",
      "manual: 2.4767228226702453 2.4767228226702453 2.4767228226702453\n",
      "step: 45 Loss: 1.533538985083917\n",
      "engine: -2.1652400271005803 -3.2478600406508704 -1.0826200135502901\n",
      "manual: -2.1652400271005803 -3.2478600406508704 -1.0826200135502901\n",
      "step: 46 Loss: 0.2930165234349076\n",
      "engine: 2.4580338549360654 2.4580338549360654 2.4580338549360654\n",
      "manual: 2.4580338549360654 2.4580338549360654 2.4580338549360654\n",
      "step: 46 Loss: 1.5104826080029636\n",
      "engine: -2.1489009446970684 -3.2233514170456026 -1.0744504723485342\n",
      "manual: -2.1489009446970684 -3.2233514170456026 -1.0744504723485342\n",
      "step: 47 Loss: 0.28861095438249706\n",
      "engine: 2.439485880321726 2.439485880321726 2.439485880321726\n",
      "manual: 2.439485880321726 2.439485880321726 2.439485880321726\n",
      "step: 47 Loss: 1.4877728400722667\n",
      "engine: -2.1326852914591115 -3.199027937188667 -1.0663426457295557\n",
      "manual: -2.1326852914591115 -3.199027937188667 -1.0663426457295557\n",
      "step: 48 Loss: 0.2842716595253772\n",
      "engine: 2.421077844989968 2.421077844989968 2.421077844989968\n",
      "manual: 2.421077844989968 2.421077844989968 2.421077844989968\n",
      "step: 48 Loss: 1.465404482875317\n",
      "engine: -2.1165920926481547 -3.174888138972232 -1.0582960463240774\n",
      "manual: -2.1165920926481547 -3.174888138972232 -1.0582960463240774\n",
      "step: 49 Loss: 0.27999763041629344\n",
      "engine: 2.4028086998494587 2.4028086998494587 2.4028086998494587\n",
      "manual: 2.4028086998494587 2.4028086998494587 2.4028086998494587\n",
      "step: 49 Loss: 1.4433724120180615\n",
      "engine: -2.1006203946705355 -3.1509305920058033 -1.0503101973352678\n",
      "manual: -2.1006203946705355 -3.1509305920058033 -1.0503101973352678\n",
      "step: 50 Loss: 0.2757878776566123\n",
      "engine: 2.384677401538724 2.384677401538724 2.384677401538724\n",
      "manual: 2.384677401538724 2.384677401538724 2.384677401538724\n",
      "step: 50 Loss: 1.4216715773523703\n",
      "engine: -2.0847692605320844 -3.1271538907981267 -1.0423846302660422\n",
      "manual: -2.0847692605320844 -3.1271538907981267 -1.0423846302660422\n",
      "step: 51 Loss: 0.2716414293537184\n",
      "engine: 2.366682913078325 2.366682913078325 2.366682913078325\n",
      "manual: 2.366682913078325 2.366682913078325 2.366682913078325\n",
      "step: 51 Loss: 1.4002970027642265\n",
      "engine: -2.069037766721898 -3.1035566500828473 -1.034518883360949\n",
      "manual: -2.069037766721898 -3.1035566500828473 -1.034518883360949\n",
      "step: 52 Loss: 0.26755733000759624\n",
      "engine: 2.3488242042969407 2.3488242042969407 2.3488242042969407\n",
      "manual: 2.3488242042969407 2.3488242042969407 2.3488242042969407\n",
      "step: 52 Loss: 1.3792437856727893\n",
      "engine: -2.0534250010710338 -3.0801375016065506 -1.0267125005355169\n",
      "manual: -2.0534250010710338 -3.0801375016065506 -1.0267125005355169\n",
      "step: 53 Loss: 0.26353463968897345\n",
      "engine: 2.3311002521033863 2.3311002521033863 2.3311002521033863\n",
      "manual: 2.3311002521033863 2.3311002521033863 2.3311002521033863\n",
      "step: 53 Loss: 1.358507096339118\n",
      "engine: -2.0379300612759508 -3.056895091913926 -1.0189650306379754\n",
      "manual: -2.0379300612759508 -3.056895091913926 -1.0189650306379754\n",
      "step: 54 Loss: 0.2595724334157625\n",
      "engine: 2.313510040653739 2.313510040653739 2.313510040653739\n",
      "manual: 2.313510040653739 2.313510040653739 2.313510040653739\n",
      "step: 54 Loss: 1.3380821770514162\n",
      "engine: -2.022552053875586 -3.033828080813379 -1.011276026937793\n",
      "manual: -2.022552053875586 -3.033828080813379 -1.011276026937793\n",
      "step: 55 Loss: 0.2556698006647719\n",
      "engine: 2.29605256144705 2.29605256144705 2.29605256144705\n",
      "manual: 2.29605256144705 2.29605256144705 2.29605256144705\n",
      "step: 55 Loss: 1.31796434123189\n",
      "engine: -2.007290093537712 -3.010935140306568 -1.003645046768856\n",
      "manual: -2.007290093537712 -3.010935140306568 -1.003645046768856\n",
      "step: 56 Loss: 0.2518258449759148\n",
      "engine: 2.27872681337249 2.27872681337249 2.27872681337249\n",
      "manual: 2.27872681337249 2.27872681337249 2.27872681337249\n",
      "step: 56 Loss: 1.2981489724956858\n",
      "engine: -1.9921433025565491 -2.9882149538348237 -0.9960716512782746\n",
      "manual: -1.9921433025565491 -2.9882149538348237 -0.9960716512782746\n",
      "step: 57 Loss: 0.24803968362005716\n",
      "engine: 2.2615318027235336 2.2615318027235336 2.2615318027235336\n",
      "manual: 2.2615318027235336 2.2615318027235336 2.2615318027235336\n",
      "step: 57 Loss: 1.278631523682489\n",
      "engine: -1.9771108104943664 -2.9656662157415496 -0.9885554052471832\n",
      "manual: -1.9771108104943664 -2.9656662157415496 -0.9885554052471832\n",
      "step: 58 Loss: 0.24431044731085566\n",
      "engine: 2.244466543189784 2.244466543189784 2.244466543189784\n",
      "manual: 2.244466543189784 2.244466543189784 2.244466543189784\n",
      "step: 58 Loss: 1.2594075158745746\n",
      "engine: -1.9621917539214877 -2.9432876308822316 -0.9810958769607439\n",
      "manual: -1.9621917539214877 -2.9432876308822316 -0.9810958769607439\n",
      "step: 59 Loss: 0.24063727994734277\n",
      "engine: 2.227530055833686 2.227530055833686 2.227530055833686\n",
      "manual: 2.227530055833686 2.227530055833686 2.227530055833686\n",
      "step: 59 Loss: 1.240472537410606\n",
      "engine: -1.9473852762235566 -2.921077914335335 -0.9736926381117783\n",
      "manual: -1.9473852762235566 -2.921077914335335 -0.9736926381117783\n",
      "step: 60 Loss: 0.2370193383782686\n",
      "engine: 2.2107213690570777 2.2107213690570777 2.2107213690570777\n",
      "manual: 2.2107213690570777 2.2107213690570777 2.2107213690570777\n",
      "step: 60 Loss: 1.2218222429014\n",
      "engine: -1.9326905274546604 -2.8990357911819906 -0.9663452637273302\n",
      "manual: -1.9326905274546604 -2.8990357911819906 -0.9663452637273302\n",
      "step: 61 Loss: 0.23345579218206083\n",
      "engine: 2.194039518560933 2.194039518560933 2.194039518560933\n",
      "manual: 2.194039518560933 2.194039518560933 2.194039518560933\n",
      "step: 61 Loss: 1.2034523522517726\n",
      "engine: -1.9181066642219804 -2.8771599963329706 -0.9590533321109902\n",
      "manual: -1.9181066642219804 -2.8771599963329706 -0.9590533321109902\n",
      "step: 62 Loss: 0.2299458234582983\n",
      "engine: 2.1774835473005965 2.1774835473005965 2.1774835473005965\n",
      "manual: 2.1774835473005965 2.1774835473005965 2.1774835473005965\n",
      "step: 62 Loss: 1.1853586496911972\n",
      "engine: -1.9036328495919719 -2.855449274387958 -0.9518164247959859\n",
      "manual: -1.9036328495919719 -2.855449274387958 -0.9518164247959859\n",
      "step: 63 Loss: 0.22648862662785318\n",
      "engine: 2.1610525054380787 2.1610525054380787 2.1610525054380787\n",
      "manual: 2.1610525054380787 2.1610525054380787 2.1610525054380787\n",
      "step: 63 Loss: 1.1675369828150493\n",
      "engine: -1.8892682530113518 -2.8339023795170277 -0.9446341265056759\n",
      "manual: -1.8892682530113518 -2.8339023795170277 -0.9446341265056759\n",
      "step: 64 Loss: 0.22308340823978534\n",
      "engine: 2.144745450292474 2.144745450292474 2.144745450292474\n",
      "manual: 2.144745450292474 2.144745450292474 2.144745450292474\n",
      "step: 64 Loss: 1.1499832616375667\n",
      "engine: -1.8750120502383751 -2.8125180753575627 -0.9375060251191876\n",
      "manual: -1.8750120502383751 -2.8125180753575627 -0.9375060251191876\n",
      "step: 65 Loss: 0.2197293867836947\n",
      "engine: 2.1285614462892273 2.1285614462892273 2.1285614462892273\n",
      "manual: 2.1285614462892273 2.1285614462892273 2.1285614462892273\n",
      "step: 65 Loss: 1.1326934576572218\n",
      "engine: -1.860863423281053 -2.7912951349215795 -0.9304317116405265\n",
      "manual: -1.860863423281053 -2.7912951349215795 -0.9304317116405265\n",
      "step: 66 Loss: 0.21642579250657998\n",
      "engine: 2.1124995649087364 2.1124995649087364 2.1124995649087364\n",
      "manual: 2.1124995649087364 2.1124995649087364 2.1124995649087364\n",
      "step: 66 Loss: 1.1156636029349\n",
      "engine: -1.8468215603404445 -2.770232340510667 -0.9234107801702223\n",
      "manual: -1.8468215603404445 -2.770232340510667 -0.9234107801702223\n",
      "step: 67 Loss: 0.21317186723364465\n",
      "engine: 2.0965588846346392 2.0965588846346392 2.0965588846346392\n",
      "manual: 2.0965588846346392 2.0965588846346392 2.0965588846346392\n",
      "step: 67 Loss: 1.0988897891851106\n",
      "engine: -1.832885655757437 -2.7493284836361553 -0.9164428278787184\n",
      "manual: -1.832885655757437 -2.7493284836361553 -0.9164428278787184\n",
      "step: 68 Loss: 0.2099668641925856\n",
      "engine: 2.080738490902009 2.080738490902009 2.080738490902009\n",
      "manual: 2.080738490902009 2.080738490902009 2.080738490902009\n",
      "step: 68 Loss: 1.0823681668802925\n",
      "engine: -1.8190549099618352 -2.7285823649427527 -0.9095274549809176\n",
      "manual: -1.8190549099618352 -2.7285823649427527 -0.9095274549809176\n",
      "step: 69 Loss: 0.20681004784101625\n",
      "engine: 2.065037476045596 2.065037476045596 2.065037476045596\n",
      "manual: 2.065037476045596 2.065037476045596 2.065037476045596\n",
      "step: 69 Loss: 1.0660949443681915\n",
      "engine: -1.8053285294234627 -2.707992794135194 -0.9026642647117313\n",
      "manual: -1.8053285294234627 -2.707992794135194 -0.9026642647117313\n",
      "step: 70 Loss: 0.20370069369689264\n",
      "engine: 2.0494549392482693 2.0494549392482693 2.0494549392482693\n",
      "manual: 2.0494549392482693 2.0494549392482693 2.0494549392482693\n",
      "step: 70 Loss: 1.0500663870022817\n",
      "engine: -1.7917057266044765 -2.6875585899067147 -0.8958528633022382\n",
      "manual: -1.7917057266044765 -2.6875585899067147 -0.8958528633022382\n",
      "step: 71 Loss: 0.2006380881717047\n",
      "engine: 2.0339899864896402 2.0339899864896402 2.0339899864896402\n",
      "manual: 2.0339899864896402 2.0339899864896402 2.0339899864896402\n",
      "step: 71 Loss: 1.0342788162850318\n",
      "engine: -1.7781857199127487 -2.667278579869123 -0.8890928599563743\n",
      "manual: -1.7781857199127487 -2.667278579869123 -0.8890928599563743\n",
      "step: 72 Loss: 0.19762152840635125\n",
      "engine: 2.018641730495027 2.018641730495027 2.018641730495027\n",
      "manual: 2.018641730495027 2.018641730495027 2.018641730495027\n",
      "step: 72 Loss: 1.018728609023989\n",
      "engine: -1.7647677336559866 -2.64715160048398 -0.8823838668279933\n",
      "manual: -1.7647677336559866 -2.64715160048398 -0.8823838668279933\n",
      "step: 73 Loss: 0.19465032210958044\n",
      "engine: 2.0034092906846848 2.0034092906846848 2.0034092906846848\n",
      "manual: 2.0034092906846848 2.0034092906846848 2.0034092906846848\n",
      "step: 73 Loss: 1.003412196500428\n",
      "engine: -1.7514509979966348 -2.6271764969949523 -0.8757254989983174\n",
      "manual: -1.7514509979966348 -2.6271764969949523 -0.8757254989983174\n",
      "step: 74 Loss: 0.191723787398963\n",
      "engine: 1.9882917931234 1.9882917931234 1.9882917931234\n",
      "manual: 1.9882917931234 1.9882917931234 1.9882917931234\n",
      "step: 74 Loss: 0.9883260636504664\n",
      "engine: -1.738234748907196 -2.607352123360794 -0.869117374453598\n",
      "manual: -1.738234748907196 -2.607352123360794 -0.869117374453598\n",
      "step: 75 Loss: 0.18884125264427895\n",
      "engine: 1.9732883704704278 1.9732883704704278 1.9732883704704278\n",
      "manual: 1.9732883704704278 1.9732883704704278 1.9732883704704278\n",
      "step: 75 Loss: 0.9734667482584591\n",
      "engine: -1.7251182281260853 -2.587677342189128 -0.8625591140630426\n",
      "manual: -1.7251182281260853 -2.587677342189128 -0.8625591140630426\n",
      "step: 76 Loss: 0.18600205631330524\n",
      "engine: 1.9583981619297663 1.9583981619297663 1.9583981619297663\n",
      "manual: 1.9583981619297663 1.9583981619297663 1.9583981619297663\n",
      "step: 76 Loss: 0.9588308401624719\n",
      "engine: -1.712100683113924 -2.568151024670886 -0.856050341556962\n",
      "manual: -1.712100683113924 -2.568151024670886 -0.856050341556962\n",
      "step: 77 Loss: 0.18320554681994783\n",
      "engine: 1.9436203132008156 1.9436203132008156 1.9436203132008156\n",
      "manual: 1.9436203132008156 1.9436203132008156 1.9436203132008156\n",
      "step: 77 Loss: 0.9444149804717091\n",
      "engine: -1.699181367010226 -2.548772050515339 -0.849590683505113\n",
      "manual: -1.699181367010226 -2.548772050515339 -0.849590683505113\n",
      "step: 78 Loss: 0.18045108237467128\n",
      "engine: 1.9289539764293817 1.9289539764293817 1.9289539764293817\n",
      "manual: 1.9289539764293817 1.9289539764293817 1.9289539764293817\n",
      "step: 78 Loss: 0.930215860795681\n",
      "engine: -1.686359538590409 -2.5295393078856137 -0.8431797692952046\n",
      "manual: -1.686359538590409 -2.5295393078856137 -0.8431797692952046\n",
      "step: 79 Loss: 0.1777380308371786\n",
      "engine: 1.9143983101590418 1.9143983101590418 1.9143983101590418\n",
      "manual: 1.9143983101590418 1.9143983101590418 1.9143983101590418\n",
      "step: 79 Loss: 0.9162302224849487\n",
      "engine: -1.6736344622232764 -2.5104516933349146 -0.8368172311116382\n",
      "manual: -1.6736344622232764 -2.5104516933349146 -0.8368172311116382\n",
      "step: 80 Loss: 0.1750657695713372\n",
      "engine: 1.899952479282895 1.899952479282895 1.899952479282895\n",
      "manual: 1.899952479282895 1.899952479282895 1.899952479282895\n",
      "step: 80 Loss: 0.9024548558833049\n",
      "engine: -1.6610054078286538 -2.4915081117429807 -0.8305027039143269\n",
      "manual: -1.6610054078286538 -2.4915081117429807 -0.8305027039143269\n",
      "step: 81 Loss: 0.17243368530225203\n",
      "engine: 1.8856156549956413 1.8856156549956413 1.8856156549956413\n",
      "manual: 1.8856156549956413 1.8856156549956413 1.8856156549956413\n",
      "step: 81 Loss: 0.8888865995911603\n",
      "engine: -1.6484716508355817 -2.4727074762533725 -0.8242358254177908\n",
      "manual: -1.6484716508355817 -2.4727074762533725 -0.8242358254177908\n",
      "step: 82 Loss: 0.16984117397553675\n",
      "engine: 1.8713870147460376 1.8713870147460376 1.8713870147460376\n",
      "manual: 1.8713870147460376 1.8713870147460376 1.8713870147460376\n",
      "step: 82 Loss: 0.8755223397400216\n",
      "engine: -1.6360324721406627 -2.454048708210994 -0.8180162360703314\n",
      "manual: -1.6360324721406627 -2.454048708210994 -0.8180162360703314\n",
      "step: 83 Loss: 0.16728764061866802\n",
      "engine: 1.8572657421897159 1.8572657421897159 1.8572657421897159\n",
      "manual: 1.8572657421897159 1.8572657421897159 1.8572657421897159\n",
      "step: 83 Loss: 0.862359009277879\n",
      "engine: -1.623687158066808 -2.435530737100212 -0.811843579033404\n",
      "manual: -1.623687158066808 -2.435530737100212 -0.811843579033404\n",
      "step: 84 Loss: 0.1647724992044417\n",
      "engine: 1.8432510271423403 1.8432510271423403 1.8432510271423403\n",
      "manual: 1.8432510271423403 1.8432510271423403 1.8432510271423403\n",
      "step: 84 Loss: 0.8493935872653232\n",
      "engine: -1.611435000322274 -2.417152500483411 -0.805717500161137\n",
      "manual: -1.611435000322274 -2.417152500483411 -0.805717500161137\n",
      "step: 85 Loss: 0.16229517251647793\n",
      "engine: 1.8293420655331367 1.8293420655331367 1.8293420655331367\n",
      "manual: 1.8293420655331367 1.8293420655331367 1.8293420655331367\n",
      "step: 85 Loss: 0.8366230981822608\n",
      "engine: -1.599275295959977 -2.3989129439399655 -0.7996376479799885\n",
      "manual: -1.599275295959977 -2.3989129439399655 -0.7996376479799885\n",
      "step: 86 Loss: 0.159855092016742\n",
      "engine: 1.8155380593587491 1.8155380593587491 1.8155380593587491\n",
      "manual: 1.8155380593587491 1.8155380593587491 1.8155380593587491\n",
      "step: 86 Loss: 0.8240446112450333\n",
      "engine: -1.5872073473372907 -2.380811021005936 -0.7936036736686454\n",
      "manual: -1.5872073473372907 -2.380811021005936 -0.7936036736686454\n",
      "step: 87 Loss: 0.15745169771509243\n",
      "engine: 1.80183821663746 1.80183821663746 1.80183821663746\n",
      "manual: 1.80183821663746 1.80183821663746 1.80183821663746\n",
      "step: 87 Loss: 0.8116552397338156\n",
      "engine: -1.5752304620758366 -2.362845693113755 -0.7876152310379183\n",
      "manual: -1.5752304620758366 -2.362845693113755 -0.7876152310379183\n",
      "step: 88 Loss: 0.15508443804072836\n",
      "engine: 1.7882417513637616 1.7882417513637616 1.7882417513637616\n",
      "manual: 1.7882417513637616 1.7882417513637616 1.7882417513637616\n",
      "step: 88 Loss: 0.7994521403301333\n",
      "engine: -1.5633439530219064 -2.3450159295328596 -0.7816719765109532\n",
      "manual: -1.5633439530219064 -2.3450159295328596 -0.7816719765109532\n",
      "step: 89 Loss: 0.15275276971563506\n",
      "engine: 1.7747478834632506 1.7747478834632506 1.7747478834632506\n",
      "manual: 1.7747478834632506 1.7747478834632506 1.7747478834632506\n",
      "step: 89 Loss: 0.787432512464322\n",
      "engine: -1.5515471382069492 -2.327320707310424 -0.7757735691034746\n",
      "manual: -1.5515471382069492 -2.327320707310424 -0.7757735691034746\n",
      "step: 90 Loss: 0.15045615762988587\n",
      "engine: 1.761355838747873 1.761355838747873 1.761355838747873\n",
      "manual: 1.761355838747873 1.761355838747873 1.761355838747873\n",
      "step: 90 Loss: 0.7755935976728058\n",
      "engine: -1.5398393408084985 -2.3097590112127477 -0.7699196704042492\n",
      "manual: -1.5398393408084985 -2.3097590112127477 -0.7699196704042492\n",
      "step: 91 Loss: 0.14819407471884694\n",
      "engine: 1.7480648488715111 1.7480648488715111 1.7480648488715111\n",
      "manual: 1.7480648488715111 1.7480648488715111 1.7480648488715111\n",
      "step: 91 Loss: 0.7639326789650448\n",
      "engine: -1.528219889111277 -2.2923298336669156 -0.7641099445556385\n",
      "manual: -1.528219889111277 -2.2923298336669156 -0.7641099445556385\n",
      "step: 92 Loss: 0.14596600184220523\n",
      "engine: 1.7348741512858954 1.7348741512858954 1.7348741512858954\n",
      "manual: 1.7348741512858954 1.7348741512858954 1.7348741512858954\n",
      "step: 92 Loss: 0.752447080199989\n",
      "engine: -1.5166881164687354 -2.275032174703103 -0.7583440582343677\n",
      "manual: -1.5166881164687354 -2.275032174703103 -0.7583440582343677\n",
      "step: 93 Loss: 0.14377142766484252\n",
      "engine: 1.7217829891968677 1.7217829891968677 1.7217829891968677\n",
      "manual: 1.7217829891968677 1.7217829891968677 1.7217829891968677\n",
      "step: 93 Loss: 0.7411341654719252\n",
      "engine: -1.5052433612647391 -2.2578650418971087 -0.7526216806323696\n",
      "manual: -1.5052433612647391 -2.2578650418971087 -0.7526216806323696\n",
      "step: 94 Loss: 0.14160984853947312\n",
      "engine: 1.7087906115209393 1.7087906115209393 1.7087906115209393\n",
      "manual: 1.7087906115209393 1.7087906115209393 1.7087906115209393\n",
      "step: 94 Loss: 0.7299913385055263\n",
      "engine: -1.4938849668756475 -2.2408274503134713 -0.7469424834378238\n",
      "manual: -1.4938849668756475 -2.2408274503134713 -0.7469424834378238\n",
      "step: 95 Loss: 0.1394807683910659\n",
      "engine: 1.695896272842221 1.695896272842221 1.695896272842221\n",
      "manual: 1.695896272842221 1.695896272842221 1.695896272842221\n",
      "step: 95 Loss: 0.7190160420600343\n",
      "engine: -1.4826122816325835 -2.2239184224488753 -0.7413061408162918\n",
      "manual: -1.4826122816325835 -2.2239184224488753 -0.7413061408162918\n",
      "step: 96 Loss: 0.13738369860298594\n",
      "engine: 1.683099233369644 1.683099233369644 1.683099233369644\n",
      "manual: 1.683099233369644 1.683099233369644 1.683099233369644\n",
      "step: 96 Loss: 0.7082057573423708\n",
      "engine: -1.47142465878418 -2.20713698817627 -0.73571232939209\n",
      "manual: -1.47142465878418 -2.20713698817627 -0.73571232939209\n",
      "step: 97 Loss: 0.13531815790488377\n",
      "engine: 1.6703987588945175 1.6703987588945175 1.6703987588945175\n",
      "manual: 1.6703987588945175 1.6703987588945175 1.6703987588945175\n",
      "step: 97 Loss: 0.6975580034290861\n",
      "engine: -1.4603214564592903 -2.1904821846889355 -0.7301607282296452\n",
      "manual: -1.4603214564592903 -2.1904821846889355 -0.7301607282296452\n",
      "step: 98 Loss: 0.13328367226221144\n",
      "engine: 1.6577941207484024 1.6577941207484024 1.6577941207484024\n",
      "manual: 1.6577941207484024 1.6577941207484024 1.6577941207484024\n",
      "step: 98 Loss: 0.6870703366969921\n",
      "engine: -1.4493020376303178 -2.1739530564454768 -0.7246510188151589\n",
      "manual: -1.4493020376303178 -2.1739530564454768 -0.7246510188151589\n",
      "step: 99 Loss: 0.13127977476746194\n",
      "engine: 1.645284595761316 1.645284595761316 1.645284595761316\n",
      "manual: 1.645284595761316 1.645284595761316 1.645284595761316\n",
      "step: 99 Loss: 0.6767403502623692\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "model = neuron(2)\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for w in model.w:\n",
    "            zero_grad(w)\n",
    "        zero_grad(model.b)\n",
    "\n",
    "        y = model.pred(x)\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "\n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", model.w[0].grad, model.w[1].grad, model.b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        for w in model.w:\n",
    "            w.value -= n * w.grad\n",
    "        model.b.value -= n* model.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192012b",
   "metadata": {},
   "source": [
    "## Phase 3 — Adding Depth (2 → 3 → 1)\n",
    "\n",
    "Now we build a multi-layer network:\n",
    "\n",
    "Input (2)\n",
    "  ↓\n",
    "Linear (2 → 3)\n",
    "  ↓\n",
    "ReLU\n",
    "  ↓\n",
    "Linear (3 → 1)\n",
    "  ↓\n",
    "Loss\n",
    "\n",
    "This is where real backpropagation complexity begins.\n",
    "\n",
    "We now test:\n",
    "\n",
    "- Gradient flow through depth\n",
    "- Nonlinear gating via ReLU\n",
    "- Reverse topological traversal correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9daae",
   "metadata": {},
   "source": [
    "### Layer Abstraction\n",
    "\n",
    "A layer is simply a collection of neurons sharing the same input.\n",
    "\n",
    "Each neuron corresponds to one row of a weight matrix.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "y = W·x + b\n",
    "\n",
    "Even though everything is scalar-based,\n",
    "this reproduces matrix multiplication behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1f700aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class for the different layers\n",
    "class layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.neurons = [neuron(dim_in) for _ in range(dim_out)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        yout = []\n",
    "        if len(x) != self.dim_in:\n",
    "            raise ValueError(f\"Layer expected input dimension {self.dim_in}, got {len(x)}\")\n",
    "        else:\n",
    "            for neuron in self.neurons:\n",
    "                yout.append(neuron.pred(x))\n",
    "        return yout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5a0c3",
   "metadata": {},
   "source": [
    "### ReLU — First Nonlinearity\n",
    "\n",
    "ReLU(z) = max(0, z)\n",
    "\n",
    "Backward rule:\n",
    "\n",
    "If z > 0:\n",
    "    dL/dz = dL/da\n",
    "Else:\n",
    "    dL/dz = 0\n",
    "\n",
    "This introduces gradient gating.\n",
    "\n",
    "If a neuron’s pre-activation is negative,\n",
    "it receives zero gradient and does not learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e3789416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(node):\n",
    "    out = Node(\n",
    "        value = max(0, node.value),\n",
    "        parents = (node,),\n",
    "    )\n",
    "    def backward():\n",
    "        if node.value > 0:\n",
    "            node.grad += out.grad\n",
    "        else:\n",
    "            node.grad += 0\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13776fb",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e5b2b",
   "metadata": {},
   "source": [
    "### Full Forward Pass\n",
    "\n",
    "For each example:\n",
    "\n",
    "1. h = hidden.forward(x)\n",
    "2. a = relu(h)  (element-wise)\n",
    "3. y = output.forward(a)\n",
    "4. L = (y − y_true)²\n",
    "\n",
    "Backward then computes gradients for all 13 parameters\n",
    "in a single reverse traversal.\n",
    "\n",
    "This is where reverse-mode autodiff becomes powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d2d49dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layer(2,3)\n",
    "output = layer(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f3d81b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d511dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training output:\n",
      "Step   0 | Loss: 21.232770\n",
      "Step  10 | Loss: 5.194363\n",
      "Step  20 | Loss: 1.437143\n",
      "Step  30 | Loss: 0.345769\n",
      "Step  40 | Loss: 0.073744\n",
      "Step  50 | Loss: 0.014611\n",
      "Step  60 | Loss: 0.002788\n",
      "Step  70 | Loss: 0.000523\n",
      "Step  80 | Loss: 0.000097\n",
      "Step  90 | Loss: 0.000018\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        h = hidden.forward(x)\n",
    "        a = [relu(node) for node in h]\n",
    "        y = output.forward(a)[0]\n",
    "        l = (y - ytrue)**2\n",
    "\n",
    "        backward(l)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            print([w.grad for w in neuron.w], neuron.b.grad)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(\"Step:\", step, \"| Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187deef",
   "metadata": {},
   "source": [
    "#### Dead Neurons and Gradient Flow\n",
    "\n",
    "If a hidden neuron has negative pre-activation:\n",
    "\n",
    "ReLU'(z) = 0\n",
    "\n",
    "Then:\n",
    "\n",
    "dL/dw_hidden = 0\n",
    "\n",
    "This demonstrates how nonlinearities gate gradients.\n",
    "\n",
    "Only active neurons update.\n",
    "\n",
    "This is also why initialization matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a821d5",
   "metadata": {},
   "source": [
    "## Key Engineering Insights\n",
    "\n",
    "- Reverse-mode autodiff computes all gradients in one backward pass.\n",
    "- Gradients accumulate via +=, not assignment.\n",
    "- zero_grad is mandatory before backward.\n",
    "- Random initialization breaks symmetry.\n",
    "- ReLU introduces gradient gating.\n",
    "- Depth multiplies derivative terms via chain rule.\n",
    "- Even multi-layer networks can collapse to simpler representations.\n",
    "\n",
    "This project transformed backpropagation\n",
    "from an abstract formula into a concrete computational system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
