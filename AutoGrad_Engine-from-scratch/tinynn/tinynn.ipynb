{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135423c2",
   "metadata": {},
   "source": [
    "# Tiny Neural Network using The Autograd Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfab81",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, I build a tiny neural network entirely on top of my own reverse-mode autodiff engine.\n",
    "\n",
    "There is:\n",
    "- No NumPy\n",
    "- No PyTorch\n",
    "- No vectorization\n",
    "- Only scalar Nodes and a computational graph\n",
    "\n",
    "The goal is not performance — it is understanding gradient flow deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6146a85",
   "metadata": {},
   "source": [
    "Importing our autograd engine as a library to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3561fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de77e43",
   "metadata": {},
   "source": [
    "## Phase 1 — Single Input, Single Neuron\n",
    "\n",
    "We start with the simplest possible model:\n",
    "\n",
    "y = w·x + b\n",
    "\n",
    "Loss:\n",
    "L = (y − y_true)²\n",
    "\n",
    "This phase validates:\n",
    "\n",
    "- Parameter nodes store gradients correctly\n",
    "- Multiplication and addition propagate gradients\n",
    "- reverse-mode traversal works\n",
    "- zero_grad lifecycle is correct\n",
    "- manual derivative matches engine output\n",
    "\n",
    "If this fails, the engine is broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50610c",
   "metadata": {},
   "source": [
    "### Single-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e648c",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e394a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba937",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5697f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "ytrue = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75b56e",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1adb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -40 -20\n",
      "manual: -40 -20\n",
      "0 100\n",
      "engine: -36.0 -18.0\n",
      "manual: -36.0 -18.0\n",
      "1 81.0\n",
      "engine: -32.4 -16.2\n",
      "manual: -32.4 -16.2\n",
      "2 65.61\n",
      "engine: -29.16 -14.58\n",
      "manual: -29.16 -14.58\n",
      "3 53.1441\n",
      "engine: -26.244 -13.122\n",
      "manual: -26.244 -13.122\n",
      "4 43.046721\n",
      "engine: -23.6196 -11.8098\n",
      "manual: -23.6196 -11.8098\n",
      "5 34.86784400999999\n",
      "engine: -21.25764 -10.62882\n",
      "manual: -21.25764 -10.62882\n",
      "6 28.242953648099995\n",
      "engine: -19.131876 -9.565938\n",
      "manual: -19.131876 -9.565938\n",
      "7 22.876792454960995\n",
      "engine: -17.218688399999994 -8.609344199999997\n",
      "manual: -17.218688399999994 -8.609344199999997\n",
      "8 18.5302018885184\n",
      "engine: -15.496819559999999 -7.748409779999999\n",
      "manual: -15.496819559999999 -7.748409779999999\n",
      "9 15.009463529699909\n",
      "engine: -13.947137603999998 -6.973568801999999\n",
      "manual: -13.947137603999998 -6.973568801999999\n",
      "10 12.157665459056926\n",
      "engine: -12.552423843599996 -6.276211921799998\n",
      "manual: -12.552423843599996 -6.276211921799998\n",
      "11 9.847709021836106\n",
      "engine: -11.29718145924 -5.64859072962\n",
      "manual: -11.29718145924 -5.64859072962\n",
      "12 7.976644307687252\n",
      "engine: -10.167463313315999 -5.083731656657999\n",
      "manual: -10.167463313315999 -5.083731656657999\n",
      "13 6.461081889226672\n",
      "engine: -9.150716981984395 -4.575358490992198\n",
      "manual: -9.150716981984395 -4.575358490992198\n",
      "14 5.2334763302736\n",
      "engine: -8.235645283785956 -4.117822641892978\n",
      "manual: -8.235645283785956 -4.117822641892978\n",
      "15 4.2391158275216165\n",
      "engine: -7.412080755407359 -3.7060403777036797\n",
      "manual: -7.412080755407359 -3.7060403777036797\n",
      "16 3.433683820292508\n",
      "engine: -6.670872679866626 -3.335436339933313\n",
      "manual: -6.670872679866626 -3.335436339933313\n",
      "17 2.781283894436934\n",
      "engine: -6.00378541187996 -3.00189270593998\n",
      "manual: -6.00378541187996 -3.00189270593998\n",
      "18 2.252839954493914\n",
      "engine: -5.403406870691967 -2.7017034353459835\n",
      "manual: -5.403406870691967 -2.7017034353459835\n",
      "19 1.8248003631400722\n",
      "engine: -4.86306618362277 -2.431533091811385\n",
      "manual: -4.86306618362277 -2.431533091811385\n",
      "20 1.4780882941434585\n",
      "engine: -4.3767595652604925 -2.1883797826302462\n",
      "manual: -4.3767595652604925 -2.1883797826302462\n",
      "21 1.197251518256201\n",
      "engine: -3.9390836087344425 -1.9695418043672213\n",
      "manual: -3.9390836087344425 -1.9695418043672213\n",
      "22 0.9697737297875224\n",
      "engine: -3.545175247860996 -1.772587623930498\n",
      "manual: -3.545175247860996 -1.772587623930498\n",
      "23 0.7855167211278922\n",
      "engine: -3.1906577230748994 -1.5953288615374497\n",
      "manual: -3.1906577230748994 -1.5953288615374497\n",
      "24 0.6362685441135938\n",
      "engine: -2.87159195076741 -1.435795975383705\n",
      "manual: -2.87159195076741 -1.435795975383705\n",
      "25 0.5153775207320113\n",
      "engine: -2.5844327556906705 -1.2922163778453353\n",
      "manual: -2.5844327556906705 -1.2922163778453353\n",
      "26 0.4174557917929296\n",
      "engine: -2.3259894801215992 -1.1629947400607996\n",
      "manual: -2.3259894801215992 -1.1629947400607996\n",
      "27 0.3381391913522717\n",
      "engine: -2.0933905321094386 -1.0466952660547193\n",
      "manual: -2.0933905321094386 -1.0466952660547193\n",
      "28 0.2738927449953399\n",
      "engine: -1.884051478898499 -0.9420257394492495\n",
      "manual: -1.884051478898499 -0.9420257394492495\n",
      "29 0.22185312344622632\n",
      "engine: -1.6956463310086463 -0.8478231655043231\n",
      "manual: -1.6956463310086463 -0.8478231655043231\n",
      "30 0.17970102999144272\n",
      "engine: -1.5260816979077845 -0.7630408489538922\n",
      "manual: -1.5260816979077845 -0.7630408489538922\n",
      "31 0.14555783429306915\n",
      "engine: -1.3734735281170032 -0.6867367640585016\n",
      "manual: -1.3734735281170032 -0.6867367640585016\n",
      "32 0.11790184577738552\n",
      "engine: -1.2361261753053014 -0.6180630876526507\n",
      "manual: -1.2361261753053014 -0.6180630876526507\n",
      "33 0.09550049507968206\n",
      "engine: -1.1125135577747756 -0.5562567788873878\n",
      "manual: -1.1125135577747756 -0.5562567788873878\n",
      "34 0.07735540101454305\n",
      "engine: -1.0012622019972923 -0.5006311009986462\n",
      "manual: -1.0012622019972923 -0.5006311009986462\n",
      "35 0.06265787482177916\n",
      "engine: -0.9011359817975659 -0.45056799089878297\n",
      "manual: -0.9011359817975659 -0.45056799089878297\n",
      "36 0.05075287860564144\n",
      "engine: -0.8110223836178108 -0.4055111918089054\n",
      "manual: -0.8110223836178108 -0.4055111918089054\n",
      "37 0.04110983167056971\n",
      "engine: -0.729920145256024 -0.364960072628012\n",
      "manual: -0.729920145256024 -0.364960072628012\n",
      "38 0.03329896365316095\n",
      "engine: -0.656928130730428 -0.328464065365214\n",
      "manual: -0.656928130730428 -0.328464065365214\n",
      "39 0.026972160559060893\n",
      "engine: -0.5912353176573788 -0.2956176588286894\n",
      "manual: -0.5912353176573788 -0.2956176588286894\n",
      "40 0.021847450052838852\n",
      "engine: -0.5321117858916438 -0.2660558929458219\n",
      "manual: -0.5321117858916438 -0.2660558929458219\n",
      "41 0.01769643454279966\n",
      "engine: -0.47890060730247797 -0.23945030365123898\n",
      "manual: -0.47890060730247797 -0.23945030365123898\n",
      "42 0.014334111979667639\n",
      "engine: -0.4310105465722316 -0.2155052732861158\n",
      "manual: -0.4310105465722316 -0.2155052732861158\n",
      "43 0.011610630703530864\n",
      "engine: -0.38790949191501056 -0.19395474595750528\n",
      "manual: -0.38790949191501056 -0.19395474595750528\n",
      "44 0.009404610869860103\n",
      "engine: -0.3491185427235095 -0.17455927136175475\n",
      "manual: -0.3491185427235095 -0.17455927136175475\n",
      "45 0.007617734804586683\n",
      "engine: -0.31420668845115785 -0.15710334422557892\n",
      "manual: -0.31420668845115785 -0.15710334422557892\n",
      "46 0.006170365191715186\n",
      "engine: -0.28278601960603567 -0.14139300980301783\n",
      "manual: -0.28278601960603567 -0.14139300980301783\n",
      "47 0.004997995805289074\n",
      "engine: -0.25450741764543494 -0.12725370882271747\n",
      "manual: -0.25450741764543494 -0.12725370882271747\n",
      "48 0.004048376602284241\n",
      "engine: -0.22905667588089074 -0.11452833794044537\n",
      "manual: -0.22905667588089074 -0.11452833794044537\n",
      "49 0.0032791850478502147\n",
      "engine: -0.20615100829279953 -0.10307550414639977\n",
      "manual: -0.20615100829279953 -0.10307550414639977\n",
      "50 0.002656139888758619\n",
      "engine: -0.18553590746351745 -0.09276795373175872\n",
      "manual: -0.18553590746351745 -0.09276795373175872\n",
      "51 0.002151473309894432\n",
      "engine: -0.16698231671716712 -0.08349115835858356\n",
      "manual: -0.16698231671716712 -0.08349115835858356\n",
      "52 0.0017426933810145194\n",
      "engine: -0.150284085045449 -0.0751420425227245\n",
      "manual: -0.150284085045449 -0.0751420425227245\n",
      "53 0.0014115816386217341\n",
      "engine: -0.13525567654090764 -0.06762783827045382\n",
      "manual: -0.13525567654090764 -0.06762783827045382\n",
      "54 0.0011433811272836647\n",
      "engine: -0.12173010888681546 -0.06086505444340773\n",
      "manual: -0.12173010888681546 -0.06086505444340773\n",
      "55 0.0009261387130997467\n",
      "engine: -0.10955709799813462 -0.05477854899906731\n",
      "manual: -0.10955709799813462 -0.05477854899906731\n",
      "56 0.0007501723576108046\n",
      "engine: -0.09860138819831832 -0.04930069409915916\n",
      "manual: -0.09860138819831832 -0.04930069409915916\n",
      "57 0.0006076396096647167\n",
      "engine: -0.08874124937848649 -0.04437062468924324\n",
      "manual: -0.08874124937848649 -0.04437062468924324\n",
      "58 0.0004921880838284205\n",
      "engine: -0.07986712444063926 -0.03993356222031963\n",
      "manual: -0.07986712444063926 -0.03993356222031963\n",
      "59 0.0003986723479010348\n",
      "engine: -0.07188041199657391 -0.035940205998286956\n",
      "manual: -0.07188041199657391 -0.035940205998286956\n",
      "60 0.0003229246017998254\n",
      "engine: -0.06469237079691226 -0.03234618539845613\n",
      "manual: -0.06469237079691226 -0.03234618539845613\n",
      "61 0.00026156892745782414\n",
      "engine: -0.05822313371722743 -0.029111566858613713\n",
      "manual: -0.05822313371722743 -0.029111566858613713\n",
      "62 0.00021187083124088408\n",
      "engine: -0.05240082034550397 -0.026200410172751987\n",
      "manual: -0.05240082034550397 -0.026200410172751987\n",
      "63 0.00017161537330511145\n",
      "engine: -0.04716073831095002 -0.02358036915547501\n",
      "manual: -0.04716073831095002 -0.02358036915547501\n",
      "64 0.00013900845237711933\n",
      "engine: -0.04244466447985218 -0.02122233223992609\n",
      "manual: -0.04244466447985218 -0.02122233223992609\n",
      "65 0.00011259684642545158\n",
      "engine: -0.03820019803186625 -0.019100099015933125\n",
      "manual: -0.03820019803186625 -0.019100099015933125\n",
      "66 9.120344560461239e-05\n",
      "engine: -0.034380178228680336 -0.017190089114340168\n",
      "manual: -0.034380178228680336 -0.017190089114340168\n",
      "67 7.387479093973908e-05\n",
      "engine: -0.030942160405814434 -0.015471080202907217\n",
      "manual: -0.030942160405814434 -0.015471080202907217\n",
      "68 5.9838580661196906e-05\n",
      "engine: -0.02784794436523441 -0.013923972182617206\n",
      "manual: -0.02784794436523441 -0.013923972182617206\n",
      "69 4.846925033557444e-05\n",
      "engine: -0.025063149928705286 -0.012531574964352643\n",
      "manual: -0.025063149928705286 -0.012531574964352643\n",
      "70 3.926009277179749e-05\n",
      "engine: -0.022556834935841152 -0.011278417467920576\n",
      "manual: -0.022556834935841152 -0.011278417467920576\n",
      "71 3.1800675145173994e-05\n",
      "engine: -0.020301151442254195 -0.010150575721127097\n",
      "manual: -0.020301151442254195 -0.010150575721127097\n",
      "72 2.5758546867583725e-05\n",
      "engine: -0.018271036298024512 -0.009135518149012256\n",
      "manual: -0.018271036298024512 -0.009135518149012256\n",
      "73 2.086442296273308e-05\n",
      "engine: -0.016443932668224193 -0.008221966334112096\n",
      "manual: -0.016443932668224193 -0.008221966334112096\n",
      "74 1.6900182599818178e-05\n",
      "engine: -0.014799539401401773 -0.007399769700700887\n",
      "manual: -0.014799539401401773 -0.007399769700700887\n",
      "75 1.3689147905852722e-05\n",
      "engine: -0.013319585461260885 -0.006659792730630443\n",
      "manual: -0.013319585461260885 -0.006659792730630443\n",
      "76 1.1088209803739523e-05\n",
      "engine: -0.011987626915136218 -0.005993813457568109\n",
      "manual: -0.011987626915136218 -0.005993813457568109\n",
      "77 8.981449941031142e-06\n",
      "engine: -0.010788864223620465 -0.005394432111810232\n",
      "manual: -0.010788864223620465 -0.005394432111810232\n",
      "78 7.27497445223235e-06\n",
      "engine: -0.009709977801264813 -0.0048549889006324065\n",
      "manual: -0.009709977801264813 -0.0048549889006324065\n",
      "79 5.892729306315966e-06\n",
      "engine: -0.00873898002113549 -0.004369490010567745\n",
      "manual: -0.00873898002113549 -0.004369490010567745\n",
      "80 4.773110738112827e-06\n",
      "engine: -0.007865082019023362 -0.003932541009511681\n",
      "manual: -0.007865082019023362 -0.003932541009511681\n",
      "81 3.866219697872787e-06\n",
      "engine: -0.007078573817118183 -0.0035392869085590917\n",
      "manual: -0.007078573817118183 -0.0035392869085590917\n",
      "82 3.131637955274443e-06\n",
      "engine: -0.0063707164354056545 -0.0031853582177028272\n",
      "manual: -0.0063707164354056545 -0.0031853582177028272\n",
      "83 2.536626743771733e-06\n",
      "engine: -0.005733644791867221 -0.0028668223959336103\n",
      "manual: -0.005733644791867221 -0.0028668223959336103\n",
      "84 2.0546676624566314e-06\n",
      "engine: -0.005160280312679788 -0.002580140156339894\n",
      "manual: -0.005160280312679788 -0.002580140156339894\n",
      "85 1.664280806589413e-06\n",
      "engine: -0.004644252281408967 -0.0023221261407044835\n",
      "manual: -0.004644252281408967 -0.0023221261407044835\n",
      "86 1.3480674533357747e-06\n",
      "engine: -0.004179827053263807 -0.0020899135266319036\n",
      "manual: -0.004179827053263807 -0.0020899135266319036\n",
      "87 1.0919346371997501e-06\n",
      "engine: -0.0037618443479345842 -0.0018809221739672921\n",
      "manual: -0.0037618443479345842 -0.0018809221739672921\n",
      "88 8.844670561304611e-07\n",
      "engine: -0.003385659913142547 -0.0016928299565712734\n",
      "manual: -0.003385659913142547 -0.0016928299565712734\n",
      "89 7.164183154662749e-07\n",
      "engine: -0.0030470939218290027 -0.0015235469609145014\n",
      "manual: -0.0030470939218290027 -0.0015235469609145014\n",
      "90 5.802988355279533e-07\n",
      "engine: -0.002742384529646813 -0.0013711922648234065\n",
      "manual: -0.002742384529646813 -0.0013711922648234065\n",
      "91 4.700420567778857e-07\n",
      "engine: -0.00246814607668 -0.00123407303834\n",
      "manual: -0.00246814607668 -0.00123407303834\n",
      "92 3.807340659894298e-07\n",
      "engine: -0.0022213314690162633 -0.0011106657345081317\n",
      "manual: -0.0022213314690162633 -0.0011106657345081317\n",
      "93 3.083945934526219e-07\n",
      "engine: -0.0019991983221103737 -0.0009995991610551869\n",
      "manual: -0.0019991983221103737 -0.0009995991610551869\n",
      "94 2.4979962069555833e-07\n",
      "engine: -0.0017992784899050207 -0.0008996392449525104\n",
      "manual: -0.0017992784899050207 -0.0008996392449525104\n",
      "95 2.0233769276468074e-07\n",
      "engine: -0.0016193506409152292 -0.0008096753204576146\n",
      "manual: -0.0016193506409152292 -0.0008096753204576146\n",
      "96 1.6389353113953522e-07\n",
      "engine: -0.0014574155768229957 -0.0007287077884114979\n",
      "manual: -0.0014574155768229957 -0.0007287077884114979\n",
      "97 1.3275376022289407e-07\n",
      "engine: -0.0013116740191350118 -0.0006558370095675059\n",
      "manual: -0.0013116740191350118 -0.0006558370095675059\n",
      "98 1.0753054577961221e-07\n",
      "engine: -0.0011805066172243528 -0.0005902533086121764\n",
      "manual: -0.0011805066172243528 -0.0005902533086121764\n",
      "99 8.709974208190529e-08\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w)\n",
    "    zero_grad(b)\n",
    "\n",
    "    y = w*x + b\n",
    "    l = (y-ytrue)**2\n",
    "    \n",
    "    backward(l)\n",
    "    \n",
    "    manual_dw = 2*(y.value - ytrue)*x\n",
    "    manual_db = 2*(y.value - ytrue)\n",
    "\n",
    "    print(\"engine:\", w.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw, manual_db)\n",
    "    \n",
    "    w.value -= n*w.grad\n",
    "    b.value -= n*b.grad\n",
    "    \n",
    "    print(step, l.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fbeae",
   "metadata": {},
   "source": [
    "### What This Confirms\n",
    "\n",
    "The gradients printed by the engine match the manually derived gradients:\n",
    "\n",
    "dL/dw = 2(y − y_true)x  \n",
    "dL/db = 2(y − y_true)\n",
    "\n",
    "This confirms:\n",
    "\n",
    "- Chain rule is implemented correctly\n",
    "- Gradients accumulate via +=\n",
    "- Backward traversal is correct\n",
    "- Parameter updates modify value, not graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137dcdc",
   "metadata": {},
   "source": [
    "## Phase 2 — Multi-Input Neuron\n",
    "\n",
    "Now we extend the neuron to:\n",
    "\n",
    "y = w1·x1 + w2·x2 + b\n",
    "\n",
    "This introduces multiple parents in the computational graph.\n",
    "\n",
    "Graph structure becomes branched:\n",
    "\n",
    "(w1·x1) + (w2·x2) + b\n",
    "\n",
    "Reverse-mode must:\n",
    "\n",
    "- Propagate gradients through multiple branches\n",
    "- Accumulate gradient contributions correctly\n",
    "- Handle shared nodes safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21203234",
   "metadata": {},
   "source": [
    "### Multi-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d0a5b",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5eab5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47a78",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 3\n",
    "ytrue = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96085",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117a12c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -46.08 -69.12 -23.04\n",
      "manual: -46.08 -69.12 -23.04\n",
      "step: 1 Loss: 132.7104\n",
      "engine: -33.1776 -49.7664 -16.5888\n",
      "manual: -33.1776 -49.7664 -16.5888\n",
      "step: 2 Loss: 68.79707135999999\n",
      "engine: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "manual: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "step: 3 Loss: 35.664401793023984\n",
      "engine: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "manual: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "step: 4 Loss: 18.488425889503635\n",
      "engine: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "manual: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "step: 5 Loss: 9.584399981118693\n",
      "engine: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "manual: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "step: 6 Loss: 4.968552950211923\n",
      "engine: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "manual: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "step: 7 Loss: 2.5756978493898646\n",
      "engine: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "manual: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "step: 8 Loss: 1.3352417651237016\n",
      "engine: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "manual: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "step: 9 Loss: 0.6921893310401275\n",
      "engine: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "manual: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "step: 10 Loss: 0.35883094921120207\n",
      "engine: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "manual: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "step: 11 Loss: 0.18601796407108673\n",
      "engine: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "manual: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "step: 12 Loss: 0.09643171257445238\n",
      "engine: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "manual: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "step: 13 Loss: 0.049990199798596015\n",
      "engine: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "manual: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "step: 14 Loss: 0.025914919575591878\n",
      "engine: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "manual: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "step: 15 Loss: 0.013434294307986566\n",
      "engine: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "manual: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "step: 16 Loss: 0.006964338169260342\n",
      "engine: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "manual: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "step: 17 Loss: 0.0036103129069445785\n",
      "engine: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "manual: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "step: 18 Loss: 0.0018715862109599897\n",
      "engine: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "manual: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "step: 19 Loss: 0.0009702302917615258\n",
      "engine: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "manual: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "step: 20 Loss: 0.00050296738324921\n",
      "engine: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "manual: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "step: 21 Loss: 0.000260738291476379\n",
      "engine: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "manual: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "step: 22 Loss: 0.00013516673030137142\n",
      "engine: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "manual: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "step: 23 Loss: 7.00704329882464e-05\n",
      "engine: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "manual: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "step: 24 Loss: 3.6324512461082954e-05\n",
      "engine: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "manual: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "step: 25 Loss: 1.883062725985562e-05\n",
      "engine: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "manual: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "step: 26 Loss: 9.761797171499829e-06\n",
      "engine: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "manual: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "step: 27 Loss: 5.060515653709667e-06\n",
      "engine: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "manual: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "step: 28 Loss: 2.6233713148759562e-06\n",
      "engine: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "manual: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "step: 29 Loss: 1.3599556896305355e-06\n",
      "engine: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "manual: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "step: 30 Loss: 7.050010295060208e-07\n",
      "engine: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "manual: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "step: 31 Loss: 3.654725336985844e-07\n",
      "engine: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "manual: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "step: 32 Loss: 1.894609614669338e-07\n",
      "engine: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "manual: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "step: 33 Loss: 9.821656242414673e-08\n",
      "engine: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "manual: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "step: 34 Loss: 5.0915465961094525e-08\n",
      "engine: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "manual: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "step: 35 Loss: 2.6394577554231402e-08\n",
      "engine: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "manual: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "step: 36 Loss: 1.368294900422992e-08\n",
      "engine: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "manual: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "step: 37 Loss: 7.0932407638047585e-09\n",
      "engine: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "manual: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "step: 38 Loss: 3.6771360120942648e-09\n",
      "engine: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "manual: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "step: 39 Loss: 1.9062273085828036e-09\n",
      "engine: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "manual: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "step: 40 Loss: 9.881882366799804e-10\n",
      "engine: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "manual: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "step: 41 Loss: 5.122767820396407e-10\n",
      "engine: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "manual: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "step: 42 Loss: 2.655642836935587e-10\n",
      "engine: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "manual: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "step: 43 Loss: 1.3766852470509082e-10\n",
      "engine: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "manual: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "step: 44 Loss: 7.136736320591857e-11\n",
      "engine: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "manual: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "step: 45 Loss: 3.699684109113444e-11\n",
      "engine: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "manual: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "step: 46 Loss: 1.9179162423511145e-11\n",
      "engine: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "manual: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "step: 47 Loss: 9.9424777931787e-12\n",
      "engine: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "manual: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "step: 48 Loss: 5.154180495404247e-12\n",
      "engine: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "manual: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "step: 49 Loss: 2.671927166726942e-12\n",
      "engine: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "manual: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "step: 50 Loss: 1.3851270464089885e-12\n",
      "engine: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "manual: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "step: 51 Loss: 7.180498583296061e-13\n",
      "engine: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "manual: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "step: 52 Loss: 3.722370470782809e-13\n",
      "engine: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "manual: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "step: 53 Loss: 1.9296768383201834e-13\n",
      "engine: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "manual: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "step: 54 Loss: 1.0003444828733929e-13\n",
      "engine: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "manual: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "step: 55 Loss: 5.1857858380493653e-14\n",
      "engine: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "manual: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "step: 56 Loss: 2.688311322524268e-14\n",
      "engine: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "manual: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "step: 57 Loss: 1.3936205963070433e-14\n",
      "engine: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "manual: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "step: 58 Loss: 7.224529207492211e-15\n",
      "engine: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "manual: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "step: 59 Loss: 3.7451960194348e-15\n",
      "engine: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "manual: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "step: 60 Loss: 1.941509647783336e-15\n",
      "engine: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "manual: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "step: 61 Loss: 1.0064784120580766e-15\n",
      "engine: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "manual: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "step: 62 Loss: 5.217584380253387e-16\n",
      "engine: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "manual: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "step: 63 Loss: 2.7047962568973864e-16\n",
      "engine: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "manual: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "step: 64 Loss: 1.4021654204074408e-16\n",
      "engine: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "manual: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "step: 65 Loss: 7.268827477921314e-17\n",
      "engine: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "manual: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "step: 66 Loss: 3.768161211360287e-17\n",
      "engine: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "manual: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "step: 67 Loss: 1.953415776903015e-17\n",
      "engine: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "manual: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "step: 68 Loss: 1.0126506030804414e-17\n",
      "engine: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "manual: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "step: 69 Loss: 5.2495735632025836e-18\n",
      "engine: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "manual: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "step: 70 Loss: 2.721383858213691e-18\n",
      "engine: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "manual: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "step: 71 Loss: 1.410763535404805e-18\n",
      "engine: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "manual: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "step: 72 Loss: 7.313403028698023e-19\n",
      "engine: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "manual: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "step: 73 Loss: 3.791296130412209e-19\n",
      "engine: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "manual: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "step: 74 Loss: 1.9653896437957275e-19\n",
      "engine: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "manual: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "step: 75 Loss: 1.0188793108506885e-19\n",
      "engine: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "manual: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "step: 76 Loss: 5.281791964122085e-20\n",
      "engine: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "manual: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "step: 77 Loss: 2.7380715482445175e-20\n",
      "engine: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "manual: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "step: 78 Loss: 1.4194399936986123e-20\n",
      "engine: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "manual: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "step: 79 Loss: 7.358559781631122e-21\n",
      "engine: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "manual: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "step: 80 Loss: 3.8143438697772514e-21\n",
      "engine: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "manual: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "step: 81 Loss: 1.977520164900858e-21\n",
      "engine: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "manual: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "step: 82 Loss: 1.0250918539904007e-21\n",
      "engine: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "manual: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "step: 83 Loss: 5.314665854129708e-22\n",
      "engine: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "manual: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "step: 84 Loss: 2.755028427684428e-22\n",
      "engine: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "manual: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "step: 85 Loss: 1.4283426045583935e-22\n",
      "engine: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "manual: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "step: 86 Loss: 7.40403893494799e-23\n",
      "engine: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "manual: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "step: 87 Loss: 3.836757225630106e-23\n",
      "engine: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "manual: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "step: 88 Loss: 1.9879610355931586e-23\n",
      "engine: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "manual: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "step: 89 Loss: 1.031471605633751e-23\n",
      "engine: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "manual: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "step: 90 Loss: 5.340907028151959e-24\n",
      "engine: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "manual: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "step: 91 Loss: 2.770381680383946e-24\n",
      "engine: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "manual: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "step: 92 Loss: 1.4334423063207214e-24\n",
      "engine: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "manual: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "step: 93 Loss: 7.483734081214486e-25\n",
      "engine: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "manual: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "step: 94 Loss: 3.8433618846729784e-25\n",
      "engine: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "manual: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "step: 95 Loss: 2.0038329170062053e-25\n",
      "engine: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "manual: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "step: 96 Loss: 1.0337548846378227e-25\n",
      "engine: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "manual: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "step: 97 Loss: 5.415056797799113e-26\n",
      "engine: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "manual: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "step: 98 Loss: 2.8477878678478526e-26\n",
      "engine: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "manual: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "step: 99 Loss: 1.4590771302967834e-26\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w1)\n",
    "    zero_grad(w2)\n",
    "    zero_grad(b)\n",
    "    \n",
    "    y = w1*x1 + w2*x2 + b\n",
    "    l = (y-ytrue)**2\n",
    "\n",
    "    backward(l)\n",
    "\n",
    "    delta = 2*(y.value - ytrue)\n",
    "\n",
    "    manual_dw1 = delta * x1\n",
    "    manual_dw2 = delta * x2\n",
    "    manual_db = delta\n",
    "    \n",
    "    print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "    w1.value -= n * w1.grad\n",
    "    w2.value -= n * w2.grad\n",
    "    b.value -= n * b.grad\n",
    "\n",
    "    print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21723baf",
   "metadata": {},
   "source": [
    "Now for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbe8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bde1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -6.16 -6.16 -6.16\n",
      "manual: -6.16 -6.16 -6.16\n",
      "step: 0 Loss: 9.4864\n",
      "engine: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "manual: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "step: 1 Loss: 124.33142016000002\n",
      "engine: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "manual: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "step: 1 Loss: 2.4247223511039984\n",
      "engine: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "manual: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "step: 2 Loss: 61.48802068101364\n",
      "engine: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "manual: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "step: 2 Loss: 0.2732689308521132\n",
      "engine: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "manual: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "step: 3 Loss: 31.170997460282468\n",
      "engine: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "manual: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "step: 3 Loss: 0.03189284586729491\n",
      "engine: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "manual: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "step: 4 Loss: 16.33179658342692\n",
      "engine: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "manual: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "step: 4 Loss: 0.4261765009868418\n",
      "engine: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "manual: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "step: 5 Loss: 8.92842546475222\n",
      "engine: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "manual: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "step: 5 Loss: 0.9452083970312428\n",
      "engine: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "manual: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "step: 6 Loss: 5.144096642410019\n",
      "engine: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "manual: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "step: 6 Loss: 1.4067203007232514\n",
      "engine: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "manual: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "step: 7 Loss: 3.1517950617409176\n",
      "engine: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "manual: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "step: 7 Loss: 1.7633949131753535\n",
      "engine: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "manual: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "step: 8 Loss: 2.0666613938848766\n",
      "engine: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "manual: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "step: 8 Loss: 2.0185695667441066\n",
      "engine: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "manual: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "step: 9 Loss: 1.4533641651357307\n",
      "engine: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "manual: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "step: 9 Loss: 2.19094635544156\n",
      "engine: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "manual: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "step: 10 Loss: 1.0933256565007086\n",
      "engine: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "manual: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "step: 10 Loss: 2.300828311885715\n",
      "engine: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "manual: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "step: 11 Loss: 0.8739813892243515\n",
      "engine: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "manual: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "step: 11 Loss: 2.3655105271774577\n",
      "engine: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "manual: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "step: 12 Loss: 0.7355957569813151\n",
      "engine: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "manual: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "step: 12 Loss: 2.39834960772937\n",
      "engine: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "manual: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "step: 13 Loss: 0.6453885758378625\n",
      "engine: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "manual: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "step: 13 Loss: 2.409151760465584\n",
      "engine: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "manual: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "step: 14 Loss: 0.5847310457140027\n",
      "engine: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "manual: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "step: 14 Loss: 2.404908812355005\n",
      "engine: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "manual: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "step: 15 Loss: 0.5426690205794957\n",
      "engine: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "manual: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "step: 15 Loss: 2.3905164858697434\n",
      "engine: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "manual: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "step: 16 Loss: 0.5125575727467586\n",
      "engine: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "manual: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "step: 16 Loss: 2.369362902444438\n",
      "engine: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "manual: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "step: 17 Loss: 0.4902566940029108\n",
      "engine: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "manual: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "step: 17 Loss: 2.343774490634568\n",
      "engine: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "manual: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "step: 18 Loss: 0.4731303138646301\n",
      "engine: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "manual: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "step: 18 Loss: 2.3153399170214897\n",
      "engine: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "manual: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "step: 19 Loss: 0.45947122411636654\n",
      "engine: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "manual: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "step: 19 Loss: 2.2851396186686763\n",
      "engine: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "manual: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "step: 20 Loss: 0.44815948369410363\n",
      "engine: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "manual: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "step: 20 Loss: 2.2539059385445808\n",
      "engine: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "manual: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "step: 21 Loss: 0.43845337086104164\n",
      "engine: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "manual: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "step: 21 Loss: 2.222133731465707\n",
      "engine: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "manual: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "step: 22 Loss: 0.42985819753731525\n",
      "engine: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "manual: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "step: 22 Loss: 2.1901562572347317\n",
      "engine: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "manual: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "step: 23 Loss: 0.42204231181996454\n",
      "engine: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "manual: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "step: 23 Loss: 2.15819702616437\n",
      "engine: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "manual: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "step: 24 Loss: 0.41478247554885156\n",
      "engine: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "manual: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "step: 24 Loss: 2.1264051115482006\n",
      "engine: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "manual: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "step: 25 Loss: 0.4079279314824542\n",
      "engine: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "manual: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "step: 25 Loss: 2.0948791527557025\n",
      "engine: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "manual: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "step: 26 Loss: 0.4013765655318351\n",
      "engine: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "manual: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "step: 26 Loss: 2.0636836484305476\n",
      "engine: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "manual: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "step: 27 Loss: 0.39505899726057586\n",
      "engine: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "manual: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "step: 27 Loss: 2.0328600057264787\n",
      "engine: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "manual: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "step: 28 Loss: 0.38892791640667984\n",
      "engine: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "manual: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "step: 28 Loss: 2.002434028386879\n",
      "engine: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "manual: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "step: 29 Loss: 0.38295091418671107\n",
      "engine: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "manual: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "step: 29 Loss: 1.9724209890373758\n",
      "engine: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "manual: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "step: 30 Loss: 0.37710565395421897\n",
      "engine: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "manual: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "step: 30 Loss: 1.9428290638754178\n",
      "engine: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "manual: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "step: 31 Loss: 0.3713766130825078\n",
      "engine: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "manual: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "step: 31 Loss: 1.9136616578256025\n",
      "engine: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "manual: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "step: 32 Loss: 0.3657528826485063\n",
      "engine: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "manual: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "step: 32 Loss: 1.884918978206111\n",
      "engine: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "manual: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "step: 33 Loss: 0.3602266804314682\n",
      "engine: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "manual: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "step: 33 Loss: 1.8565990995326203\n",
      "engine: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "manual: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "step: 34 Loss: 0.3547923454722424\n",
      "engine: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "manual: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "step: 34 Loss: 1.8286986838094759\n",
      "engine: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "manual: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "step: 35 Loss: 0.3494456579904879\n",
      "engine: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "manual: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "step: 35 Loss: 1.8012134676049756\n",
      "engine: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "manual: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "step: 36 Loss: 0.3441833792432918\n",
      "engine: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "manual: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "step: 36 Loss: 1.7741385912669188\n",
      "engine: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "manual: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "step: 37 Loss: 0.33900294011960047\n",
      "engine: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "manual: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "step: 37 Loss: 1.7474688212932457\n",
      "engine: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "manual: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "step: 38 Loss: 0.33390223034363575\n",
      "engine: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "manual: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "step: 38 Loss: 1.7211987003911826\n",
      "engine: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "manual: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "step: 39 Loss: 0.3288794557454377\n",
      "engine: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "manual: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "step: 39 Loss: 1.6953226486000565\n",
      "engine: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "manual: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "step: 40 Loss: 0.3239330415882179\n",
      "engine: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "manual: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "step: 40 Loss: 1.6698350312993278\n",
      "engine: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "manual: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "step: 41 Loss: 0.3190615670625761\n",
      "engine: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "manual: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "step: 41 Loss: 1.6447302048105017\n",
      "engine: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "manual: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "step: 42 Loss: 0.3142637208729977\n",
      "engine: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "manual: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "step: 42 Loss: 1.6200025468407362\n",
      "engine: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "manual: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "step: 43 Loss: 0.30953827109960846\n",
      "engine: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "manual: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "step: 43 Loss: 1.5956464766734495\n",
      "engine: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "manual: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "step: 44 Loss: 0.3048840447220777\n",
      "engine: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "manual: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "step: 44 Loss: 1.5716564684259073\n",
      "engine: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "manual: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "step: 45 Loss: 0.300299913683752\n",
      "engine: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "manual: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "step: 45 Loss: 1.5480270596206327\n",
      "engine: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "manual: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "step: 46 Loss: 0.2957847853833086\n",
      "engine: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "manual: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "step: 46 Loss: 1.5247528565912059\n",
      "engine: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "manual: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "step: 47 Loss: 0.29133759616397464\n",
      "engine: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "manual: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "step: 47 Loss: 1.5018285377515561\n",
      "engine: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "manual: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "step: 48 Loss: 0.2869573068327053\n",
      "engine: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "manual: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "step: 48 Loss: 1.4792488554250944\n",
      "engine: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "manual: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "step: 49 Loss: 0.2826428995542664\n",
      "engine: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "manual: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "step: 49 Loss: 1.4570086367048911\n",
      "engine: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "manual: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "step: 50 Loss: 0.2783933756769821\n",
      "engine: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "manual: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "step: 50 Loss: 1.4351027836637926\n",
      "engine: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "manual: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "step: 51 Loss: 0.2742077541900975\n",
      "engine: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "manual: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "step: 51 Loss: 1.4135262731301397\n",
      "engine: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "manual: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "step: 52 Loss: 0.27008507060972936\n",
      "engine: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "manual: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "step: 52 Loss: 1.3922741561749867\n",
      "engine: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "manual: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "step: 53 Loss: 0.26602437615586866\n",
      "engine: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "manual: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "step: 53 Loss: 1.3713415574095253\n",
      "engine: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "manual: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "step: 54 Loss: 0.2620247371274635\n",
      "engine: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "manual: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "step: 54 Loss: 1.3507236741593498\n",
      "engine: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "manual: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "step: 55 Loss: 0.2580852344125613\n",
      "engine: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "manual: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "step: 55 Loss: 1.3304157755607073\n",
      "engine: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "manual: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "step: 56 Loss: 0.2542049630908597\n",
      "engine: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "manual: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "step: 56 Loss: 1.3104132016090455\n",
      "engine: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "manual: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "step: 57 Loss: 0.250383032099792\n",
      "engine: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "manual: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "step: 57 Loss: 1.2907113621804505\n",
      "engine: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "manual: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "step: 58 Loss: 0.24661856394460346\n",
      "engine: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "manual: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "step: 58 Loss: 1.2713057360397508\n",
      "engine: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "manual: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "step: 59 Loss: 0.2429106944391491\n",
      "engine: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "manual: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "step: 59 Loss: 1.252191869844529\n",
      "engine: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "manual: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "step: 60 Loss: 0.23925857246842042\n",
      "engine: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "manual: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "step: 60 Loss: 1.2333653771512865\n",
      "engine: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "manual: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "step: 61 Loss: 0.23566135976675504\n",
      "engine: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "manual: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "step: 61 Loss: 1.2148219374278124\n",
      "engine: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "manual: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "step: 62 Loss: 0.2321182307075031\n",
      "engine: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "manual: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "step: 62 Loss: 1.1965572950745806\n",
      "engine: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "manual: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "step: 63 Loss: 0.22862837210144146\n",
      "engine: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "manual: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "step: 63 Loss: 1.1785672584568012\n",
      "engine: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "manual: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "step: 64 Loss: 0.22519098300191842\n",
      "engine: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "manual: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "step: 64 Loss: 1.1608476989483916\n",
      "engine: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "manual: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "step: 65 Loss: 0.22180527451550977\n",
      "engine: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "manual: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "step: 65 Loss: 1.1433945499884604\n",
      "engine: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "manual: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "step: 66 Loss: 0.2184704696171666\n",
      "engine: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "manual: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "step: 66 Loss: 1.1262038061507598\n",
      "engine: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "manual: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "step: 67 Loss: 0.21518580296939233\n",
      "engine: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "manual: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "step: 67 Loss: 1.1092715222262894\n",
      "engine: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "manual: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "step: 68 Loss: 0.2119505207448405\n",
      "engine: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "manual: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "step: 68 Loss: 1.092593812319109\n",
      "engine: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "manual: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "step: 69 Loss: 0.2087638804521744\n",
      "engine: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "manual: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "step: 69 Loss: 1.0761668489553933\n",
      "engine: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "manual: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "step: 70 Loss: 0.20562515076485635\n",
      "engine: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "manual: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "step: 70 Loss: 1.0599868622056043\n",
      "engine: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "manual: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "step: 71 Loss: 0.202533611352796\n",
      "engine: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "manual: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "step: 71 Loss: 1.0440501388196815\n",
      "engine: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "manual: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "step: 72 Loss: 0.1994885527166665\n",
      "engine: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "manual: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "step: 72 Loss: 1.0283530213751464\n",
      "engine: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "manual: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "step: 73 Loss: 0.19648927602481972\n",
      "engine: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "manual: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "step: 73 Loss: 1.0128919074378824\n",
      "engine: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "manual: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "step: 74 Loss: 0.19353509295277788\n",
      "engine: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "manual: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "step: 74 Loss: 0.997663248735532\n",
      "engine: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "manual: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "step: 75 Loss: 0.19062532552510686\n",
      "engine: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "manual: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "step: 75 Loss: 0.9826635503432468\n",
      "engine: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "manual: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "step: 76 Loss: 0.18775930595978385\n",
      "engine: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "manual: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "step: 76 Loss: 0.967889369881674\n",
      "engine: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "manual: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "step: 77 Loss: 0.184936376514874\n",
      "engine: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "manual: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "step: 77 Loss: 0.9533373167269823\n",
      "engine: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "manual: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "step: 78 Loss: 0.1821558893375515\n",
      "engine: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "manual: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "step: 78 Loss: 0.9390040512327443\n",
      "engine: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "manual: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "step: 79 Loss: 0.179417206315393\n",
      "engine: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "manual: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "step: 79 Loss: 0.9248862839635181\n",
      "engine: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "manual: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "step: 80 Loss: 0.17671969892992428\n",
      "engine: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "manual: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "step: 80 Loss: 0.9109807749399496\n",
      "engine: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "manual: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "step: 81 Loss: 0.1740627481123579\n",
      "engine: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "manual: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "step: 81 Loss: 0.8972843328952061\n",
      "engine: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "manual: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "step: 82 Loss: 0.1714457441015169\n",
      "engine: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "manual: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "step: 82 Loss: 0.883793814542602\n",
      "engine: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "manual: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "step: 83 Loss: 0.16886808630389047\n",
      "engine: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "manual: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "step: 83 Loss: 0.8705061238542326\n",
      "engine: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "manual: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "step: 84 Loss: 0.1663291831557962\n",
      "engine: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "manual: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "step: 84 Loss: 0.8574182113504414\n",
      "engine: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "manual: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "step: 85 Loss: 0.16382845198761636\n",
      "engine: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "manual: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "step: 85 Loss: 0.8445270733999863\n",
      "engine: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "manual: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "step: 86 Loss: 0.16136531889007713\n",
      "engine: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "manual: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "step: 86 Loss: 0.8318297515306993\n",
      "engine: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "manual: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "step: 87 Loss: 0.15893921858253074\n",
      "engine: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "manual: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "step: 87 Loss: 0.8193233317505548\n",
      "engine: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "manual: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "step: 88 Loss: 0.15654959428323653\n",
      "engine: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "manual: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "step: 88 Loss: 0.8070049438788954\n",
      "engine: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "manual: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "step: 89 Loss: 0.15419589758156857\n",
      "engine: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "manual: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "step: 89 Loss: 0.7948717608877401\n",
      "engine: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "manual: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "step: 90 Loss: 0.1518775883121652\n",
      "engine: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "manual: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "step: 90 Loss: 0.782920998253007\n",
      "engine: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "manual: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "step: 91 Loss: 0.14959413443096498\n",
      "engine: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "manual: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "step: 91 Loss: 0.7711499133154561\n",
      "engine: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "manual: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "step: 92 Loss: 0.1473450118930849\n",
      "engine: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "manual: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "step: 92 Loss: 0.7595558046512645\n",
      "engine: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "manual: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "step: 93 Loss: 0.1451297045325846\n",
      "engine: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "manual: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "step: 93 Loss: 0.7481360114520615\n",
      "engine: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "manual: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "step: 94 Loss: 0.14294770394398368\n",
      "engine: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "manual: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "step: 94 Loss: 0.7368879129142843\n",
      "engine: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "manual: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "step: 95 Loss: 0.14079850936559313\n",
      "engine: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "manual: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "step: 95 Loss: 0.725808927637705\n",
      "engine: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "manual: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "step: 96 Loss: 0.13868162756459101\n",
      "engine: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "manual: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "step: 96 Loss: 0.7148965130330096\n",
      "engine: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "manual: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "step: 97 Loss: 0.13659657272382797\n",
      "engine: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "manual: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "step: 97 Loss: 0.7041481647382901\n",
      "engine: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "manual: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "step: 98 Loss: 0.13454286633033774\n",
      "engine: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "manual: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "step: 98 Loss: 0.6935614160442927\n",
      "engine: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "manual: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "step: 99 Loss: 0.13252003706550594\n",
      "engine: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "manual: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "step: 99 Loss: 0.6831338373283254\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        \n",
    "        zero_grad(w1)\n",
    "        zero_grad(w2)\n",
    "        zero_grad(b)\n",
    "\n",
    "        y = w1*x1 + w2*x2 + b\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "        \n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        w1.value -= n * w1.grad\n",
    "        w2.value -= n * w2.grad\n",
    "        b.value -= n * b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fe5c5",
   "metadata": {},
   "source": [
    "### Observations with Dataset Training\n",
    "\n",
    "Training over multiple examples introduces SGD dynamics.\n",
    "\n",
    "We observe:\n",
    "\n",
    "- Loss oscillates because updates happen per sample\n",
    "- Gradients must be zeroed before backward\n",
    "- Parameters must persist across steps\n",
    "\n",
    "This validates the full training lifecycle:\n",
    "\n",
    "1. zero_grad  \n",
    "2. forward  \n",
    "3. backward  \n",
    "4. update  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e5b5a",
   "metadata": {},
   "source": [
    "## Class Functions of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c765068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4405ab",
   "metadata": {},
   "source": [
    "### Abstraction — Turning Neuron into a Class\n",
    "\n",
    "To scale the system, we abstract the neuron into a reusable class.\n",
    "\n",
    "This introduces:\n",
    "\n",
    "- Weight vector abstraction\n",
    "- Dimension safety checks\n",
    "- Dot-product computation\n",
    "- Random initialization to break symmetry\n",
    "\n",
    "Now the neuron represents a hyperplane in ℝⁿ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d95f",
   "metadata": {},
   "source": [
    "#### For a single input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron for a single input\n",
    "class neuron:\n",
    "    def __init__(self):\n",
    "        self.w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.b = Node(0)\n",
    "    def pred(self, x):\n",
    "        y = self.w * x + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39690",
   "metadata": {},
   "source": [
    "#### For multi-input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated __init___ and pred for multi-inputs\n",
    "def __init__(self, dim=1):\n",
    "    self.w = []\n",
    "    self.b = Node(0)\n",
    "    self.dim = dim\n",
    "    for i in range (0, self.dim):\n",
    "        w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.w.append(w)\n",
    "        i += 1\n",
    "\n",
    "neuron.__init__ = __init__\n",
    "\n",
    "def pred(self, x):\n",
    "    if len(x) != len(self.w):\n",
    "        raise ValueError(\"Input dimension does not match neuron weight dimension\")\n",
    "    else:\n",
    "        wx = Node(0)\n",
    "        for i in range(0, self.dim):\n",
    "            wx += x[i]*self.w[i]\n",
    "        y = wx + self.b\n",
    "        return y\n",
    "\n",
    "neuron.pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd57a4a",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba23d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64.70027579457376 -97.05041369186063 -32.35013789728688\n",
      "manual: -64.70027579457376 -97.05041369186063 -32.35013789728688\n",
      "step: 0 Loss: 261.63285549336916\n",
      "engine: -6.209737713378473 -6.209737713378473 -6.209737713378473\n",
      "manual: -6.209737713378473 -6.209737713378473 -6.209737713378473\n",
      "step: 0 Loss: 9.640210617238727\n",
      "engine: -45.09386152088227 -67.6407922813234 -22.546930760441136\n",
      "manual: -45.09386152088227 -67.6407922813234 -22.546930760441136\n",
      "step: 1 Loss: 127.09102167903167\n",
      "engine: -3.131521759322829 -3.131521759322829 -3.131521759322829\n",
      "manual: -3.131521759322829 -3.131521759322829 -3.131521759322829\n",
      "step: 1 Loss: 2.4516071322780864\n",
      "engine: -31.71601507279776 -47.57402260919664 -15.85800753639888\n",
      "manual: -31.71601507279776 -47.57402260919664 -15.85800753639888\n",
      "step: 2 Loss: 62.869100756120915\n",
      "engine: -1.0406695493955933 -1.0406695493955933 -1.0406695493955933\n",
      "manual: -1.0406695493955933 -1.0406695493955933 -1.0406695493955933\n",
      "step: 2 Loss: 0.2707482777598068\n",
      "engine: -22.58577016055944 -33.878655240839166 -11.29288508027972\n",
      "manual: -22.58577016055944 -33.878655240839166 -11.29288508027972\n",
      "step: 3 Loss: 31.88231335910108\n",
      "engine: 0.3769168332017081 0.3769168332017081 0.3769168332017081\n",
      "manual: 0.3769168332017081 0.3769168332017081 0.3769168332017081\n",
      "step: 3 Loss: 0.03551657478770106\n",
      "engine: -16.35221455557121 -24.528321833356813 -8.176107277785604\n",
      "manual: -16.35221455557121 -24.528321833356813 -8.176107277785604\n",
      "step: 4 Loss: 16.712182554464682\n",
      "engine: 1.3354346965438815 1.3354346965438815 1.3354346965438815\n",
      "manual: 1.3354346965438815 1.3354346965438815 1.3354346965438815\n",
      "step: 4 Loss: 0.4458464571833122\n",
      "engine: -12.094098807181794 -18.14114821077269 -6.047049403590897\n",
      "manual: -12.094098807181794 -18.14114821077269 -6.047049403590897\n",
      "step: 5 Loss: 9.141701622367256\n",
      "engine: 1.980954543182154 1.980954543182154 1.980954543182154\n",
      "manual: 1.980954543182154 1.980954543182154 1.980954543182154\n",
      "step: 5 Loss: 0.9810452255385042\n",
      "engine: -9.183180231534607 -13.77477034730191 -4.591590115767303\n",
      "manual: -9.183180231534607 -13.77477034730191 -4.591590115767303\n",
      "step: 6 Loss: 5.270674947802999\n",
      "engine: 2.4130880844833023 2.4130880844833023 2.4130880844833023\n",
      "manual: 2.4130880844833023 2.4130880844833023 2.4130880844833023\n",
      "step: 6 Loss: 1.4557485258688232\n",
      "engine: -7.191030906980913 -10.78654636047137 -3.5955154534904565\n",
      "manual: -7.191030906980913 -10.78654636047137 -3.5955154534904565\n",
      "step: 7 Loss: 3.231932844072171\n",
      "engine: 2.6997646538331583 2.6997646538331583 2.6997646538331583\n",
      "manual: 2.6997646538331583 2.6997646538331583 2.6997646538331583\n",
      "step: 7 Loss: 1.8221822965217183\n",
      "engine: -5.825485769946212 -8.738228654919318 -2.912742884973106\n",
      "manual: -5.825485769946212 -8.738228654919318 -2.912742884973106\n",
      "step: 8 Loss: 2.1210177784903634\n",
      "engine: 2.887307920799943 2.887307920799943 2.887307920799943\n",
      "manual: 2.887307920799943 2.887307920799943 2.887307920799943\n",
      "step: 8 Loss: 2.084136757378522\n",
      "engine: -4.887303655353271 -7.330955483029907 -2.4436518276766357\n",
      "manual: -4.887303655353271 -7.330955483029907 -2.4436518276766357\n",
      "step: 9 Loss: 1.4928585637268406\n",
      "engine: 3.0073076648731405 3.0073076648731405 3.0073076648731405\n",
      "manual: 3.0073076648731405 3.0073076648731405 3.0073076648731405\n",
      "step: 9 Loss: 2.2609748478011853\n",
      "engine: -4.240612471423908 -6.360918707135863 -2.120306235711954\n",
      "manual: -4.240612471423908 -6.360918707135863 -2.120306235711954\n",
      "step: 10 Loss: 1.1239246332997492\n",
      "engine: 3.0813059532661846 3.0813059532661846 3.0813059532661846\n",
      "manual: 3.0813059532661846 3.0813059532661846 3.0813059532661846\n",
      "step: 10 Loss: 2.3736115944084077\n",
      "engine: -3.792754408209099 -5.689131612313648 -1.8963772041045495\n",
      "manual: -3.792754408209099 -5.689131612313648 -1.8963772041045495\n",
      "step: 11 Loss: 0.8990616250618471\n",
      "engine: 3.12399286056276 3.12399286056276 3.12399286056276\n",
      "manual: 3.12399286056276 3.12399286056276 3.12399286056276\n",
      "step: 11 Loss: 2.439832848211774\n",
      "engine: -3.4805414604456217 -5.220812190668433 -1.7402707302228109\n",
      "manual: -3.4805414604456217 -5.220812190668433 -1.7402707302228109\n",
      "step: 12 Loss: 0.7571355536175588\n",
      "engine: 3.1453857765557327 3.1453857765557327 3.1453857765557327\n",
      "manual: 3.1453857765557327 3.1453857765557327 3.1453857765557327\n",
      "step: 12 Loss: 2.4733629208397776\n",
      "engine: -3.2608824378942174 -4.891323656841326 -1.6304412189471087\n",
      "manual: -3.2608824378942174 -4.891323656841326 -1.6304412189471087\n",
      "step: 13 Loss: 0.6645846421104334\n",
      "engine: 3.1523155762360417 3.1523155762360417 3.1523155762360417\n",
      "manual: 3.1523155762360417 3.1523155762360417 3.1523155762360417\n",
      "step: 13 Loss: 2.484273373045092\n",
      "engine: -3.104391093580489 -4.656586640370733 -1.5521955467902444\n",
      "manual: -3.104391093580489 -4.656586640370733 -1.5521955467902444\n",
      "step: 14 Loss: 0.6023277538688665\n",
      "engine: 3.1494401072767086 3.1494401072767086 3.1494401072767086\n",
      "manual: 3.1494401072767086 3.1494401072767086 3.1494401072767086\n",
      "step: 14 Loss: 2.4797432473307817\n",
      "engine: -2.9910272131243545 -4.486540819686532 -1.4955136065621772\n",
      "manual: -2.9910272131243545 -4.486540819686532 -1.4955136065621772\n",
      "step: 15 Loss: 0.5591402368531526\n",
      "engine: 3.1399353336275677 3.1399353336275677 3.1399353336275677\n",
      "manual: 3.1399353336275677 3.1399353336275677 3.1399353336275677\n",
      "step: 15 Loss: 2.464798474840716\n",
      "engine: -2.9071240735201584 -4.360686110280238 -1.4535620367600792\n",
      "manual: -2.9071240735201584 -4.360686110280238 -1.4535620367600792\n",
      "step: 16 Loss: 0.5282106486775274\n",
      "engine: 3.125966658021122 3.125966658021122 3.125966658021122\n",
      "manual: 3.125966658021122 3.125966658021122 3.125966658021122\n",
      "step: 16 Loss: 2.4429168867649356\n",
      "engine: -2.8433613308595795 -4.265041996289369 -1.4216806654297898\n",
      "manual: -2.8433613308595795 -4.265041996289369 -1.4216806654297898\n",
      "step: 17 Loss: 0.5052939786142224\n",
      "engine: 3.10901033839143 3.10901033839143 3.10901033839143\n",
      "manual: 3.10901033839143 3.10901033839143 3.10901033839143\n",
      "step: 17 Loss: 2.4164863210561984\n",
      "engine: -2.793382639432835 -4.190073959149252 -1.3966913197164175\n",
      "manual: -2.793382639432835 -4.190073959149252 -1.3966913197164175\n",
      "step: 18 Loss: 0.487686660642797\n",
      "engine: 3.0900726764539144 3.0900726764539144 3.0900726764539144\n",
      "manual: 3.0900726764539144 3.0900726764539144 3.0900726764539144\n",
      "step: 18 Loss: 2.3871372864417646\n",
      "engine: -2.7528529427405886 -4.129279414110883 -1.3764264713702943\n",
      "manual: -2.7528529427405886 -4.129279414110883 -1.3764264713702943\n",
      "step: 19 Loss: 0.4736374577722199\n",
      "engine: 3.0698394924311163 3.0698394924311163 3.0698394924311163\n",
      "manual: 3.0698394924311163 3.0698394924311163 3.0698394924311163\n",
      "step: 19 Loss: 2.3559786273224335\n",
      "engine: -2.718815596956695 -4.078223395435042 -1.3594077984783475\n",
      "manual: -2.718815596956695 -4.078223395435042 -1.3594077984783475\n",
      "step: 20 Loss: 0.46199739064093687\n",
      "engine: 3.0487780587026485 3.0487780587026485 3.0487780587026485\n",
      "manual: 3.0487780587026485 3.0487780587026485 3.0487780587026485\n",
      "step: 20 Loss: 2.3237619128066727\n",
      "engine: -2.689253963897464 -4.033880945846196 -1.344626981948732\n",
      "manual: -2.689253963897464 -4.033880945846196 -1.344626981948732\n",
      "step: 21 Loss: 0.4520054301461389\n",
      "engine: 3.0272066130143376 3.0272066130143376 3.0272066130143376\n",
      "manual: 3.0272066130143376 3.0272066130143376 3.0272066130143376\n",
      "step: 21 Loss: 2.2909949694694345\n",
      "engine: -2.6627924411296036 -3.9941886616944053 -1.3313962205648018\n",
      "manual: -2.6627924411296036 -3.9941886616944053 -1.3313962205648018\n",
      "step: 22 Loss: 0.44315397403355955\n",
      "engine: 3.0053417627012546 3.0053417627012546 3.0053417627012546\n",
      "manual: 3.0053417627012546 3.0053417627012546 3.0053417627012546\n",
      "step: 22 Loss: 2.258019777659071\n",
      "engine: -2.6384925806616195 -3.9577388709924293 -1.3192462903308098\n",
      "manual: -2.6384925806616195 -3.9577388709924293 -1.3192462903308098\n",
      "step: 23 Loss: 0.4351026936379008\n",
      "engine: 2.983330811778874 2.983330811778874 2.983330811778874\n",
      "manual: 2.983330811778874 2.983330811778874 2.983330811778874\n",
      "step: 23 Loss: 2.2250656831272986\n",
      "engine: -2.6157140529033 -3.92357107935495 -1.30785702645165\n",
      "manual: -2.6157140529033 -3.92357107935495 -1.30785702645165\n",
      "step: 24 Loss: 0.427622500409738\n",
      "engine: 2.961273806246341 2.961273806246341 2.961273806246341\n",
      "manual: 2.961273806246341 2.961273806246341 2.961273806246341\n",
      "step: 24 Loss: 2.1922856388901732\n",
      "engine: -2.594019831589492 -3.8910297473842377 -1.297009915794746\n",
      "manual: -2.594019831589492 -3.8910297473842377 -1.297009915794746\n",
      "step: 25 Loss: 0.42055868041747346\n",
      "engine: 2.93923856776693 2.93923856776693 2.93923856776693\n",
      "manual: 2.93923856776693 2.93923856776693 2.93923856776693\n",
      "step: 25 Loss: 2.1597808395621483\n",
      "engine: -2.5731115350085005 -3.859667302512751 -1.2865557675042503\n",
      "manual: -2.5731115350085005 -3.859667302512751 -1.2865557675042503\n",
      "step: 26 Loss: 0.4138064357246126\n",
      "engine: 2.917270945801425 2.917270945801425 2.917270945801425\n",
      "manual: 2.917270945801425 2.917270945801425 2.917270945801425\n",
      "step: 26 Loss: 2.127617442804285\n",
      "engine: -2.552785332198461 -3.8291779982976912 -1.2763926660992304\n",
      "manual: -2.552785332198461 -3.8291779982976912 -1.2763926660992304\n",
      "step: 27 Loss: 0.4072945595179754\n",
      "engine: 2.8954018089852465 2.8954018089852465 2.8954018089852465\n",
      "manual: 2.8954018089852465 2.8954018089852465 2.8954018089852465\n",
      "step: 27 Loss: 2.0958379088687593\n",
      "engine: -2.5329018733393482 -3.7993528100090224 -1.2664509366696741\n",
      "manual: -2.5329018733393482 -3.7993528100090224 -1.2664509366696741\n",
      "step: 28 Loss: 0.40097449374787375\n",
      "engine: 2.873651812846493 2.873651812846493 2.873651812846493\n",
      "manual: 2.873651812846493 2.873651812846493 2.873651812846493\n",
      "step: 28 Loss: 2.064468685368984\n",
      "engine: -2.5133657838874868 -3.77004867583123 -1.2566828919437434\n",
      "manual: -2.5133657838874868 -3.77004867583123 -1.2566828919437434\n",
      "step: 29 Loss: 0.39481297272602256\n",
      "engine: 2.852034651108953 2.852034651108953 2.852034651108953\n",
      "manual: 2.852034651108953 2.852034651108953 2.852034651108953\n",
      "step: 29 Loss: 2.033525412781542\n",
      "engine: -2.494111680665142 -3.7411675209977133 -1.247055840332571\n",
      "manual: -2.494111680665142 -3.7411675209977133 -1.247055840332571\n",
      "step: 30 Loss: 0.38878706722689377\n",
      "engine: 2.8305592728823257 2.8305592728823257 2.8305592728823257\n",
      "manual: 2.8305592728823257 2.8305592728823257 2.8305592728823257\n",
      "step: 30 Loss: 2.00301644932503\n",
      "engine: -2.475094635570649 -3.7126419533559734 -1.2375473177853245\n",
      "manual: -2.475094635570649 -3.7126419533559734 -1.2375473177853245\n",
      "step: 31 Loss: 0.3828808409394127\n",
      "engine: 2.809231394643625 2.809231394643625 2.809231394643625\n",
      "manual: 2.809231394643625 2.809231394643625 2.809231394643625\n",
      "step: 31 Loss: 1.9729452571628414\n",
      "engine: -2.4562836723253483 -3.6844255084880224 -1.2281418361626741\n",
      "manual: -2.4562836723253483 -3.6844255084880224 -1.2281418361626741\n",
      "step: 32 Loss: 0.37708309243325616\n",
      "engine: 2.7880545313045246 2.7880545313045246 2.7880545313045246\n",
      "manual: 2.7880545313045246 2.7880545313045246 2.7880545313045246\n",
      "step: 32 Loss: 1.9433120173819232\n",
      "engine: -2.4376573315873316 -3.6564859973809973 -1.2188286657936658\n",
      "manual: -2.4376573315873316 -3.6564859973809973 -1.2188286657936658\n",
      "step: 33 Loss: 0.3713858291400919\n",
      "engine: 2.7670306993214933 2.7670306993214933 2.7670306993214933\n",
      "manual: 2.7670306993214933 2.7670306993214933 2.7670306993214933\n",
      "step: 33 Loss: 1.914114722746898\n",
      "engine: -2.4192006465800517 -3.6288009698700776 -1.2096003232900259\n",
      "manual: -2.4192006465800517 -3.6288009698700776 -1.2096003232900259\n",
      "step: 34 Loss: 0.36578323552583375\n",
      "engine: 2.746160896157008 2.746160896157008 2.746160896157008\n",
      "manual: 2.746160896157008 2.746160896157008 2.746160896157008\n",
      "step: 34 Loss: 1.885349916895465\n",
      "engine: -2.400903080615315 -3.6013546209229723 -1.2004515403076574\n",
      "manual: -2.400903080615315 -3.6013546209229723 -1.2004515403076574\n",
      "step: 35 Loss: 0.3602709751567568\n",
      "engine: 2.725445427224507 2.725445427224507 2.725445427224507\n",
      "manual: 2.725445427224507 2.725445427224507 2.725445427224507\n",
      "step: 35 Loss: 1.8570131941947439\n",
      "engine: -2.382757120576912 -3.5741356808653677 -1.191378560288456\n",
      "manual: -2.382757120576912 -3.5741356808653677 -1.191378560288456\n",
      "step: 36 Loss: 0.35484571847874846\n",
      "engine: 2.7048841288256504 2.7048841288256504 2.7048841288256504\n",
      "manual: 2.7048841288256504 2.7048841288256504 2.7048841288256504\n",
      "step: 36 Loss: 1.8290995375932244\n",
      "engine: -2.3647573177335346 -3.547135976600302 -1.1823786588667673\n",
      "manual: -2.3647573177335346 -3.547135976600302 -1.1823786588667673\n",
      "step: 37 Loss: 0.3495048232358938\n",
      "engine: 2.684476520160123 2.684476520160123 2.684476520160123\n",
      "manual: 2.684476520160123 2.684476520160123 2.684476520160123\n",
      "step: 37 Loss: 1.801603546822751\n",
      "engine: -2.346899633606583 -3.5203494504098742 -1.1734498168032914\n",
      "manual: -2.346899633606583 -3.5203494504098742 -1.1734498168032914\n",
      "step: 38 Loss: 0.34424611813891953\n",
      "engine: 2.6642219069669117 2.6642219069669117 2.6642219069669117\n",
      "manual: 2.6642219069669117 2.6642219069669117 2.6642219069669117\n",
      "step: 38 Loss: 1.7745195923906019\n",
      "engine: -2.329180993868789 -3.4937714908031836 -1.1645904969343945\n",
      "manual: -2.329180993868789 -3.4937714908031836 -1.1645904969343945\n",
      "step: 39 Loss: 0.339067756387475\n",
      "engine: 2.644119452181025 2.644119452181025 2.644119452181025\n",
      "manual: 2.644119452181025 2.644119452181025 2.644119452181025\n",
      "step: 39 Loss: 1.7478419193505208\n",
      "engine: -2.3115989841089757 -3.4673984761634635 -1.1557994920544878\n",
      "manual: -2.3115989841089757 -3.4673984761634635 -1.1557994920544878\n",
      "step: 40 Loss: 0.333968116458353\n",
      "engine: 2.6241682240966995 2.6241682240966995 2.6241682240966995\n",
      "manual: 2.6241682240966995 2.6241682240966995 2.6241682240966995\n",
      "step: 40 Loss: 1.7215647170897064\n",
      "engine: -2.2941516423416743 -3.4412274635125115 -1.1470758211708372\n",
      "manual: -2.2941516423416743 -3.4412274635125115 -1.1470758211708372\n",
      "step: 41 Loss: 0.3289457348786876\n",
      "engine: 2.604367229191398 2.604367229191398 2.604367229191398\n",
      "manual: 2.604367229191398 2.604367229191398 2.604367229191398\n",
      "step: 41 Loss: 1.6956821661215202\n",
      "engine: -2.276837317491946 -3.415255976237919 -1.138418658745973\n",
      "manual: -2.276837317491946 -3.415255976237919 -1.138418658745973\n",
      "step: 42 Loss: 0.32399926064524504\n",
      "engine: 2.5847154344894303 2.5847154344894303 2.5847154344894303\n",
      "manual: 2.5847154344894303 2.5847154344894303 2.5847154344894303\n",
      "step: 42 Loss: 1.670188469321971\n",
      "engine: -2.2596545728716606 -3.389481859307491 -1.1298272864358303\n",
      "manual: -2.2596545728716606 -3.389481859307491 -1.1298272864358303\n",
      "step: 43 Loss: 0.3191274242937379\n",
      "engine: 2.565211782792364 2.565211782792364 2.565211782792364\n",
      "manual: 2.565211782792364 2.565211782792364 2.565211782792364\n",
      "step: 43 Loss: 1.6450778726441946\n",
      "engine: -2.2426021203377644 -3.3639031805066466 -1.1213010601688822\n",
      "manual: -2.2426021203377644 -3.3639031805066466 -1.1213010601688822\n",
      "step: 44 Loss: 0.3143290168839648\n",
      "engine: 2.5458552030450896 2.5458552030450896 2.5458552030450896\n",
      "manual: 2.5458552030450896 2.5458552030450896 2.5458552030450896\n",
      "step: 44 Loss: 1.6203446787179385\n",
      "engine: -2.225678775374007 -3.3385181630610106 -1.1128393876870035\n",
      "manual: -2.225678775374007 -3.3385181630610106 -1.1128393876870035\n",
      "step: 45 Loss: 0.3096028756968962\n",
      "engine: 2.5266446173848234 2.5266446173848234 2.5266446173848234\n",
      "manual: 2.5266446173848234 2.5266446173848234 2.5266446173848234\n",
      "step: 45 Loss: 1.5959832556399252\n",
      "engine: -2.208883426441645 -3.3133251396624672 -1.1044417132208224\n",
      "manual: -2.208883426441645 -3.3133251396624672 -1.1044417132208224\n",
      "step: 46 Loss: 0.30494787447553634\n",
      "engine: 2.5075789459282323 2.5075789459282323 2.5075789459282323\n",
      "manual: 2.5075789459282323 2.5075789459282323 2.5075789459282323\n",
      "step: 46 Loss: 1.571988042515636\n",
      "engine: -2.192215014060757 -3.2883225210911355 -1.0961075070303785\n",
      "manual: -2.192215014060757 -3.2883225210911355 -1.0961075070303785\n",
      "step: 47 Loss: 0.3003629167420878\n",
      "engine: 2.4886571100161827 2.4886571100161827 2.4886571100161827\n",
      "manual: 2.4886571100161827 2.4886571100161827 2.4886571100161827\n",
      "step: 47 Loss: 1.5483535528085246\n",
      "engine: -2.1756725165276336 -3.2635087747914504 -1.0878362582638168\n",
      "manual: -2.1756725165276336 -3.2635087747914504 -1.0878362582638168\n",
      "step: 48 Loss: 0.29584693119835537\n",
      "engine: 2.4698780344068716 2.4698780344068716 2.4698780344068716\n",
      "manual: 2.4698780344068716 2.4698780344068716 2.4698780344068716\n",
      "step: 48 Loss: 1.5250743762113879\n",
      "engine: -2.1592549401575383 -3.2388824102363074 -1.0796274700787691\n",
      "manual: -2.1592549401575383 -3.2388824102363074 -1.0796274700787691\n",
      "step: 49 Loss: 0.2913988685371709\n",
      "engine: 2.4512406487519094 2.4512406487519094 2.4512406487519094\n",
      "manual: 2.4512406487519094 2.4512406487519094 2.4512406487519094\n",
      "step: 49 Loss: 1.5021451795234204\n",
      "engine: -2.142961312613899 -3.2144419689208483 -1.0714806563069494\n",
      "manual: -2.142961312613899 -3.2144419689208483 -1.0714806563069494\n",
      "step: 50 Loss: 0.28701769920999276\n",
      "engine: 2.43274388858363 2.43274388858363 2.43274388858363\n",
      "manual: 2.43274388858363 2.43274388858363 2.43274388858363\n",
      "step: 50 Loss: 1.4795607068602503\n",
      "engine: -2.1267906783420756 -3.1901860175131134 -1.0633953391710378\n",
      "manual: -2.1267906783420756 -3.1901860175131134 -1.0633953391710378\n",
      "step: 51 Loss: 0.28270241184267164\n",
      "engine: 2.414386695969137 2.414386695969137 2.414386695969137\n",
      "manual: 2.414386695969137 2.414386695969137 2.414386695969137\n",
      "step: 51 Loss: 1.4573157794181915\n",
      "engine: -2.110742095438887 -3.1661131431583307 -1.0553710477194436\n",
      "manual: -2.110742095438887 -3.1661131431583307 -1.0553710477194436\n",
      "step: 52 Loss: 0.278452012091109\n",
      "engine: 2.3961680199373223 2.3961680199373223 2.3961680199373223\n",
      "manual: 2.3961680199373223 2.3961680199373223 2.3961680199373223\n",
      "step: 52 Loss: 1.435405294942587\n",
      "engine: -2.0948146335009525 -3.142221950251429 -1.0474073167504763\n",
      "manual: -2.0948146335009525 -3.142221950251429 -1.0474073167504763\n",
      "step: 53 Loss: 0.27426552179560815\n",
      "engine: 2.3780868167511393 2.3780868167511393 2.3780868167511393\n",
      "manual: 2.3780868167511393 2.3780868167511393 2.3780868167511393\n",
      "step: 53 Loss: 1.4138242270013917\n",
      "engine: -2.079007372140964 -3.118511058211446 -1.039503686070482\n",
      "manual: -2.079007372140964 -3.118511058211446 -1.039503686070482\n",
      "step: 54 Loss: 0.2701419783385298\n",
      "engine: 2.3601420500745274 2.3601420500745274 2.3601420500745274\n",
      "manual: 2.3601420500745274 2.3601420500745274 2.3601420500745274\n",
      "step: 54 Loss: 1.3925676241324982\n",
      "engine: -2.0633193999593757 -3.0949790999390636 -1.0316596999796879\n",
      "manual: -2.0633193999593757 -3.0949790999390636 -1.0316596999796879\n",
      "step: 55 Loss: 0.2660804341405449\n",
      "engine: 2.34233269106762 2.34233269106762 2.34233269106762\n",
      "manual: 2.34233269106762 2.34233269106762 2.34233269106762\n",
      "step: 55 Loss: 1.3716306089110195\n",
      "engine: -2.047749813826975 -3.0716247207404628 -1.0238749069134876\n",
      "manual: -2.047749813826975 -3.0716247207404628 -1.0238749069134876\n",
      "step: 56 Loss: 0.2620799562517757\n",
      "engine: 2.3246577184331834 2.3246577184331834 2.3246577184331834\n",
      "manual: 2.3246577184331834 2.3246577184331834 2.3246577184331834\n",
      "step: 56 Loss: 1.3510083769677435\n",
      "engine: -2.0322977183793824 -3.0484465775690737 -1.0161488591896912\n",
      "manual: -2.0322977183793824 -3.0484465775690737 -1.0161488591896912\n",
      "step: 57 Loss: 0.25813962600812773\n",
      "engine: 2.3071161184299545 2.3071161184299545 2.3071161184299545\n",
      "manual: 2.3071161184299545 2.3071161184299545 2.3071161184299545\n",
      "step: 57 Loss: 1.330696195979825\n",
      "engine: -2.01696222565635 -3.025443338484525 -1.008481112828175\n",
      "manual: -2.01696222565635 -3.025443338484525 -1.008481112828175\n",
      "step: 58 Loss: 0.25425853873278853\n",
      "engine: 2.289706884863538 2.289706884863538 2.289706884863538\n",
      "manual: 2.289706884863538 2.289706884863538 2.289706884863538\n",
      "step: 58 Loss: 1.3106894046478716\n",
      "engine: -2.001742454839814 -3.002613682259721 -1.000871227419907\n",
      "manual: -2.001742454839814 -3.002613682259721 -1.000871227419907\n",
      "step: 59 Loss: 0.2504358034692578\n",
      "engine: 2.272429019062116 2.272429019062116 2.272429019062116\n",
      "manual: 2.272429019062116 2.272429019062116 2.272429019062116\n",
      "step: 59 Loss: 1.290983411668903\n",
      "engine: -1.986637532059575 -2.9799562980893626 -0.9933187660297875\n",
      "manual: -1.986637532059575 -2.9799562980893626 -0.9933187660297875\n",
      "step: 60 Loss: 0.24667054273673494\n",
      "engine: 2.2552815298419624 2.2552815298419624 2.2552815298419624\n",
      "manual: 2.2552815298419624 2.2552815298419624 2.2552815298419624\n",
      "step: 60 Loss: 1.2715736947115757\n",
      "engine: -1.9716465902449656 -2.9574698853674484 -0.9858232951224828\n",
      "manual: -1.9716465902449656 -2.9574698853674484 -0.9858232951224828\n",
      "step: 61 Loss: 0.24296189230153745\n",
      "engine: 2.2382634334661446 2.2382634334661446 2.2382634334661446\n",
      "manual: 2.2382634334661446 2.2382634334661446 2.2382634334661446\n",
      "step: 61 Loss: 1.2524557993979137\n",
      "engine: -1.9567687690082423 -2.9351531535123634 -0.9783843845041211\n",
      "manual: -1.9567687690082423 -2.9351531535123634 -0.9783843845041211\n",
      "step: 62 Loss: 0.239309000960377\n",
      "engine: 2.2213737535986713 2.2213737535986713 2.2213737535986713\n",
      "manual: 2.2213737535986713 2.2213737535986713 2.2213737535986713\n",
      "step: 62 Loss: 1.2336253382942626\n",
      "engine: -1.9420032145496222 -2.9130048218244333 -0.9710016072748111\n",
      "manual: -1.9420032145496222 -2.9130048218244333 -0.9710016072748111\n",
      "step: 63 Loss: 0.23571103033256663\n",
      "engine: 2.2046115212557282 2.2046115212557282 2.2046115212557282\n",
      "manual: 2.2046115212557282 2.2046115212557282 2.2046115212557282\n",
      "step: 63 Loss: 1.2150779899133741\n",
      "engine: -1.9273490795771053 -2.891023619365658 -0.9636745397885527\n",
      "manual: -1.9273490795771053 -2.891023619365658 -0.9636745397885527\n",
      "step: 64 Loss: 0.2321671546591697\n",
      "engine: 2.18797577475501 2.18797577475501 2.18797577475501\n",
      "manual: 2.18797577475501 2.18797577475501 2.18797577475501\n",
      "step: 64 Loss: 1.1968094977286967\n",
      "engine: -1.912805523236706 -2.869208284855059 -0.956402761618353\n",
      "manual: -1.912805523236706 -2.869208284855059 -0.956402761618353\n",
      "step: 65 Loss: 0.22867656060780306\n",
      "engine: 2.171465559663911 2.171465559663911 2.171465559663911\n",
      "manual: 2.171465559663911 2.171465559663911 2.171465559663911\n",
      "step: 65 Loss: 1.1788156692016258\n",
      "engine: -1.8983717110497764 -2.8475575665746646 -0.9491858555248882\n",
      "manual: -1.8983717110497764 -2.8475575665746646 -0.9491858555248882\n",
      "step: 66 Loss: 0.22523844708212848\n",
      "engine: 2.155079928747062 2.155079928747062 2.155079928747062\n",
      "manual: 2.155079928747062 2.155079928747062 2.155079928747062\n",
      "step: 66 Loss: 1.1610923748221105\n",
      "engine: -1.8840468148551395 -2.826070222282709 -0.9420234074275697\n",
      "manual: -1.8840468148551395 -2.826070222282709 -0.9420234074275697\n",
      "step: 67 Loss: 0.22185202503536225\n",
      "engine: 2.138817941913546 2.138817941913546 2.138817941913546\n",
      "manual: 2.138817941913546 2.138817941913546 2.138817941913546\n",
      "step: 67 Loss: 1.143635547162824\n",
      "engine: -1.8698300127549459 -2.804745019132419 -0.9349150063774729\n",
      "manual: -1.8698300127549459 -2.804745019132419 -0.9349150063774729\n",
      "step: 68 Loss: 0.21851651728744756\n",
      "engine: 2.122678666164031 2.122678666164031 2.122678666164031\n",
      "manual: 2.122678666164031 2.122678666164031 2.122678666164031\n",
      "step: 68 Loss: 1.1264411799469776\n",
      "engine: -1.8557204890629322 -2.7835807335943983 -0.9278602445314661\n",
      "manual: -1.8557204890629322 -2.7835807335943983 -0.9278602445314661\n",
      "step: 69 Loss: 0.21523115834549802\n",
      "engine: 2.1066611755379636 2.1066611755379636 2.1066611755379636\n",
      "manual: 2.1066611755379636 2.1066611755379636 2.1066611755379636\n",
      "step: 69 Loss: 1.1095053271297486\n",
      "engine: -1.8417174342544271 -2.7625761513816407 -0.9208587171272136\n",
      "manual: -1.8417174342544271 -2.7625761513816407 -0.9208587171272136\n",
      "step: 70 Loss: 0.21199519422729438\n",
      "engine: 2.0907645510609534 2.0907645510609534 2.0907645510609534\n",
      "manual: 2.0907645510609534 2.0907645510609534 2.0907645510609534\n",
      "step: 70 Loss: 1.0928241019932774\n",
      "engine: -1.8278200449178144 -2.7417300673767215 -0.9139100224589072\n",
      "manual: -1.8278200449178144 -2.7417300673767215 -0.9139100224589072\n",
      "step: 71 Loss: 0.20880788228771005\n",
      "engine: 2.0749878806923636 2.0749878806923636 2.0749878806923636\n",
      "manual: 2.0749878806923636 2.0749878806923636 2.0749878806923636\n",
      "step: 71 Loss: 1.0763936762550466\n",
      "engine: -1.8140275237069972 -2.7210412855604957 -0.9070137618534986\n",
      "manual: -1.8140275237069972 -2.7210412855604957 -0.9070137618534986\n",
      "step: 72 Loss: 0.20566849104790877\n",
      "engine: 2.059330259273244 2.059330259273244 2.059330259273244\n",
      "manual: 2.059330259273244 2.059330259273244 2.059330259273244\n",
      "step: 72 Loss: 1.0602102791896015\n",
      "engine: -1.8003390792946092 -2.700508618941914 -0.9001695396473046\n",
      "manual: -1.8003390792946092 -2.700508618941914 -0.9001695396473046\n",
      "step: 73 Loss: 0.20257630002721008\n",
      "engine: 2.0437907884745243 2.0437907884745243 2.0437907884745243\n",
      "manual: 2.0437907884745243 2.0437907884745243 2.0437907884745243\n",
      "step: 73 Loss: 1.0442701967633294\n",
      "engine: -1.7867539263260142 -2.6801308894890212 -0.8933769631630071\n",
      "manual: -1.7867539263260142 -2.6801308894890212 -0.8933769631630071\n",
      "step: 74 Loss: 0.19953059957758923\n",
      "engine: 2.0283685767456134 2.0283685767456134 2.0283685767456134\n",
      "manual: 2.0283685767456134 2.0283685767456134 2.0283685767456134\n",
      "step: 74 Loss: 1.0285697707822563\n",
      "engine: -1.7732712853736814 -2.659906928060522 -0.8866356426868407\n",
      "manual: -1.7732712853736814 -2.659906928060522 -0.8866356426868407\n",
      "step: 75 Loss: 0.19653069072067678\n",
      "engine: 2.0130627392632956 2.0130627392632956 2.0130627392632956\n",
      "manual: 2.0130627392632956 2.0130627392632956 2.0130627392632956\n",
      "step: 75 Loss: 1.0131053980525608\n",
      "engine: -1.7598903828922374 -2.639835574338356 -0.8799451914461187\n",
      "manual: -1.7598903828922374 -2.639835574338356 -0.8799451914461187\n",
      "step: 76 Loss: 0.19357588498728662\n",
      "engine: 1.9978723978810322 1.9978723978810322 1.9978723978810322\n",
      "manual: 1.9978723978810322 1.9978723978810322 1.9978723978810322\n",
      "step: 76 Loss: 0.9978735295537263\n",
      "engine: -1.7466104511738436 -2.6199156767607654 -0.8733052255869218\n",
      "manual: -1.7466104511738436 -2.6199156767607654 -0.8733052255869218\n",
      "step: 77 Loss: 0.1906655042593561\n",
      "engine: 1.9827966810786037 1.9827966810786037 1.9827966810786037\n",
      "manual: 1.9827966810786037 1.9827966810786037 1.9827966810786037\n",
      "step: 77 Loss: 0.9828706696240815\n",
      "engine: -1.733430728304036 -2.600146092456054 -0.866715364152018\n",
      "manual: -1.733430728304036 -2.600146092456054 -0.866715364152018\n",
      "step: 78 Loss: 0.18779888061429129\n",
      "engine: 1.96783472391213 1.96783472391213 1.96783472391213\n",
      "manual: 1.96783472391213 1.96783472391213 1.96783472391213\n",
      "step: 78 Loss: 0.9680933751585823\n",
      "engine: -1.7203504581178208 -2.5805256871767313 -0.8601752290589104\n",
      "manual: -1.7203504581178208 -2.5805256871767313 -0.8601752290589104\n",
      "step: 79 Loss: 0.18497535617163724\n",
      "engine: 1.9529856679644713 1.9529856679644713 1.9529856679644713\n",
      "manual: 1.9529856679644713 1.9529856679644713 1.9529856679644713\n",
      "step: 79 Loss: 0.953538254818658\n",
      "engine: -1.707368890156303 -2.5610533352344547 -0.8536844450781516\n",
      "manual: -1.707368890156303 -2.5610533352344547 -0.8536844450781516\n",
      "step: 80 Loss: 0.18219428294209788\n",
      "engine: 1.9382486612959813 1.9382486612959813 1.9382486612959813\n",
      "manual: 1.9382486612959813 1.9382486612959813 1.9382486612959813\n",
      "step: 80 Loss: 0.9392019682539159\n",
      "engine: -1.6944852796235708 -2.541727919435356 -0.8472426398117854\n",
      "manual: -1.6944852796235708 -2.541727919435356 -0.8472426398117854\n",
      "step: 81 Loss: 0.17945502267881067\n",
      "engine: 1.9236228583956354 1.9236228583956354 1.9236228583956354\n",
      "manual: 1.9236228583956354 1.9236228583956354 1.9236228583956354\n",
      "step: 81 Loss: 0.9250812253355487\n",
      "engine: -1.6816988873439271 -2.5225483310158907 -0.8408494436719636\n",
      "manual: -1.6816988873439271 -2.5225483310158907 -0.8408494436719636\n",
      "step: 82 Loss: 0.17675694673086265\n",
      "engine: 1.9091074201325338 1.9091074201325338 1.9091074201325338\n",
      "manual: 1.9091074201325338 1.9091074201325338 1.9091074201325338\n",
      "step: 82 Loss: 0.9111727854012747\n",
      "engine: -1.6690089797194432 -2.5035134695791648 -0.8345044898597216\n",
      "manual: -1.6690089797194432 -2.5035134695791648 -0.8345044898597216\n",
      "step: 83 Loss: 0.17409943589900853\n",
      "engine: 1.894701513707746 1.894701513707746 1.894701513707746\n",
      "manual: 1.894701513707746 1.894701513707746 1.894701513707746\n",
      "step: 83 Loss: 0.897473456511606\n",
      "engine: -1.6564148286878648 -2.4846222430317972 -0.8282074143439324\n",
      "manual: -1.6564148286878648 -2.4846222430317972 -0.8282074143439324\n",
      "step: 84 Loss: 0.17148188029356554\n",
      "engine: 1.8804043126065526 1.8804043126065526 1.8804043126065526\n",
      "manual: 1.8804043126065526 1.8804043126065526 1.8804043126065526\n",
      "step: 84 Loss: 0.8839800947173304\n",
      "engine: -1.6439157116808403 -2.4658735675212604 -0.8219578558404201\n",
      "manual: -1.6439157116808403 -2.4658735675212604 -0.8219578558404201\n",
      "step: 85 Loss: 0.1689036791944452\n",
      "engine: 1.86621499655101 1.86621499655101 1.86621499655101\n",
      "manual: 1.86621499655101 1.86621499655101 1.86621499655101\n",
      "step: 85 Loss: 0.8706896033379715\n",
      "engine: -1.6315109115824455 -2.4472663673736683 -0.8157554557912228\n",
      "manual: -1.6315109115824455 -2.4472663673736683 -0.8157554557912228\n",
      "step: 86 Loss: 0.16636424091328642\n",
      "engine: 1.8521327514528956 1.8521327514528956 1.8521327514528956\n",
      "manual: 1.8521327514528956 1.8521327514528956 1.8521327514528956\n",
      "step: 86 Loss: 0.8575989322511184\n",
      "engine: -1.6191997166880583 -2.4287995750320874 -0.8095998583440291\n",
      "manual: -1.6191997166880583 -2.4287995750320874 -0.8095998583440291\n",
      "step: 87 Loss: 0.16386298265766802\n",
      "engine: 1.8381567693670036 1.8381567693670036 1.8381567693670036\n",
      "manual: 1.8381567693670036 1.8381567693670036 1.8381567693670036\n",
      "step: 87 Loss: 0.8447050771924349\n",
      "engine: -1.6069814206634732 -2.4104721309952097 -0.8034907103317366\n",
      "manual: -1.6069814206634732 -2.4104721309952097 -0.8034907103317366\n",
      "step: 88 Loss: 0.16139933039734966\n",
      "engine: 1.8242862484447926 1.8242862484447926 1.8242862484447926\n",
      "manual: 1.8242862484447926 1.8242862484447926 1.8242862484447926\n",
      "step: 88 Loss: 0.8320050790661939\n",
      "engine: -1.5948553225044435 -2.3922829837566653 -0.7974276612522218\n",
      "manual: -1.5948553225044435 -2.3922829837566653 -0.7974276612522218\n",
      "step: 89 Loss: 0.15897271873254704\n",
      "engine: 1.8105203928883729 1.8105203928883729 1.8105203928883729\n",
      "manual: 1.8105203928883729 1.8105203928883729 1.8105203928883729\n",
      "step: 89 Loss: 0.819496023266167\n",
      "engine: -1.5828207264964291 -2.3742310897446437 -0.7914103632482146\n",
      "manual: -1.5828207264964291 -2.3742310897446437 -0.7914103632482146\n",
      "step: 90 Loss: 0.15658259076416772\n",
      "engine: 1.7968584129048555 1.7968584129048555 1.7968584129048555\n",
      "manual: 1.7968584129048555 1.7968584129048555 1.7968584129048555\n",
      "step: 90 Loss: 0.807175039006739\n",
      "engine: -1.5708769421745785 -2.3563154132618678 -0.7854384710872893\n",
      "manual: -1.5708769421745785 -2.3563154132618678 -0.7854384710872893\n",
      "step: 91 Loss: 0.15422839796598464\n",
      "engine: 1.7832995246610395 1.7832995246610395 1.7832995246610395\n",
      "manual: 1.7832995246610395 1.7832995246610395 1.7832995246610395\n",
      "step: 91 Loss: 0.7950392986640723\n",
      "engine: -1.5590232842843434 -2.338534926426515 -0.7795116421421717\n",
      "manual: -1.5590232842843434 -2.338534926426515 -0.7795116421421717\n",
      "step: 92 Loss: 0.1519096000587963\n",
      "engine: 1.7698429502384379 1.7698429502384379 1.7698429502384379\n",
      "manual: 1.7698429502384379 1.7698429502384379 1.7698429502384379\n",
      "step: 92 Loss: 0.7830860171271744\n",
      "engine: -1.5472590727419586 -2.320888609112938 -0.7736295363709793\n",
      "manual: -1.5472590727419586 -2.320888609112938 -0.7736295363709793\n",
      "step: 93 Loss: 0.1496256648863941\n",
      "engine: 1.7564879175886485 1.7564879175886485 1.7564879175886485\n",
      "manual: 1.7564879175886485 1.7564879175886485 1.7564879175886485\n",
      "step: 93 Loss: 0.7713124511587267\n",
      "engine: -1.5355836325954897 -2.3033754488932345 -0.7677918162977448\n",
      "manual: -1.5355836325954897 -2.3033754488932345 -0.7677918162977448\n",
      "step: 94 Loss: 0.1473760682934475\n",
      "engine: 1.7432336604890608 1.7432336604890608 1.7432336604890608\n",
      "manual: 1.7432336604890608 1.7432336604890608 1.7432336604890608\n",
      "step: 94 Loss: 0.7597158987655225\n",
      "engine: -1.523996293986123 -2.2859944409791844 -0.7619981469930615\n",
      "manual: -1.523996293986123 -2.2859944409791844 -0.7619981469930615\n",
      "step: 95 Loss: 0.14516029400521482\n",
      "engine: 1.7300794184988835 1.7300794184988835 1.7300794184988835\n",
      "manual: 1.7300794184988835 1.7300794184988835 1.7300794184988835\n",
      "step: 95 Loss: 0.7482936985783587\n",
      "engine: -1.512496392109739 -2.2687445881646084 -0.7562481960548695\n",
      "manual: -1.512496392109739 -2.2687445881646084 -0.7562481960548695\n",
      "step: 96 Loss: 0.14297783350906107\n",
      "engine: 1.717024436915537 1.717024436915537 1.717024436915537\n",
      "manual: 1.717024436915537 1.717024436915537 1.717024436915537\n",
      "step: 96 Loss: 0.7370432292412792\n",
      "engine: -1.5010832671787426 -2.251624900768114 -0.7505416335893713\n",
      "manual: -1.5010832671787426 -2.251624900768114 -0.7505416335893713\n",
      "step: 97 Loss: 0.14082818593775054\n",
      "engine: 1.704067966731328 1.704067966731328 1.704067966731328\n",
      "manual: 1.704067966731328 1.704067966731328 1.704067966731328\n",
      "step: 97 Loss: 0.7259619088099607\n",
      "engine: -1.489756264384205 -2.2346343965763076 -0.7448781321921025\n",
      "manual: -1.489756264384205 -2.2346343965763076 -0.7448781321921025\n",
      "step: 98 Loss: 0.13871085795449883\n",
      "engine: 1.6912092645904995 1.6912092645904995 1.6912092645904995\n",
      "manual: 1.6912092645904995 1.6912092645904995 1.6912092645904995\n",
      "step: 98 Loss: 0.7150471941591845\n",
      "engine: -1.478514733858347 -2.2177721007875206 -0.7392573669291735\n",
      "manual: -1.478514733858347 -2.2177721007875206 -0.7392573669291735\n",
      "step: 99 Loss: 0.1366253636397637\n",
      "engine: 1.678447592746572 1.678447592746572 1.678447592746572\n",
      "manual: 1.678447592746572 1.678447592746572 1.678447592746572\n",
      "step: 99 Loss: 0.7042965803991906\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "model = neuron(2)\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for w in model.w:\n",
    "            zero_grad(w)\n",
    "        zero_grad(model.b)\n",
    "\n",
    "        y = model.pred(x)\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "\n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", model.w[0].grad, model.w[1].grad, model.b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        for w in model.w:\n",
    "            w.value -= n * w.grad\n",
    "        model.b.value -= n* model.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192012b",
   "metadata": {},
   "source": [
    "## Phase 3 — Adding Depth (2 → 3 → 1)\n",
    "\n",
    "Now we build a multi-layer network:\n",
    "\n",
    "Input (2)\n",
    "  ↓\n",
    "Linear (2 → 3)\n",
    "  ↓\n",
    "ReLU\n",
    "  ↓\n",
    "Linear (3 → 1)\n",
    "  ↓\n",
    "Loss\n",
    "\n",
    "This is where real backpropagation complexity begins.\n",
    "\n",
    "We now test:\n",
    "\n",
    "- Gradient flow through depth\n",
    "- Nonlinear gating via ReLU\n",
    "- Reverse topological traversal correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9daae",
   "metadata": {},
   "source": [
    "### Layer Abstraction\n",
    "\n",
    "A layer is simply a collection of neurons sharing the same input.\n",
    "\n",
    "Each neuron corresponds to one row of a weight matrix.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "y = W·x + b\n",
    "\n",
    "Even though everything is scalar-based,\n",
    "this reproduces matrix multiplication behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f700aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class for the different layers\n",
    "class layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.neurons = [neuron(dim_in) for _ in range(dim_out)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        yout = []\n",
    "        if len(x) != self.dim_in:\n",
    "            raise ValueError(f\"Layer expected input dimension {self.dim_in}, got {len(x)}\")\n",
    "        else:\n",
    "            for neuron in self.neurons:\n",
    "                yout.append(neuron.pred(x))\n",
    "        return yout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5a0c3",
   "metadata": {},
   "source": [
    "### ReLU — First Nonlinearity\n",
    "\n",
    "ReLU(z) = max(0, z)\n",
    "\n",
    "Backward rule:\n",
    "\n",
    "If z > 0:\n",
    "    dL/dz = dL/da\n",
    "Else:\n",
    "    dL/dz = 0\n",
    "\n",
    "This introduces gradient gating.\n",
    "\n",
    "If a neuron’s pre-activation is negative,\n",
    "it receives zero gradient and does not learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3789416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(node):\n",
    "    out = Node(\n",
    "        value = max(0, node.value),\n",
    "        parents = (node,),\n",
    "    )\n",
    "    def backward():\n",
    "        if node.value > 0:\n",
    "            node.grad += out.grad\n",
    "        else:\n",
    "            node.grad += 0\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13776fb",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e5b2b",
   "metadata": {},
   "source": [
    "### Full Forward Pass\n",
    "\n",
    "For each example:\n",
    "\n",
    "1. h = hidden.forward(x)\n",
    "2. a = relu(h)  (element-wise)\n",
    "3. y = output.forward(a)\n",
    "4. L = (y − y_true)²\n",
    "\n",
    "Backward then computes gradients for all 13 parameters\n",
    "in a single reverse traversal.\n",
    "\n",
    "This is where reverse-mode autodiff becomes powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2d49dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layer(2,3)\n",
    "output = layer(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d81b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d511dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 0\n",
      "[4.5337523252251515, 6.800628487837727] 2.2668761626125757\n",
      "[0, 0] 0\n",
      "step: 0 Loss: 256.0311659201917\n",
      "[-0.6309703493253606, -0.6309703493253606] -0.6309703493253606\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 0 Loss: 21.888306391376396\n",
      "[-4.332121299492328, -6.498181949238491] -2.166060649746164\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 1 Loss: 242.86516024888584\n",
      "[-0.6793615958436997, -0.6793615958436997] -0.6793615958436997\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 1 Loss: 18.156986023145553\n",
      "[-5.715125570784611, -8.572688356176917] -2.8575627853923056\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 2 Loss: 229.64309145684152\n",
      "[-1.58899079319995, -1.58899079319995] -1.58899079319995\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 2 Loss: 14.522149504105649\n",
      "[-13.79427095987796, -20.691406439816937] -6.89713547993898\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 3 Loss: 213.28422711143315\n",
      "[-3.0567797603346607, -3.0567797603346607] -3.0567797603346607\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 3 Loss: 9.69387451918746\n",
      "[-28.9759901079676, -43.4639851619514] -14.4879950539838\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 4 Loss: 178.48744417625562\n",
      "[-2.719797906057739, -2.719797906057739] -2.719797906057739\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 4 Loss: 1.578819504615381\n",
      "[-42.4491371757942, -63.6737057636913] -21.2245685878971\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 5 Loss: 88.64569464356146\n",
      "[11.735642395599417, 11.735642395599417] 11.735642395599417\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 5 Loss: 9.336055704661971\n",
      "[-19.605169892574075, -29.407754838861113] -9.802584946287038\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 6 Loss: 8.040390853923311\n",
      "[17.175983085663002, 17.175983085663002] 17.175983085663002\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 6 Loss: 16.788069132849905\n",
      "[-12.011346863744501, -18.017020295616753] -6.005673431872251\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 7 Loss: 2.7242538052759397\n",
      "[14.547719568851221, 14.547719568851221] 14.547719568851221\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 7 Loss: 12.657629364251605\n",
      "[-12.884095777108563, -19.326143665662844] -6.442047888554281\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 8 Loss: 3.1484649776172837\n",
      "[14.21785709642924, 14.21785709642924] 14.21785709642924\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 8 Loss: 11.952585831077759\n",
      "[-11.545821837425912, -17.318732756138868] -5.772910918712956\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 9 Loss: 2.4691069712484737\n",
      "[13.300740365805158, 13.300740365805158] 13.300740365805158\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 9 Loss: 10.505066459275803\n",
      "[-11.009675634800926, -16.51451345220139] -5.504837817400463\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 10 Loss: 2.2100412158484333\n",
      "[12.638030988679404, 12.638030988679404] 12.638030988679404\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 10 Loss: 9.454475914172425\n",
      "[-10.270717864216103, -15.406076796324154] -5.135358932108051\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 11 Loss: 1.8869786469578271\n",
      "[11.945232746650754, 11.945232746650754] 11.945232746650754\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 11 Loss: 8.424862267378725\n",
      "[-9.675885337796407, -14.51382800669461] -4.8379426688982035\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 12 Loss: 1.6444203025782607\n",
      "[11.316530404514513, 11.316530404514513] 11.316530404514513\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 12 Loss: 7.5271022906879255\n",
      "[-9.083633434665714, -13.625450151998571] -4.541816717332857\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 13 Loss: 1.4223054522477343\n",
      "[10.710128039383235, 10.710128039383235] 10.710128039383235\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 13 Loss: 6.706595233732452\n",
      "[-8.544991422895997, -12.817487134343995] -4.2724957114479984\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 14 Loss: 1.235443265977519\n",
      "[10.138177469055714, 10.138177469055714] 10.138177469055714\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 14 Loss: 5.971798349912464\n",
      "[-8.031799834369899, -12.047699751554848] -4.015899917184949\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 15 Loss: 1.0714515425508075\n",
      "[9.591255394124314, 9.591255394124314] 9.591255394124314\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 15 Loss: 5.3078572206427515\n",
      "[-7.551985744381307, -11.327978616571961] -3.7759928721906535\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 16 Loss: 0.9300877878147362\n",
      "[9.07015211455746, 9.07015211455746] 9.07015211455746\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 16 Loss: 4.710973543719012\n",
      "[-7.097439941869033, -10.64615991280355] -3.5487199709345165\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 17 Loss: 0.8068230144718398\n",
      "[8.571525633797888, 8.571525633797888] 8.571525633797888\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 17 Loss: 4.173625280126149\n",
      "[-6.668616200671858, -10.002924301007788] -3.334308100335929\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 18 Loss: 0.6998057872713469\n",
      "[8.094575095761245, 8.094575095761245] 8.094575095761245\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 18 Loss: 3.690988832666446\n",
      "[-6.262158906814658, -9.393238360221988] -3.131079453407329\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 19 Loss: 0.606544649501725\n",
      "[7.6376564592860845, 7.6376564592860845] 7.6376564592860845\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 19 Loss: 3.2577908054823124\n",
      "[-5.877239761851698, -8.815859642777546] -2.938619880925849\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 20 Loss: 0.5253776404282249\n",
      "[7.199959812845584, 7.199959812845584] 7.199959812845584\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 20 Loss: 2.869721014346554\n",
      "[-5.5120428093282445, -8.268064213992368] -2.7560214046641223\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 21 Loss: 0.4546511564317412\n",
      "[6.7805416147543465, 6.7805416147543465] 6.7805416147543465\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 21 Loss: 2.522613038608657\n",
      "[-5.165667465879792, -7.748501198819687] -2.582833732939896\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 22 Loss: 0.3930690211740661\n",
      "[6.378806516740659, 6.378806516740659] 6.378806516740659\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 22 Loss: 2.2127893781659402\n",
      "[-4.836960931740547, -7.255441397610821] -2.4184804658702737\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 23 Loss: 0.3394456532172373\n",
      "[5.9941752847088745, 5.9941752847088745] 5.9941752847088745\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 23 Loss: 1.9368030558368126\n",
      "[-4.525164759350153, -6.787747139025229] -2.2625823796750764\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 24 Loss: 0.29279229985185207\n",
      "[5.626234407013309, 5.626234407013309] 5.626234407013309\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 24 Loss: 1.6915241661950389\n",
      "[-4.229468951686968, -6.344203427530452] -2.114734475843484\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 25 Loss: 0.2522273078397769\n",
      "[5.274600216871189, 5.274600216871189] 5.274600216871189\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 25 Loss: 1.4740425082265092\n",
      "[-3.949254006498572, -5.923881009747858] -1.974627003249286\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 26 Loss: 0.21699469586829034\n",
      "[4.938965540507739, 4.938965540507739] 4.938965540507739\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 26 Loss: 1.2816865941090074\n",
      "[-3.683898183279171, -5.525847274918757] -1.8419490916395855\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 27 Loss: 0.18642517647559878\n",
      "[4.619034984420175, 4.619034984420175] 4.619034984420175\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 27 Loss: 1.1119808238663742\n",
      "[-3.432875434399856, -5.1493131515997845] -1.716437717199928\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 28 Loss: 0.1599368089409106\n",
      "[4.314538209853481, 4.314538209853481] 4.314538209853481\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 28 Loss: 0.9626464830654896\n",
      "[-3.195663120885294, -4.793494681327941] -1.597831560442647\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 29 Loss: 0.13701525082093827\n",
      "[4.0251960648465746, 4.0251960648465746] 4.0251960648465746\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 29 Loss: 0.8315799978208124\n",
      "[-2.971784743785588, -4.457677115678383] -1.485892371892794\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 30 Loss: 0.11721010948482087\n",
      "[3.7507239427840164, 3.7507239427840164] 3.7507239427840164\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 30 Loss: 0.7168490898512304\n",
      "[-2.7607625223140615, -4.141143783471092] -1.3803812611570307\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 31 Loss: 0.10012360400748382\n",
      "[3.4908139345553684, 3.4908139345553684] 3.4908139345553684\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 31 Loss: 0.6166796787387434\n",
      "[-2.562138080310263, -3.8432071204653946] -1.2810690401551315\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 32 Loss: 0.08540639199056324\n",
      "[3.245136548205318, 3.245136548205318] 3.245136548205318\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 32 Loss: 0.5294504292313614\n",
      "[-2.3754473496689323, -3.5631710245033985] -1.1877236748344662\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 33 Loss: 0.07275037163815426\n",
      "[3.01333238803053, 3.01333238803053] 3.01333238803053\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 33 Loss: 0.4536830438159049\n",
      "[-2.2002323009052223, -3.3003484513578334] -1.1001161504526111\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 34 Loss: 0.06188493548879103\n",
      "[2.7950148782603175, 2.7950148782603175] 2.7950148782603175\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 34 Loss: 0.38803585755115405\n",
      "[-2.0360279151189746, -3.0540418726784617] -1.0180139575594873\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 35 Loss: 0.052572013318063895\n",
      "[2.5897679109514127, 2.5897679109514127] 2.5897679109514127\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 35 Loss: 0.3312952378883253\n",
      "[-1.8823701910811925, -2.823555286621789] -0.9411850955405963\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 36 Loss: 0.044602902183766974\n",
      "[2.3971499615436875, 2.3971499615436875] 2.3971499615436875\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 36 Loss: 0.28236842810487345\n",
      "[-1.7387899170245864, -2.6081848755368795] -0.8693949585122932\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 37 Loss: 0.037794628630443104\n",
      "[2.216695340354647, 2.216695340354647] 2.216695340354647\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 37 Loss: 0.24027521059008677\n",
      "[-1.604818943542943, -2.4072284153144143] -0.8024094717714715\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 38 Loss: 0.03198730547128131\n",
      "[2.047919226388344, 2.047919226388344] 2.047919226388344\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 38 Loss: 0.20414027728777254\n",
      "[-1.4799877497963125, -2.219981624694469] -0.7399938748981563\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 39 Loss: 0.027041340572008313\n",
      "[1.8903208090777615, 1.8903208090777615] 1.8903208090777615\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 39 Loss: 0.1731850342320596\n",
      "[-1.3638306352389813, -2.045745952858472] -0.6819153176194906\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 40 Loss: 0.022835243755270514\n",
      "[1.7433885522933845, 1.7433885522933845] 1.7433885522933845\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 40 Loss: 0.1467198852576994\n",
      "[-1.255885256041878, -1.883827884062817] -0.627942628020939\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 41 Loss: 0.019263428992790738\n",
      "[1.6066040017878078, 1.6066040017878078] 1.6066040017878078\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 41 Loss: 0.12413636979745063\n",
      "[-1.1556968882364755, -1.7335453323547134] -0.5778484441182378\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 42 Loss: 0.016234406482505653\n",
      "[1.4794466789190843, 1.4794466789190843] 1.4794466789190843\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 42 Loss: 0.10489975842632886\n",
      "[-1.0628188178661175, -1.5942282267991763] -0.5314094089330588\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 43 Loss: 0.013669034291743869\n",
      "[1.3613977459726818, 1.3613977459726818] 1.3613977459726818\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 43 Loss: 0.08854179006391427\n",
      "[-0.9768156932413777, -1.4652235398620665] -0.48840784662068887\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 44 Loss: 0.011499043491389074\n",
      "[1.251944130929194, 1.251944130929194] 1.251944130929194\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 44 Loss: 0.07465390242913748\n",
      "[-0.8972641154356954, -1.345896173153543] -0.4486320577178477\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 45 Loss: 0.0096656487146908\n",
      "[1.150581589380973, 1.150581589380973] 1.150581589380973\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 45 Loss: 0.06288078306876381\n",
      "[-0.823755106147463, -1.2356326592211944] -0.4118775530737315\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 46 Loss: 0.008118361452027177\n",
      "[1.0568178729084183, 1.0568178729084183] 1.0568178729084183\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 46 Loss: 0.05291443927921454\n",
      "[-0.7558945548504, -1.1338418322756] -0.3779472774252\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 47 Loss: 0.006813894593132797\n",
      "[0.9701749907617075, 0.9701749907617075] 0.9701749907617075\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 47 Loss: 0.04448867858960879\n",
      "[-0.6933048763036483, -1.0399573144554726] -0.34665243815182417\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 48 Loss: 0.005715222302044072\n",
      "[0.890191403960836, 0.890191403960836] 0.890191403960836\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 48 Loss: 0.037374102514723485\n",
      "[-0.6356251711930628, -0.9534377567895942] -0.3178125855965314\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 49 Loss: 0.004790726320978609\n",
      "[0.8164234703500935, 0.8164234703500935] 0.8164234703500935\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 49 Loss: 0.031373534032694904\n",
      "[-0.5825121886557811, -0.8737682829836717] -0.29125609432789057\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 50 Loss: 0.004013462956292036\n",
      "[0.7484467583774257, 0.7484467583774257] 0.7484467583774257\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 50 Loss: 0.026317922609774655\n",
      "[-0.53364017778365, -0.800460266675475] -0.266820088891825\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 51 Loss: 0.003360506420936529\n",
      "[0.6858567656945997, 0.6858567656945997] 0.6858567656945997\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 51 Loss: 0.022062661580874363\n",
      "[-0.4887012915935807, -0.7330519373903711] -0.24435064579679036\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 52 Loss: 0.0028123860147268036\n",
      "[0.6282695047906198, 0.6282695047906198] 0.6282695047906198\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 52 Loss: 0.01848432713619287\n",
      "[-0.44740517067170205, -0.6711077560075531] -0.22370258533585102\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 53 Loss: 0.0023525876135525045\n",
      "[0.5753216350249681, 0.5753216350249681] 0.5753216350249681\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 53 Loss: 0.015477783099644564\n",
      "[-0.40947892634257704, -0.6142183895138655] -0.20473946317128852\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 54 Loss: 0.001967127632025297\n",
      "[0.5266704897964795, 0.5266704897964795] 0.5266704897964795\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 54 Loss: 0.01295364218251366\n",
      "[-0.37466652418511526, -0.5619997862776729] -0.18733326209255763\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 55 Loss: 0.0016441792419203969\n",
      "[0.4819937730132118, 0.4819937730132118] 0.4819937730132118\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 55 Loss: 0.010836035929162238\n",
      "[-0.3427284715883335, -0.5140927073825003] -0.17136423579416676\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 56 Loss: 0.001373753984121808\n",
      "[0.4409891887359527, 0.4409891887359527] 0.4409891887359527\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 56 Loss: 0.009060675620131126\n",
      "[-0.31344107274921607, -0.4701616091238241] -0.15672053637460803\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 57 Loss: 0.0011474246639218584\n",
      "[0.40337384140692123, 0.40337384140692123] 0.40337384140692123\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 57 Loss: 0.007573164019094321\n",
      "[-0.28659592583397653, -0.4298938887509648] -0.14329796291698826\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 58 Loss: 0.0009580900946698445\n",
      "[0.36888360619989313, 0.36888360619989313] 0.36888360619989313\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 58 Loss: 0.0063275378592098485\n",
      "[-0.2619991130372813, -0.39299866955592194] -0.13099955651864065\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 59 Loss: 0.0007997717253715018\n",
      "[0.33727234967258096, 0.33727234967258096] 0.33727234967258096\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 59 Loss: 0.005285008239656776\n",
      "[-0.23947059034312682, -0.35920588551469024] -0.11973529517156341\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 60 Loss: 0.000667441520670187\n",
      "[0.30831115122160746, 0.30831115122160746] 0.30831115122160746\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 60 Loss: 0.004412879744565651\n",
      "[-0.21884336327870227, -0.3282650449180534] -0.10942168163935113\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 61 Loss: 0.0005568740138961031\n",
      "[0.2817874350045569, 0.2817874350045569] 0.2817874350045569\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 61 Loss: 0.0036836220857392305\n",
      "[-0.19996283113429367, -0.2999442467014405] -0.09998141556714683\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 62 Loss: 0.0004645214497102998\n",
      "[0.2575041254543912, 0.2575041254543912] 0.2575041254543912\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 62 Loss: 0.0030740775255642836\n",
      "[-0.18268598533748515, -0.2740289780062277] -0.09134299266874257\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 63 Loss: 0.0003874069767382386\n",
      "[0.23527875687180538, 0.23527875687180538] 0.23527875687180538\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 63 Loss: 0.0025647836433456504\n",
      "[-0.16688075171648892, -0.25032112757473335] -0.08344037585824446\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 64 Loss: 0.00032303473488818447\n",
      "[0.21494262181540308, 0.21494262181540308] 0.21494262181540308\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 64 Loss: 0.0021393976287448872\n",
      "[-0.15242523610904507, -0.2286378541635676] -0.07621261805452254\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 65 Loss: 0.000269313251932132\n",
      "[0.19633990388027042, 0.19633990388027042] 0.19633990388027042\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 65 Loss: 0.0017842064713169084\n",
      "[-0.13920709348127908, -0.20881064022191861] -0.06960354674063954\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 66 Loss: 0.0002244910973150795\n",
      "[0.1793268580870863, 0.1793268580870863] 0.1793268580870863\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 66 Loss: 0.0014877120751710755\n",
      "[-0.12712283535116273, -0.19068425302674408] -0.06356141767558136\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 67 Loss: 0.00018710224772806319\n",
      "[0.16377099573075057, 0.16377099573075057] 0.16377099573075057\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 67 Loss: 0.0012402795417660536\n",
      "[-0.11607724325381549, -0.17411586488072323] -0.058038621626907747\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 68 Loss: 0.0001559202801635698\n",
      "[0.14955032071448993, 0.14955032071448993] 0.14955032071448993\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 68 Loss: 0.0010338401497314443\n",
      "[-0.10598274494301471, -0.15897411741452205] -0.052991372471507356\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 69 Loss: 0.0001299195904853728\n",
      "[0.13655258281847418, 0.13655258281847418] 0.13655258281847418\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 69 Loss: 0.0008616403120146653\n",
      "[-0.09675888141139553, -0.1451383221170933] -0.04837944070569777\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 70 Loss: 0.00010824292941694226\n",
      "[0.12467458277954518, 0.12467458277954518] 0.12467458277954518\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 70 Loss: 0.0007180301064685559\n",
      "[-0.08833175340049597, -0.13249763010074395] -0.044165876700247984\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 71 Loss: 9.01739843216755e-05\n",
      "[0.11382150133739058, 0.11382150133739058] 0.11382150133739058\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 71 Loss: 0.0005982849883274028\n",
      "[-0.08063354539435172, -0.12095031809152758] -0.04031677269717586\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 72 Loss: 7.51144578331341e-05\n",
      "[0.10390627804425391, 0.10390627804425391] 0.10390627804425391\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 72 Loss: 0.0004984559222052067\n",
      "[-0.07360204033232555, -0.11040306049848833] -0.036801020166162775\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 73 Loss: 6.256474876731222e-05\n",
      "[0.09484901730658003, 0.09484901730658003] 0.09484901730658003\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 73 Loss: 0.0004152432942220819\n",
      "[-0.06718020014265931, -0.10077030021398897] -0.033590100071329655\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 74 Loss: 5.210781943873919e-05\n",
      "[0.08657644069259102, 0.08657644069259102] 0.08657644069259102\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 74 Loss: 0.0003458911081703707\n",
      "[-0.06131574430468524, -0.09197361645702787] -0.03065787215234262\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 75 Loss: 4.33956219148139e-05\n",
      "[0.0790213672312926, 0.0790213672312926] 0.0790213672312926\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 75 Loss: 0.00028809812545164785\n",
      "[-0.05596078408288581, -0.08394117612432872] -0.027980392041442904\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 76 Loss: 3.6137773462538364e-05\n",
      "[0.07212223571270665, 0.07212223571270665] 0.07212223571270665\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 76 Loss: 0.0002399434086699499\n",
      "[-0.051071459356683786, -0.07660718903502568] -0.025535729678341893\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 77 Loss: 3.0092042242654987e-05\n",
      "[0.06582265415180075, 0.06582265415180075] 0.06582265415180075\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 77 Loss: 0.0001998238801694212\n",
      "[-0.046607622347774005, -0.06991143352166101] -0.023303811173887003\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 78 Loss: 2.505641553225312e-05\n",
      "[0.06007098670201113, 0.06007098670201113] 0.06007098670201113\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 78 Loss: 0.00016640206528869582\n",
      "[-0.04253252662147159, -0.06379878993220739] -0.021266263310735795\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 79 Loss: 2.0862444130558185e-05\n",
      "[0.054819965967479775, 0.054819965967479775] 0.054819965967479775\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 79 Loss: 0.00013856232166837713\n",
      "[-0.03881255545109046, -0.05821883317663569] -0.01940627772554523\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 80 Loss: 1.7369697222881967e-05\n",
      "[0.050026338246079445, 0.050026338246079445] 0.050026338246079445\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 80 Loss: 0.00011537424477859541\n",
      "[-0.03541695685860119, -0.053125435287901784] -0.017708478429300595\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 81 Loss: 1.4461114333599104e-05\n",
      "[0.045650531916951766, 0.045650531916951766] 0.045650531916951766\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 81 Loss: 9.606204732572892e-05\n",
      "[-0.032317611587785525, -0.04847641738167829] -0.016158805793892762\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 82 Loss: 1.2039134723920043e-05\n",
      "[0.041656354470841234, 0.041656354470841234] 0.041656354470841234\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 82 Loss: 7.997898030022716e-05\n",
      "[-0.029488808315698074, -0.04423321247354711] -0.014744404157849037\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 83 Loss: 1.002245588315461e-05\n",
      "[0.03801071024048755, 0.03801071024048755] 0.03801071024048755\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 83 Loss: 6.65859479196359e-05\n",
      "[-0.026907046339464846, -0.04036056950919727] -0.013453523169732423\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 84 Loss: 8.343335288873147e-06\n",
      "[0.03468334282904513, 0.03468334282904513] 0.03468334282904513\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 84 Loss: 5.543365596791362e-05\n",
      "[-0.024550845524072353, -0.03682626828610853] -0.012275422762036177\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 85 Loss: 6.945332442428447e-06\n",
      "[0.03164659579412509, 0.03164659579412509] 0.03164659579412509\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 85 Loss: 4.614769771967209e-05\n",
      "[-0.022400579120285535, -0.0336008686804283] -0.011200289560142768\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 86 Loss: 5.781429936628644e-06\n",
      "[0.02887519448142914, 0.02887519448142914] 0.02887519448142914\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 86 Loss: 3.8416111223073055e-05\n",
      "[-0.020438313535754687, -0.03065747030363203] -0.010219156767877343\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 87 Loss: 4.812462137685901e-06\n",
      "[0.026346043785236275, 0.026346043785236275] 0.026346043785236275\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 87 Loss: 3.197899030550882e-05\n",
      "[-0.018647667106293314, -0.02797150065943997] -0.009323833553146657\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 88 Loss: 4.0058079664109975e-06\n",
      "[0.02403804391912903, 0.02403804391912903] 0.02403804391912903\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 88 Loss: 2.6619821220206367e-05\n",
      "[-0.01701367532475597, -0.025520512987133957] -0.008506837662377986\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 89 Loss: 3.334298305067423e-06\n",
      "[0.021931919964715146, 0.021931919964715146] 0.021931919964715146\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 89 Loss: 2.2158252833764017e-05\n",
      "[-0.015522671831075249, -0.02328400774661287] -0.007761335915537624\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 90 Loss: 2.7753072210276185e-06\n",
      "[0.02001006668829639, 0.02001006668829639] 0.02001006668829639\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 90 Loss: 1.8444070074028785e-05\n",
      "[-0.014162175272940256, -0.021243262909410386] -0.007081087636470128\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 91 Loss: 2.30999276245197e-06\n",
      "[0.018256405197304093, 0.018256405197304093] 0.018256405197304093\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 91 Loss: 1.5352166711732325e-05\n",
      "[-0.012920789220998504, -0.019381183831497756] -0.006460394610499252\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 92 Loss: 1.9226655759649785e-06\n",
      "[0.016656252491436933, 0.016656252491436933] 0.016656252491436933\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 92 Loss: 1.277835618136903e-05\n",
      "[-0.011788107341777434, -0.01768216101266615] -0.005894053670888717\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 93 Loss: 1.6002616605805684e-06\n",
      "[0.01519620113473412, 0.01519620113473412] 0.01519620113473412\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 93 Loss: 1.0635878291033811e-05\n",
      "[-0.010754629369730121, -0.01613194405459518] -0.005377314684865061\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 94 Loss: 1.3319039382887912e-06\n",
      "[0.01386400978469765, 0.01386400978469765] 0.01386400978469765\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 94 Loss: 8.852489041067646e-06\n",
      "[-0.009811681732340757, -0.014717522598511135] -0.004905840866170378\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 95 Loss: 1.108536268394243e-06\n",
      "[0.012648502335567012, 0.012648502335567012] 0.012648502335567012\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 95 Loss: 7.368034588652306e-06\n",
      "[-0.008951347102061788, -0.013427020653092682] -0.004475673551030894\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 96 Loss: 9.226191374407352e-07\n",
      "[0.011539476181053866, 0.011539476181053866] 0.011539476181053866\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 96 Loss: 6.132430616005264e-06\n",
      "[-0.00816639802709661, -0.012249597040644915] -0.004083199013548305\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 97 Loss: 7.678757110316318e-07\n",
      "[0.01052761778248976, 0.01052761778248976] 0.01052761778248976\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 97 Loss: 5.103978275764e-06\n",
      "[-0.007450237937498038, -0.011175356906247057] -0.003725118968749019\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 98 Loss: 6.390806921141959e-07\n",
      "[0.009604425881837568, 0.009604425881837568] 0.009604425881837568\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 98 Loss: 4.247961804018093e-06\n",
      "[-0.0067968456998645, -0.01019526854979675] -0.00339842284993225\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 99 Loss: 5.318841700541617e-07\n",
      "[0.008762140892210772, 0.008762140892210772] 0.008762140892210772\n",
      "[0, 0] 0\n",
      "[0, 0] 0\n",
      "step: 99 Loss: 3.5354799741922763e-06\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        h = hidden.forward(x)\n",
    "        a = [relu(node) for node in h]\n",
    "        y = output.forward(a)[0]\n",
    "        l = (y - ytrue)**2\n",
    "\n",
    "        backward(l)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            print([w.grad for w in neuron.w], neuron.b.grad)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187deef",
   "metadata": {},
   "source": [
    "#### Dead Neurons and Gradient Flow\n",
    "\n",
    "If a hidden neuron has negative pre-activation:\n",
    "\n",
    "ReLU'(z) = 0\n",
    "\n",
    "Then:\n",
    "\n",
    "dL/dw_hidden = 0\n",
    "\n",
    "This demonstrates how nonlinearities gate gradients.\n",
    "\n",
    "Only active neurons update.\n",
    "\n",
    "This is also why initialization matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a821d5",
   "metadata": {},
   "source": [
    "## Key Engineering Insights\n",
    "\n",
    "- Reverse-mode autodiff computes all gradients in one backward pass.\n",
    "- Gradients accumulate via +=, not assignment.\n",
    "- zero_grad is mandatory before backward.\n",
    "- Random initialization breaks symmetry.\n",
    "- ReLU introduces gradient gating.\n",
    "- Depth multiplies derivative terms via chain rule.\n",
    "- Even multi-layer networks can collapse to simpler representations.\n",
    "\n",
    "This project transformed backpropagation\n",
    "from an abstract formula into a concrete computational system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
