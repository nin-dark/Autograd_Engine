{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad576c1",
   "metadata": {},
   "source": [
    "# AutoGrad Engine(Scalar, Reverse-Mode)\n",
    "\n",
    "A minimal reverse-mode automatic differentiation engine built from scratch for scalar values.\n",
    "\n",
    "Core idea:\n",
    "- Forward pass builds a computatuion DAG dynamically\n",
    "- Backward pass applies the chain rule in reverse topological order\n",
    "\n",
    "This is NOT symbolic differentiation.\n",
    "This is NOT numerical differentiation.\n",
    "This is graph-based reverse-mode autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c09a1",
   "metadata": {},
   "source": [
    "## Node initialization\n",
    "\n",
    "Represents a single value in the computation graph.\n",
    "\n",
    "Each Node stores:\n",
    "- Value: forward computed scalar value\n",
    "- grad: accumulated gradient d(output)/d(value)\n",
    "- parents: nodes this value depends on (graph edges)\n",
    "- backward_fn: local backward function (closure)\n",
    "\n",
    "The computation graph is implicit:\n",
    "If each node knows its parents, the graph already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2e1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Structure\n",
    "class Node:\n",
    "    def __init__(self, value, parents=(), backward_fn=None):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        self.parents = parents\n",
    "        self.backward_fn = backward_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3d486",
   "metadata": {},
   "source": [
    "## FORWARD Operations (GRAPH CONSTRUCTION)\n",
    "\n",
    "Each operation:\n",
    "- computes a forward value\n",
    "- creates exactly one new Node\n",
    "- Records dependencies via parents\n",
    "- Attaches local backward logic as a closure\n",
    "\n",
    "Important:\n",
    "- Forward pass ONLY builds the graph\n",
    "- No gradients are computed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3ae69",
   "metadata": {},
   "source": [
    "### add(a, b)\n",
    "\n",
    "Forward:\n",
    "- value = a.value + b.value\n",
    "- parents = (a,b)\n",
    "\n",
    "Backward (local):\n",
    "- d(out)/d(a) = 1\n",
    "- d(out)/d(b) = 1\n",
    "\n",
    "Gradient contribution is accumulated into parent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db0bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for addition\n",
    "def add(a, b):\n",
    "    out = Node(\n",
    "        value = a.value + b.value,\n",
    "        parents = (a,b)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward():\n",
    "        a.grad += out.grad * 1\n",
    "        b.grad += out.grad * 1\n",
    "    out.backward_fn = backward\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ef185",
   "metadata": {},
   "source": [
    "### mul(a, b)\n",
    "\n",
    "Forward:\n",
    "- value = a.value * b.value\n",
    "- parents = (a,b)\n",
    "\n",
    "Backward (local):\n",
    "- d(out)/d(a) = b.value\n",
    "- d(out)/d(b) = a.value\n",
    "\n",
    "Uses closure to capture forward values needed for backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e237f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for multiplication\n",
    "def mul(a, b):\n",
    "    out = Node(\n",
    "        value = a.value * b.value,\n",
    "        parents = (a,b)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward():\n",
    "        a.grad += out.grad * b.value\n",
    "        b.grad += out.grad * a.value\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a4ee5",
   "metadata": {},
   "source": [
    "### Gradient Accumulation\n",
    "\n",
    "Gradients are accumulated (+=), Not overwritten.\n",
    "\n",
    "Calling backward multiple times without resetting gradients will accumulate contributions.\n",
    "\n",
    "This mirrors real frameworks like PyTorch.\n",
    "Resetting gradient is the user's responsibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c9c6a",
   "metadata": {},
   "source": [
    "## post_dfs(node)\n",
    "\n",
    "Post-order Depth-First Search over the computation DAG.\n",
    "\n",
    "Guarantees:\n",
    "- Each node is visited exactly once\n",
    "- Parents are processed before the node\n",
    "- Produces a valid topological ordering\n",
    "\n",
    "Why post-order:\n",
    "Backward pass requires all downstream gradients to be ready before propagating gradients upstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce9a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find node traversal order using Post-DFS\n",
    "def post_dfs(node, order, visited):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    for parent in node.parents:\n",
    "        post_dfs(parent, order, visited)\n",
    "    order.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691717c",
   "metadata": {},
   "source": [
    "## BACKWARD PASS (Core Autograd Engine)\n",
    "\n",
    "Computes gradients for all nodes contributing to the output.\n",
    "\n",
    "Steps:\n",
    "1. Seed output gradient: d(out)/d(out) = 1\n",
    "2. Discover computation graph via post-order DFS\n",
    "3. Build topological order (dependencies first)\n",
    "4. Execute backward functions in reverse order\n",
    "\n",
    "This separates:\n",
    "- Scheduling (engine)\n",
    "- Math (local backward functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655ee5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Pass\n",
    "def backward(out_node):\n",
    "    visited, order = set(), []\n",
    "    post_dfs(out_node, order, visited)\n",
    "    out_node.grad = 1\n",
    "    for node in reversed(order):\n",
    "        if node.backward_fn:\n",
    "            node.backward_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d42eb",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aa4cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0\n",
      "1 20 18\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "y = Node(4)\n",
    "z = add(x,x)\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad) # Outputs are (1 2 0) because x is being used twice so it takes the blame twice.\n",
    "z = mul(add(x,y),add(x,y))\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad) # Outputs are (1 20 18) the answer should be (1 18 18) but due to the gradient not being reset before calling the x node again it added the previous gradient to the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f639686",
   "metadata": {},
   "source": [
    "## Zero Gradient (Gradient Reset)\n",
    "\n",
    "Because gradients accumulate, they must be explicitly reset when starting a new computation.\n",
    "\n",
    "This is required for:\n",
    "- Numerical gradient checking\n",
    "- Training loops\n",
    "- Repeated backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402dc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reset the grads of all nodes to 0\n",
    "def zero_grad(output):\n",
    "    visited, order = set(), []\n",
    "    post_dfs(output, order, visited)\n",
    "    for node in order: # Sets the grad of each node to 0\n",
    "        node.grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec043b2",
   "metadata": {},
   "source": [
    "### Example after the implementation of Zero Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d7da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0\n",
      "1 18 18\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "y = Node(4)\n",
    "z = add(x,x)\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad)\n",
    "zero_grad(z) # Sets the gradients of all the nodes to 0.\n",
    "z = mul(add(x,y),add(x,y))\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad) # Since there's no previous grad stored in the node so it doesn't get accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6bcd1",
   "metadata": {},
   "source": [
    "## Operator Surface (Usability)\n",
    "\n",
    "To allow natural Python syntax (x + y, 3 - x, etc.), operators are overloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db69609",
   "metadata": {},
   "source": [
    "### For add function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf7a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __add__(self, other):\n",
    "    return add(self, other)\n",
    "\n",
    "Node.__add__ = __add__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904d0f3",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596b3513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "y = Node(5)\n",
    "z = x + y\n",
    "print(z.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3036f5",
   "metadata": {},
   "source": [
    "### For mul function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10273f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __mul__(self, other):\n",
    "    return mul(self, other)\n",
    "\n",
    "Node.__mul__ = __mul__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fd671",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a37fd77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = Node(2)\n",
    "y = Node(5)\n",
    "z = x * y\n",
    "print(z.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cf9f1",
   "metadata": {},
   "source": [
    "### Adding more operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a559fb1",
   "metadata": {},
   "source": [
    "#### neg(x) — Unary negation\n",
    "\n",
    "Forward:\n",
    "- value = a.value * -1\n",
    "- parents = (a)\n",
    "\n",
    "Backward:\n",
    "- d(out)/d(a) = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b07edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for negation of a single value\n",
    "def neg(a):\n",
    "    out = Node(\n",
    "        value = a.value * -1,\n",
    "        parents= (a,)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward(): # dy/dx = -1\n",
    "        a.grad += out.grad * -1\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __neg__(self):\n",
    "    return neg(self)\n",
    "\n",
    "Node.__neg__ = __neg__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be2f59",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873678d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20\n",
      "1 -5 -4\n"
     ]
    }
   ],
   "source": [
    "x = Node(4)\n",
    "y = Node(5)\n",
    "z = x * -y # Composite of ops (First runs the __neg__(self) then does the __mul__(self, other) function)\n",
    "print(z.value)\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e06fa",
   "metadata": {},
   "source": [
    "#### sub(self, other) — Composite funtion of add(self, other) and neg(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a673b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __sub__(self, other):\n",
    "    return add(self, -other) # Does the add() but with the negative value of other\n",
    "\n",
    "Node.__sub__ = __sub__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ffcde",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20956177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21\n",
      "1 1 -1\n"
     ]
    }
   ],
   "source": [
    "x = Node(1)\n",
    "y = Node(22)\n",
    "z = x - y\n",
    "print(z.value) # Composite of ops (First runs the __neg__(self) then does the __sub__(self,other) function)\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d101503",
   "metadata": {},
   "source": [
    "#### pow(a, n) — (scalar exponent)\n",
    "\n",
    "Forward:\n",
    "- value = x.value ** n (scalar value)\n",
    "- parents = (x)\n",
    "\n",
    "Backward:\n",
    "- d(out)/d(a) = na^(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d67942d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the power with scalar exponent\n",
    "def pow(a, n):\n",
    "    out = Node(\n",
    "        value= a.value ** n,\n",
    "        parents= (a,)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward(): # Backward: dy/dx = nx^(n-1)\n",
    "        a.grad += out.grad * (n * (a.value**(n-1)))\n",
    "    out.backward_fn= backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf487f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pow__(self, other):\n",
    "    return pow(self, other)\n",
    "\n",
    "Node.__pow__ = __pow__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d11120",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "404dda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "z = x ** 2\n",
    "print(z.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c2f1a",
   "metadata": {},
   "source": [
    "#### exp(a)\n",
    "\n",
    "Forward:\n",
    "- value = exp(a.value)\n",
    "- parents = (a)\n",
    "\n",
    "Backward:\n",
    "- d(out)/d(a) = out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "394751bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the exponential\n",
    "def exp(a):\n",
    "    import math\n",
    "    out = Node(\n",
    "        value= math.exp(a.value),\n",
    "        parents= (a,)\n",
    "    )\n",
    "    # Gradient accumulation\n",
    "    def backward(): # Backward: dy/dx = exp(x) = out.value\n",
    "        a.grad += out.grad * out.value\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed648c4",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c44f213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045 2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "x = Node(1)\n",
    "z = exp(x)\n",
    "backward(z)\n",
    "print(z.value, x.grad)  # ≈ 2.71828, 2.71828"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefc4e7",
   "metadata": {},
   "source": [
    "#### log(a)\n",
    "\n",
    "Forward:\n",
    "- value = log(a.value)\n",
    "- parents = (a)\n",
    "\n",
    "Backward:\n",
    "- d(out)/d(a) = 1/(a.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ae5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the logarithmic value\n",
    "def log(a):\n",
    "    import math\n",
    "    out = Node(\n",
    "        value= math.log(a.value),\n",
    "        parents= (a,)\n",
    "    )\n",
    "    # Gradient accumulation\n",
    "    def backward(): # backward: dy/dx = 1/x\n",
    "        a.grad += out.grad * (1/a.value)\n",
    "    out.backward_fn= backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eaf59c",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cff53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302585092994046 0.1\n"
     ]
    }
   ],
   "source": [
    "x = Node(10)\n",
    "z = log(x)\n",
    "backward(z)\n",
    "print(z.value, x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ea6a2",
   "metadata": {},
   "source": [
    "#### sin(a)\n",
    "\n",
    "Forward:\n",
    "- value = sin(a.value)\n",
    "- parents = (a)\n",
    "\n",
    "Backward:\n",
    "- d(out)/d(a) = cos(a.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b78901a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find sin\n",
    "def sin(a):\n",
    "    import math\n",
    "    out = Node(\n",
    "        value= math.sin(a.value),\n",
    "        parents= (a,)\n",
    "    )\n",
    "    # Gradient accumulation\n",
    "    def backward(): # backward: dy/dx = cos(x)\n",
    "        a.grad += out.grad * math.cos(a.value)\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19b746",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b15087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9589242746631385 0.28366218546322625\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "z = sin(x)\n",
    "backward(z)\n",
    "print(z.value, x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cac14",
   "metadata": {},
   "source": [
    "### Scalar lifting\n",
    "\n",
    "Raw scalars are automatically converted into Node's at the operator boundary.\n",
    "\n",
    "This ensures:\n",
    "- Engine ops only ever receive Node objects\n",
    "- No raw scalars leak into the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "191db3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the scalar value to a Node object\n",
    "def lifter(value):\n",
    "    if isinstance(value, Node): # checks if the value is a node object or not.\n",
    "        return value\n",
    "    else:\n",
    "        return Node(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bc235",
   "metadata": {},
   "source": [
    "#### Updated ops with lifter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec3cc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __add__(self, other):\n",
    "    other = lifter(other)\n",
    "    return add(self, other)\n",
    "\n",
    "Node.__add__ = __add__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2897d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __mul__(self, other):\n",
    "    other = lifter(other)\n",
    "    return mul(self, other)\n",
    "\n",
    "Node.__mul__ = __mul__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "526f5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __sub__(self,other):\n",
    "    other = lifter(other)\n",
    "    return add(self, -other)\n",
    "\n",
    "Node.__sub__ = __sub__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e68048",
   "metadata": {},
   "source": [
    "#### Right hand versions (add, mul, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7fd8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __radd__(self, other):\n",
    "    other = lifter(other)\n",
    "    return add(other, self)\n",
    "\n",
    "Node.__radd__ = __radd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aa029bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __rmul__(self, other):\n",
    "    other = lifter(other)\n",
    "    return mul(other,self)\n",
    "\n",
    "Node.__rmul__ = __rmul__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e9b42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __rsub__(self,other):\n",
    "    other = lifter(other)\n",
    "    return add(other, -self)\n",
    "\n",
    "Node.__rsub__ = __rsub__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b1f1a",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "To prove correctness, analytical gradients are compared against numerical gradients using finite differences.\n",
    "\n",
    "- d(f)/d(x) ​≈ (f(x+ϵ)−f(x−ϵ)​)/2ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d64c71",
   "metadata": {},
   "source": [
    "### Defining f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "738077ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x + 1) * (x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4256a01",
   "metadata": {},
   "source": [
    "### AutoGrad Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5454723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ag_g(f, x0):\n",
    "    x = Node(x0)\n",
    "    z = f(x)\n",
    "    zero_grad(z)\n",
    "    backward(z)\n",
    "    return x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934e4f2",
   "metadata": {},
   "source": [
    "### Numerical Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e290792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_g(f, x0, eps = 1e-4):\n",
    "    x_pos = Node(x0 + eps)\n",
    "    x_neg = Node(x0 - eps)\n",
    "\n",
    "    z_pos = f(x_pos)\n",
    "    z_neg = f(x_neg)\n",
    "\n",
    "    return (z_pos.value - z_neg.value) / (2 * eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a891fb",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "- f = Function of the equation\n",
    "- x0 = Absolute value of x\n",
    "- eps = epsilon\n",
    "- tol = tolerance\n",
    "\n",
    "If the difference of (g_auto - g_num) is less than tolerance then there's no bugs, if not then bugs are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63800a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcheck(f, x0, eps=1e-4, tol=1e-4):\n",
    "    g_auto = ag_g(f, x0)\n",
    "    g_num = num_g(f, x0, eps)\n",
    "\n",
    "    diff = abs(g_auto - g_num)\n",
    "    print(f\"x = {x0}\")\n",
    "    print(f\"autograd grad = {g_auto}\")\n",
    "    print(f\"numerical grad = {g_num}\")\n",
    "    print(f\"diff = {diff}\")\n",
    "\n",
    "    if diff < tol:\n",
    "        print(\"grad check pass\")\n",
    "    else:\n",
    "        print(\"grad check fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897efd6",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcdb212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = [\n",
    "    # addition\n",
    "    (\"x + 3\",        lambda x: x + 3,        2.0),\n",
    "    (\"3 + x\",        lambda x: 3 + x,        2.0),\n",
    "\n",
    "    # subtraction\n",
    "    (\"x - 3\",        lambda x: x - 3,        5.0),\n",
    "    (\"3 - x\",        lambda x: 3 - x,        5.0),\n",
    "\n",
    "    # multiplication\n",
    "    (\"x * 4\",        lambda x: x * 4,        3.0),\n",
    "    (\"4 * x\",        lambda x: 4 * x,        3.0),\n",
    "\n",
    "    # power\n",
    "    (\"x ** 2\",       lambda x: x ** 2,       2.0),\n",
    "\n",
    "    # unary\n",
    "    (\"-x\",           lambda x: -x,           3.0),\n",
    "\n",
    "    # nonlinear\n",
    "    (\"exp(x)\",       lambda x: exp(x),       1.0),\n",
    "    (\"log(x)\",       lambda x: log(x),       2.0),\n",
    "    (\"sin(x)\",       lambda x: sin(x),       1.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0836faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "composites = [\n",
    "    (\"(x + 1)(x + 1)\",   lambda x: (x + 1) * (x + 1),   2.0),\n",
    "    (\"(x - 2)(x + 2)\",   lambda x: (x - 2) * (x + 2),   3.0),\n",
    "    (\"(x + 1)(1 + x)\",   lambda x: (x + 1) * (1 + x),   2.0),\n",
    "\n",
    "    (\"exp(x) * x\",       lambda x: exp(x) * x,          1.0),\n",
    "    (\"log(x) * x\",       lambda x: log(x) * x,          2.0),\n",
    "\n",
    "    (\"-(x + 3)\",         lambda x: -(x + 3),            4.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3810db9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SINGLE OP TESTS =====\n",
      "\n",
      "Testing: x + 3\n",
      "x = 2.0\n",
      "autograd grad = 1\n",
      "numerical grad = 0.9999999999976694\n",
      "diff = 2.3305801732931286e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: 3 + x\n",
      "x = 2.0\n",
      "autograd grad = 1\n",
      "numerical grad = 0.9999999999976694\n",
      "diff = 2.3305801732931286e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: x - 3\n",
      "x = 5.0\n",
      "autograd grad = 1\n",
      "numerical grad = 0.9999999999976694\n",
      "diff = 2.3305801732931286e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: 3 - x\n",
      "x = 5.0\n",
      "autograd grad = -1\n",
      "numerical grad = -0.9999999999976694\n",
      "diff = 2.3305801732931286e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: x * 4\n",
      "x = 3.0\n",
      "autograd grad = 4\n",
      "numerical grad = 4.000000000008441\n",
      "diff = 8.44124770082999e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: 4 * x\n",
      "x = 3.0\n",
      "autograd grad = 4\n",
      "numerical grad = 4.000000000008441\n",
      "diff = 8.44124770082999e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: x ** 2\n",
      "x = 2.0\n",
      "autograd grad = 4.0\n",
      "numerical grad = 4.000000000004\n",
      "diff = 4.000355602329364e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: -x\n",
      "x = 3.0\n",
      "autograd grad = -1\n",
      "numerical grad = -1.0000000000021103\n",
      "diff = 2.1103119252074976e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: exp(x)\n",
      "x = 1.0\n",
      "autograd grad = 2.718281828459045\n",
      "numerical grad = 2.718281832989611\n",
      "diff = 4.53056570037802e-09\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: log(x)\n",
      "x = 2.0\n",
      "autograd grad = 0.5\n",
      "numerical grad = 0.5000000004168337\n",
      "diff = 4.1683367868472487e-10\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: sin(x)\n",
      "x = 1.5\n",
      "autograd grad = 0.0707372016677029\n",
      "numerical grad = 0.07073720155015284\n",
      "diff = 1.1755006690261638e-10\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "\n",
      "===== COMPOSITE TESTS =====\n",
      "\n",
      "Testing: (x + 1)(x + 1)\n",
      "x = 2.0\n",
      "autograd grad = 6.0\n",
      "numerical grad = 5.999999999994898\n",
      "diff = 5.101696842757519e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: (x - 2)(x + 2)\n",
      "x = 3.0\n",
      "autograd grad = 6.0\n",
      "numerical grad = 6.000000000008221\n",
      "diff = 8.220979452744359e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: (x + 1)(1 + x)\n",
      "x = 2.0\n",
      "autograd grad = 6.0\n",
      "numerical grad = 5.999999999994898\n",
      "diff = 5.101696842757519e-12\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: exp(x) * x\n",
      "x = 1.0\n",
      "autograd grad = 5.43656365691809\n",
      "numerical grad = 5.436563675040862\n",
      "diff = 1.8122771727746567e-08\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: log(x) * x\n",
      "x = 2.0\n",
      "autograd grad = 1.6931471805599454\n",
      "numerical grad = 1.6931471801440168\n",
      "diff = 4.1592862487505045e-10\n",
      "grad check pass\n",
      "----------------------------------------\n",
      "Testing: -(x + 3)\n",
      "x = 4.0\n",
      "autograd grad = -1\n",
      "numerical grad = -0.9999999999976694\n",
      "diff = 2.3305801732931286e-12\n",
      "grad check pass\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== SINGLE OP TESTS =====\\n\")\n",
    "for name, fn, x0 in equations:\n",
    "    print(f\"Testing: {name}\")\n",
    "    gradcheck(fn, x0)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n===== COMPOSITE TESTS =====\\n\")\n",
    "for name, fn, x0 in composites:\n",
    "    print(f\"Testing: {name}\")\n",
    "    gradcheck(fn, x0)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991565ff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project demonstrates the core mechanics of reverse-mode automatic differentiation:\n",
    "- Dynamic graph construction\n",
    "- Implicit DAG representation\n",
    "- Reverse topological traversal\n",
    "- Gradient accumulation\n",
    "- Explicit gradient reset\n",
    "- Numerical verification\n",
    "\n",
    "This is the foundational mechanism behind modern deep learning frameworks such as PyTorch, JAX, and TensorFlow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
