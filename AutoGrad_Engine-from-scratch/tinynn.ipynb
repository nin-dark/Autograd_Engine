{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135423c2",
   "metadata": {},
   "source": [
    "# Tiny Neural Network using The Autograd Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfab81",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, I build a tiny neural network entirely on top of my own reverse-mode autodiff engine.\n",
    "\n",
    "There is:\n",
    "- No NumPy\n",
    "- No PyTorch\n",
    "- No vectorization\n",
    "- Only scalar Nodes and a computational graph\n",
    "\n",
    "The goal is not performance — it is understanding gradient flow deeply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6146a85",
   "metadata": {},
   "source": [
    "Importing our autograd engine as a library to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3561fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de77e43",
   "metadata": {},
   "source": [
    "## Phase 1 — Single Input, Single Neuron\n",
    "\n",
    "We start with the simplest possible model:\n",
    "\n",
    "y = w·x + b\n",
    "\n",
    "Loss:\n",
    "L = (y − y_true)²\n",
    "\n",
    "This phase validates:\n",
    "\n",
    "- Parameter nodes store gradients correctly\n",
    "- Multiplication and addition propagate gradients\n",
    "- reverse-mode traversal works\n",
    "- zero_grad lifecycle is correct\n",
    "- manual derivative matches engine output\n",
    "\n",
    "If this fails, the engine is broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50610c",
   "metadata": {},
   "source": [
    "### Single-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e648c",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e394a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba937",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5697f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "ytrue = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75b56e",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f1adb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -40 -20\n",
      "manual: -40 -20\n",
      "0 100\n",
      "engine: -36.0 -18.0\n",
      "manual: -36.0 -18.0\n",
      "1 81.0\n",
      "engine: -32.4 -16.2\n",
      "manual: -32.4 -16.2\n",
      "2 65.61\n",
      "engine: -29.16 -14.58\n",
      "manual: -29.16 -14.58\n",
      "3 53.1441\n",
      "engine: -26.244 -13.122\n",
      "manual: -26.244 -13.122\n",
      "4 43.046721\n",
      "engine: -23.6196 -11.8098\n",
      "manual: -23.6196 -11.8098\n",
      "5 34.86784400999999\n",
      "engine: -21.25764 -10.62882\n",
      "manual: -21.25764 -10.62882\n",
      "6 28.242953648099995\n",
      "engine: -19.131876 -9.565938\n",
      "manual: -19.131876 -9.565938\n",
      "7 22.876792454960995\n",
      "engine: -17.218688399999994 -8.609344199999997\n",
      "manual: -17.218688399999994 -8.609344199999997\n",
      "8 18.5302018885184\n",
      "engine: -15.496819559999999 -7.748409779999999\n",
      "manual: -15.496819559999999 -7.748409779999999\n",
      "9 15.009463529699909\n",
      "engine: -13.947137603999998 -6.973568801999999\n",
      "manual: -13.947137603999998 -6.973568801999999\n",
      "10 12.157665459056926\n",
      "engine: -12.552423843599996 -6.276211921799998\n",
      "manual: -12.552423843599996 -6.276211921799998\n",
      "11 9.847709021836106\n",
      "engine: -11.29718145924 -5.64859072962\n",
      "manual: -11.29718145924 -5.64859072962\n",
      "12 7.976644307687252\n",
      "engine: -10.167463313315999 -5.083731656657999\n",
      "manual: -10.167463313315999 -5.083731656657999\n",
      "13 6.461081889226672\n",
      "engine: -9.150716981984395 -4.575358490992198\n",
      "manual: -9.150716981984395 -4.575358490992198\n",
      "14 5.2334763302736\n",
      "engine: -8.235645283785956 -4.117822641892978\n",
      "manual: -8.235645283785956 -4.117822641892978\n",
      "15 4.2391158275216165\n",
      "engine: -7.412080755407359 -3.7060403777036797\n",
      "manual: -7.412080755407359 -3.7060403777036797\n",
      "16 3.433683820292508\n",
      "engine: -6.670872679866626 -3.335436339933313\n",
      "manual: -6.670872679866626 -3.335436339933313\n",
      "17 2.781283894436934\n",
      "engine: -6.00378541187996 -3.00189270593998\n",
      "manual: -6.00378541187996 -3.00189270593998\n",
      "18 2.252839954493914\n",
      "engine: -5.403406870691967 -2.7017034353459835\n",
      "manual: -5.403406870691967 -2.7017034353459835\n",
      "19 1.8248003631400722\n",
      "engine: -4.86306618362277 -2.431533091811385\n",
      "manual: -4.86306618362277 -2.431533091811385\n",
      "20 1.4780882941434585\n",
      "engine: -4.3767595652604925 -2.1883797826302462\n",
      "manual: -4.3767595652604925 -2.1883797826302462\n",
      "21 1.197251518256201\n",
      "engine: -3.9390836087344425 -1.9695418043672213\n",
      "manual: -3.9390836087344425 -1.9695418043672213\n",
      "22 0.9697737297875224\n",
      "engine: -3.545175247860996 -1.772587623930498\n",
      "manual: -3.545175247860996 -1.772587623930498\n",
      "23 0.7855167211278922\n",
      "engine: -3.1906577230748994 -1.5953288615374497\n",
      "manual: -3.1906577230748994 -1.5953288615374497\n",
      "24 0.6362685441135938\n",
      "engine: -2.87159195076741 -1.435795975383705\n",
      "manual: -2.87159195076741 -1.435795975383705\n",
      "25 0.5153775207320113\n",
      "engine: -2.5844327556906705 -1.2922163778453353\n",
      "manual: -2.5844327556906705 -1.2922163778453353\n",
      "26 0.4174557917929296\n",
      "engine: -2.3259894801215992 -1.1629947400607996\n",
      "manual: -2.3259894801215992 -1.1629947400607996\n",
      "27 0.3381391913522717\n",
      "engine: -2.0933905321094386 -1.0466952660547193\n",
      "manual: -2.0933905321094386 -1.0466952660547193\n",
      "28 0.2738927449953399\n",
      "engine: -1.884051478898499 -0.9420257394492495\n",
      "manual: -1.884051478898499 -0.9420257394492495\n",
      "29 0.22185312344622632\n",
      "engine: -1.6956463310086463 -0.8478231655043231\n",
      "manual: -1.6956463310086463 -0.8478231655043231\n",
      "30 0.17970102999144272\n",
      "engine: -1.5260816979077845 -0.7630408489538922\n",
      "manual: -1.5260816979077845 -0.7630408489538922\n",
      "31 0.14555783429306915\n",
      "engine: -1.3734735281170032 -0.6867367640585016\n",
      "manual: -1.3734735281170032 -0.6867367640585016\n",
      "32 0.11790184577738552\n",
      "engine: -1.2361261753053014 -0.6180630876526507\n",
      "manual: -1.2361261753053014 -0.6180630876526507\n",
      "33 0.09550049507968206\n",
      "engine: -1.1125135577747756 -0.5562567788873878\n",
      "manual: -1.1125135577747756 -0.5562567788873878\n",
      "34 0.07735540101454305\n",
      "engine: -1.0012622019972923 -0.5006311009986462\n",
      "manual: -1.0012622019972923 -0.5006311009986462\n",
      "35 0.06265787482177916\n",
      "engine: -0.9011359817975659 -0.45056799089878297\n",
      "manual: -0.9011359817975659 -0.45056799089878297\n",
      "36 0.05075287860564144\n",
      "engine: -0.8110223836178108 -0.4055111918089054\n",
      "manual: -0.8110223836178108 -0.4055111918089054\n",
      "37 0.04110983167056971\n",
      "engine: -0.729920145256024 -0.364960072628012\n",
      "manual: -0.729920145256024 -0.364960072628012\n",
      "38 0.03329896365316095\n",
      "engine: -0.656928130730428 -0.328464065365214\n",
      "manual: -0.656928130730428 -0.328464065365214\n",
      "39 0.026972160559060893\n",
      "engine: -0.5912353176573788 -0.2956176588286894\n",
      "manual: -0.5912353176573788 -0.2956176588286894\n",
      "40 0.021847450052838852\n",
      "engine: -0.5321117858916438 -0.2660558929458219\n",
      "manual: -0.5321117858916438 -0.2660558929458219\n",
      "41 0.01769643454279966\n",
      "engine: -0.47890060730247797 -0.23945030365123898\n",
      "manual: -0.47890060730247797 -0.23945030365123898\n",
      "42 0.014334111979667639\n",
      "engine: -0.4310105465722316 -0.2155052732861158\n",
      "manual: -0.4310105465722316 -0.2155052732861158\n",
      "43 0.011610630703530864\n",
      "engine: -0.38790949191501056 -0.19395474595750528\n",
      "manual: -0.38790949191501056 -0.19395474595750528\n",
      "44 0.009404610869860103\n",
      "engine: -0.3491185427235095 -0.17455927136175475\n",
      "manual: -0.3491185427235095 -0.17455927136175475\n",
      "45 0.007617734804586683\n",
      "engine: -0.31420668845115785 -0.15710334422557892\n",
      "manual: -0.31420668845115785 -0.15710334422557892\n",
      "46 0.006170365191715186\n",
      "engine: -0.28278601960603567 -0.14139300980301783\n",
      "manual: -0.28278601960603567 -0.14139300980301783\n",
      "47 0.004997995805289074\n",
      "engine: -0.25450741764543494 -0.12725370882271747\n",
      "manual: -0.25450741764543494 -0.12725370882271747\n",
      "48 0.004048376602284241\n",
      "engine: -0.22905667588089074 -0.11452833794044537\n",
      "manual: -0.22905667588089074 -0.11452833794044537\n",
      "49 0.0032791850478502147\n",
      "engine: -0.20615100829279953 -0.10307550414639977\n",
      "manual: -0.20615100829279953 -0.10307550414639977\n",
      "50 0.002656139888758619\n",
      "engine: -0.18553590746351745 -0.09276795373175872\n",
      "manual: -0.18553590746351745 -0.09276795373175872\n",
      "51 0.002151473309894432\n",
      "engine: -0.16698231671716712 -0.08349115835858356\n",
      "manual: -0.16698231671716712 -0.08349115835858356\n",
      "52 0.0017426933810145194\n",
      "engine: -0.150284085045449 -0.0751420425227245\n",
      "manual: -0.150284085045449 -0.0751420425227245\n",
      "53 0.0014115816386217341\n",
      "engine: -0.13525567654090764 -0.06762783827045382\n",
      "manual: -0.13525567654090764 -0.06762783827045382\n",
      "54 0.0011433811272836647\n",
      "engine: -0.12173010888681546 -0.06086505444340773\n",
      "manual: -0.12173010888681546 -0.06086505444340773\n",
      "55 0.0009261387130997467\n",
      "engine: -0.10955709799813462 -0.05477854899906731\n",
      "manual: -0.10955709799813462 -0.05477854899906731\n",
      "56 0.0007501723576108046\n",
      "engine: -0.09860138819831832 -0.04930069409915916\n",
      "manual: -0.09860138819831832 -0.04930069409915916\n",
      "57 0.0006076396096647167\n",
      "engine: -0.08874124937848649 -0.04437062468924324\n",
      "manual: -0.08874124937848649 -0.04437062468924324\n",
      "58 0.0004921880838284205\n",
      "engine: -0.07986712444063926 -0.03993356222031963\n",
      "manual: -0.07986712444063926 -0.03993356222031963\n",
      "59 0.0003986723479010348\n",
      "engine: -0.07188041199657391 -0.035940205998286956\n",
      "manual: -0.07188041199657391 -0.035940205998286956\n",
      "60 0.0003229246017998254\n",
      "engine: -0.06469237079691226 -0.03234618539845613\n",
      "manual: -0.06469237079691226 -0.03234618539845613\n",
      "61 0.00026156892745782414\n",
      "engine: -0.05822313371722743 -0.029111566858613713\n",
      "manual: -0.05822313371722743 -0.029111566858613713\n",
      "62 0.00021187083124088408\n",
      "engine: -0.05240082034550397 -0.026200410172751987\n",
      "manual: -0.05240082034550397 -0.026200410172751987\n",
      "63 0.00017161537330511145\n",
      "engine: -0.04716073831095002 -0.02358036915547501\n",
      "manual: -0.04716073831095002 -0.02358036915547501\n",
      "64 0.00013900845237711933\n",
      "engine: -0.04244466447985218 -0.02122233223992609\n",
      "manual: -0.04244466447985218 -0.02122233223992609\n",
      "65 0.00011259684642545158\n",
      "engine: -0.03820019803186625 -0.019100099015933125\n",
      "manual: -0.03820019803186625 -0.019100099015933125\n",
      "66 9.120344560461239e-05\n",
      "engine: -0.034380178228680336 -0.017190089114340168\n",
      "manual: -0.034380178228680336 -0.017190089114340168\n",
      "67 7.387479093973908e-05\n",
      "engine: -0.030942160405814434 -0.015471080202907217\n",
      "manual: -0.030942160405814434 -0.015471080202907217\n",
      "68 5.9838580661196906e-05\n",
      "engine: -0.02784794436523441 -0.013923972182617206\n",
      "manual: -0.02784794436523441 -0.013923972182617206\n",
      "69 4.846925033557444e-05\n",
      "engine: -0.025063149928705286 -0.012531574964352643\n",
      "manual: -0.025063149928705286 -0.012531574964352643\n",
      "70 3.926009277179749e-05\n",
      "engine: -0.022556834935841152 -0.011278417467920576\n",
      "manual: -0.022556834935841152 -0.011278417467920576\n",
      "71 3.1800675145173994e-05\n",
      "engine: -0.020301151442254195 -0.010150575721127097\n",
      "manual: -0.020301151442254195 -0.010150575721127097\n",
      "72 2.5758546867583725e-05\n",
      "engine: -0.018271036298024512 -0.009135518149012256\n",
      "manual: -0.018271036298024512 -0.009135518149012256\n",
      "73 2.086442296273308e-05\n",
      "engine: -0.016443932668224193 -0.008221966334112096\n",
      "manual: -0.016443932668224193 -0.008221966334112096\n",
      "74 1.6900182599818178e-05\n",
      "engine: -0.014799539401401773 -0.007399769700700887\n",
      "manual: -0.014799539401401773 -0.007399769700700887\n",
      "75 1.3689147905852722e-05\n",
      "engine: -0.013319585461260885 -0.006659792730630443\n",
      "manual: -0.013319585461260885 -0.006659792730630443\n",
      "76 1.1088209803739523e-05\n",
      "engine: -0.011987626915136218 -0.005993813457568109\n",
      "manual: -0.011987626915136218 -0.005993813457568109\n",
      "77 8.981449941031142e-06\n",
      "engine: -0.010788864223620465 -0.005394432111810232\n",
      "manual: -0.010788864223620465 -0.005394432111810232\n",
      "78 7.27497445223235e-06\n",
      "engine: -0.009709977801264813 -0.0048549889006324065\n",
      "manual: -0.009709977801264813 -0.0048549889006324065\n",
      "79 5.892729306315966e-06\n",
      "engine: -0.00873898002113549 -0.004369490010567745\n",
      "manual: -0.00873898002113549 -0.004369490010567745\n",
      "80 4.773110738112827e-06\n",
      "engine: -0.007865082019023362 -0.003932541009511681\n",
      "manual: -0.007865082019023362 -0.003932541009511681\n",
      "81 3.866219697872787e-06\n",
      "engine: -0.007078573817118183 -0.0035392869085590917\n",
      "manual: -0.007078573817118183 -0.0035392869085590917\n",
      "82 3.131637955274443e-06\n",
      "engine: -0.0063707164354056545 -0.0031853582177028272\n",
      "manual: -0.0063707164354056545 -0.0031853582177028272\n",
      "83 2.536626743771733e-06\n",
      "engine: -0.005733644791867221 -0.0028668223959336103\n",
      "manual: -0.005733644791867221 -0.0028668223959336103\n",
      "84 2.0546676624566314e-06\n",
      "engine: -0.005160280312679788 -0.002580140156339894\n",
      "manual: -0.005160280312679788 -0.002580140156339894\n",
      "85 1.664280806589413e-06\n",
      "engine: -0.004644252281408967 -0.0023221261407044835\n",
      "manual: -0.004644252281408967 -0.0023221261407044835\n",
      "86 1.3480674533357747e-06\n",
      "engine: -0.004179827053263807 -0.0020899135266319036\n",
      "manual: -0.004179827053263807 -0.0020899135266319036\n",
      "87 1.0919346371997501e-06\n",
      "engine: -0.0037618443479345842 -0.0018809221739672921\n",
      "manual: -0.0037618443479345842 -0.0018809221739672921\n",
      "88 8.844670561304611e-07\n",
      "engine: -0.003385659913142547 -0.0016928299565712734\n",
      "manual: -0.003385659913142547 -0.0016928299565712734\n",
      "89 7.164183154662749e-07\n",
      "engine: -0.0030470939218290027 -0.0015235469609145014\n",
      "manual: -0.0030470939218290027 -0.0015235469609145014\n",
      "90 5.802988355279533e-07\n",
      "engine: -0.002742384529646813 -0.0013711922648234065\n",
      "manual: -0.002742384529646813 -0.0013711922648234065\n",
      "91 4.700420567778857e-07\n",
      "engine: -0.00246814607668 -0.00123407303834\n",
      "manual: -0.00246814607668 -0.00123407303834\n",
      "92 3.807340659894298e-07\n",
      "engine: -0.0022213314690162633 -0.0011106657345081317\n",
      "manual: -0.0022213314690162633 -0.0011106657345081317\n",
      "93 3.083945934526219e-07\n",
      "engine: -0.0019991983221103737 -0.0009995991610551869\n",
      "manual: -0.0019991983221103737 -0.0009995991610551869\n",
      "94 2.4979962069555833e-07\n",
      "engine: -0.0017992784899050207 -0.0008996392449525104\n",
      "manual: -0.0017992784899050207 -0.0008996392449525104\n",
      "95 2.0233769276468074e-07\n",
      "engine: -0.0016193506409152292 -0.0008096753204576146\n",
      "manual: -0.0016193506409152292 -0.0008096753204576146\n",
      "96 1.6389353113953522e-07\n",
      "engine: -0.0014574155768229957 -0.0007287077884114979\n",
      "manual: -0.0014574155768229957 -0.0007287077884114979\n",
      "97 1.3275376022289407e-07\n",
      "engine: -0.0013116740191350118 -0.0006558370095675059\n",
      "manual: -0.0013116740191350118 -0.0006558370095675059\n",
      "98 1.0753054577961221e-07\n",
      "engine: -0.0011805066172243528 -0.0005902533086121764\n",
      "manual: -0.0011805066172243528 -0.0005902533086121764\n",
      "99 8.709974208190529e-08\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w)\n",
    "    zero_grad(b)\n",
    "\n",
    "    y = w*x + b\n",
    "    l = (y-ytrue)**2\n",
    "    \n",
    "    backward(l)\n",
    "    \n",
    "    manual_dw = 2*(y.value - ytrue)*x\n",
    "    manual_db = 2*(y.value - ytrue)\n",
    "\n",
    "    print(\"engine:\", w.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw, manual_db)\n",
    "    \n",
    "    w.value -= n*w.grad\n",
    "    b.value -= n*b.grad\n",
    "    \n",
    "    print(step, l.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fbeae",
   "metadata": {},
   "source": [
    "### What This Confirms\n",
    "\n",
    "The gradients printed by the engine match the manually derived gradients:\n",
    "\n",
    "dL/dw = 2(y − y_true)x  \n",
    "dL/db = 2(y − y_true)\n",
    "\n",
    "This confirms:\n",
    "\n",
    "- Chain rule is implemented correctly\n",
    "- Gradients accumulate via +=\n",
    "- Backward traversal is correct\n",
    "- Parameter updates modify value, not graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137dcdc",
   "metadata": {},
   "source": [
    "## Phase 2 — Multi-Input Neuron\n",
    "\n",
    "Now we extend the neuron to:\n",
    "\n",
    "y = w1·x1 + w2·x2 + b\n",
    "\n",
    "This introduces multiple parents in the computational graph.\n",
    "\n",
    "Graph structure becomes branched:\n",
    "\n",
    "(w1·x1) + (w2·x2) + b\n",
    "\n",
    "Reverse-mode must:\n",
    "\n",
    "- Propagate gradients through multiple branches\n",
    "- Accumulate gradient contributions correctly\n",
    "- Handle shared nodes safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21203234",
   "metadata": {},
   "source": [
    "### Multi-Input Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d0a5b",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5eab5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47a78",
   "metadata": {},
   "source": [
    "Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 3\n",
    "ytrue = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96085",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "117a12c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -46.08 -69.12 -23.04\n",
      "manual: -46.08 -69.12 -23.04\n",
      "step: 1 Loss: 132.7104\n",
      "engine: -33.1776 -49.7664 -16.5888\n",
      "manual: -33.1776 -49.7664 -16.5888\n",
      "step: 2 Loss: 68.79707135999999\n",
      "engine: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "manual: -23.887871999999994 -35.831807999999995 -11.943935999999997\n",
      "step: 3 Loss: 35.664401793023984\n",
      "engine: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "manual: -17.199267839999997 -25.798901759999996 -8.599633919999999\n",
      "step: 4 Loss: 18.488425889503635\n",
      "engine: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "manual: -12.383472844800004 -18.575209267200005 -6.191736422400002\n",
      "step: 5 Loss: 9.584399981118693\n",
      "engine: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "manual: -8.916100448255996 -13.374150672383994 -4.458050224127998\n",
      "step: 6 Loss: 4.968552950211923\n",
      "engine: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "manual: -6.419592322744322 -9.629388484116483 -3.209796161372161\n",
      "step: 7 Loss: 2.5756978493898646\n",
      "engine: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "manual: -4.622106472375904 -6.933159708563856 -2.311053236187952\n",
      "step: 8 Loss: 1.3352417651237016\n",
      "engine: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "manual: -3.3279166601106525 -4.991874990165979 -1.6639583300553262\n",
      "step: 9 Loss: 0.6921893310401275\n",
      "engine: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "manual: -2.3960999952796698 -3.5941499929195047 -1.1980499976398349\n",
      "step: 10 Loss: 0.35883094921120207\n",
      "engine: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "manual: -1.7251919966013602 -2.5877879949020404 -0.8625959983006801\n",
      "step: 11 Loss: 0.18601796407108673\n",
      "engine: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "manual: -1.242138237552986 -1.8632073563294789 -0.621069118776493\n",
      "step: 12 Loss: 0.09643171257445238\n",
      "engine: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "manual: -0.894339531038149 -1.3415092965572235 -0.4471697655190745\n",
      "step: 13 Loss: 0.049990199798596015\n",
      "engine: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "manual: -0.6439244623474636 -0.9658866935211954 -0.3219622311737318\n",
      "step: 14 Loss: 0.025914919575591878\n",
      "engine: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "manual: -0.46362561289016924 -0.6954384193352539 -0.23181280644508462\n",
      "step: 15 Loss: 0.013434294307986566\n",
      "engine: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "manual: -0.3338104412809244 -0.5007156619213866 -0.1669052206404622\n",
      "step: 16 Loss: 0.006964338169260342\n",
      "engine: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "manual: -0.24034351772226614 -0.3605152765833992 -0.12017175886113307\n",
      "step: 17 Loss: 0.0036103129069445785\n",
      "engine: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "manual: -0.17304733276002793 -0.2595709991400419 -0.08652366638001396\n",
      "step: 18 Loss: 0.0018715862109599897\n",
      "engine: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "manual: -0.12459407958721158 -0.18689111938081737 -0.06229703979360579\n",
      "step: 19 Loss: 0.0009702302917615258\n",
      "engine: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "manual: -0.08970773730279546 -0.1345616059541932 -0.04485386865139773\n",
      "step: 20 Loss: 0.00050296738324921\n",
      "engine: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "manual: -0.06458957085801131 -0.09688435628701697 -0.03229478542900566\n",
      "step: 21 Loss: 0.000260738291476379\n",
      "engine: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "manual: -0.04650449101777099 -0.06975673652665648 -0.023252245508885494\n",
      "step: 22 Loss: 0.00013516673030137142\n",
      "engine: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "manual: -0.033483233532798806 -0.05022485029919821 -0.016741616766399403\n",
      "step: 23 Loss: 7.00704329882464e-05\n",
      "engine: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "manual: -0.024107928143607182 -0.036161892215410774 -0.012053964071803591\n",
      "step: 24 Loss: 3.6324512461082954e-05\n",
      "engine: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "manual: -0.017357708263411098 -0.026036562395116647 -0.008678854131705549\n",
      "step: 25 Loss: 1.883062725985562e-05\n",
      "engine: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "manual: -0.012497549949650022 -0.018746324924475033 -0.006248774974825011\n",
      "step: 26 Loss: 9.761797171499829e-06\n",
      "engine: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "manual: -0.00899823596375171 -0.013497353945627566 -0.004499117981875855\n",
      "step: 27 Loss: 5.060515653709667e-06\n",
      "engine: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "manual: -0.006478729893892421 -0.009718094840838631 -0.0032393649469462105\n",
      "step: 28 Loss: 2.6233713148759562e-06\n",
      "engine: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "manual: -0.0046646855236005536 -0.00699702828540083 -0.0023323427618002768\n",
      "step: 29 Loss: 1.3599556896305355e-06\n",
      "engine: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "manual: -0.0033585735769960934 -0.00503786036549414 -0.0016792867884980467\n",
      "step: 30 Loss: 7.050010295060208e-07\n",
      "engine: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "manual: -0.002418172975445998 -0.003627259463168997 -0.001209086487722999\n",
      "step: 31 Loss: 3.654725336985844e-07\n",
      "engine: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "manual: -0.001741084542310034 -0.002611626813465051 -0.000870542271155017\n",
      "step: 32 Loss: 1.894609614669338e-07\n",
      "engine: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "manual: -0.001253580870461235 -0.0018803713056918525 -0.0006267904352306175\n",
      "step: 33 Loss: 9.821656242414673e-08\n",
      "engine: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "manual: -0.000902578226735784 -0.001353867340103676 -0.000451289113367892\n",
      "step: 34 Loss: 5.0915465961094525e-08\n",
      "engine: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "manual: -0.0006498563232497645 -0.0009747844848746468 -0.00032492816162488225\n",
      "step: 35 Loss: 2.6394577554231402e-08\n",
      "engine: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "manual: -0.00046789655274181996 -0.0007018448291127299 -0.00023394827637090998\n",
      "step: 36 Loss: 1.368294900422992e-08\n",
      "engine: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "manual: -0.0003368855179743946 -0.0005053282769615919 -0.0001684427589871973\n",
      "step: 37 Loss: 7.0932407638047585e-09\n",
      "engine: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "manual: -0.00024255757294611158 -0.00036383635941916737 -0.00012127878647305579\n",
      "step: 38 Loss: 3.6771360120942648e-09\n",
      "engine: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "manual: -0.0001746414525172213 -0.00026196217877583194 -8.732072625861065e-05\n",
      "step: 39 Loss: 1.9062273085828036e-09\n",
      "engine: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "manual: -0.000125741845806715 -0.0001886127687100725 -6.28709229033575e-05\n",
      "step: 40 Loss: 9.881882366799804e-10\n",
      "engine: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "manual: -9.053412899362456e-05 -0.00013580119349043684 -4.526706449681228e-05\n",
      "step: 41 Loss: 5.122767820396407e-10\n",
      "engine: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "manual: -6.518457286119883e-05 -9.777685929179825e-05 -3.2592286430599415e-05\n",
      "step: 42 Loss: 2.655642836935587e-10\n",
      "engine: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "manual: -4.693289246660015e-05 -7.039933869990023e-05 -2.3466446233300076e-05\n",
      "step: 43 Loss: 1.3766852470509082e-10\n",
      "engine: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "manual: -3.379168257566789e-05 -5.068752386350184e-05 -1.6895841287833946e-05\n",
      "step: 44 Loss: 7.136736320591857e-11\n",
      "engine: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "manual: -2.4330011456186185e-05 -3.649501718427928e-05 -1.2165005728093092e-05\n",
      "step: 45 Loss: 3.699684109113444e-11\n",
      "engine: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "manual: -1.7517608249306704e-05 -2.6276412373960056e-05 -8.758804124653352e-06\n",
      "step: 46 Loss: 1.9179162423511145e-11\n",
      "engine: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "manual: -1.2612677934953354e-05 -1.891901690243003e-05 -6.306338967476677e-06\n",
      "step: 47 Loss: 9.9424777931787e-12\n",
      "engine: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "manual: -9.081128119703408e-06 -1.3621692179555112e-05 -4.540564059851704e-06\n",
      "step: 48 Loss: 5.154180495404247e-12\n",
      "engine: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "manual: -6.5384122436285e-06 -9.80761836544275e-06 -3.26920612181425e-06\n",
      "step: 49 Loss: 2.671927166726942e-12\n",
      "engine: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "manual: -4.707656820812645e-06 -7.061485231218967e-06 -2.3538284104063223e-06\n",
      "step: 50 Loss: 1.3851270464089885e-12\n",
      "engine: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "manual: -3.389512905016545e-06 -5.084269357524818e-06 -1.6947564525082726e-06\n",
      "step: 51 Loss: 7.180498583296061e-13\n",
      "engine: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "manual: -2.440449293317215e-06 -3.6606739399758226e-06 -1.2202246466586075e-06\n",
      "step: 52 Loss: 3.722370470782809e-13\n",
      "engine: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "manual: -1.7571234849356188e-06 -2.635685227403428e-06 -8.785617424678094e-07\n",
      "step: 53 Loss: 1.9296768383201834e-13\n",
      "engine: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "manual: -1.2651289154064216e-06 -1.8976933731096324e-06 -6.325644577032108e-07\n",
      "step: 54 Loss: 1.0003444828733929e-13\n",
      "engine: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "manual: -9.108928225032287e-07 -1.366339233754843e-06 -4.5544641125161434e-07\n",
      "step: 55 Loss: 5.1857858380493653e-14\n",
      "engine: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "manual: -6.558428253811144e-07 -9.837642380716716e-07 -3.279214126905572e-07\n",
      "step: 56 Loss: 2.688311322524268e-14\n",
      "engine: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "manual: -4.7220683541127073e-07 -7.083102531169061e-07 -2.3610341770563537e-07\n",
      "step: 57 Loss: 1.3936205963070433e-14\n",
      "engine: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "manual: -3.399889223487662e-07 -5.099833835231493e-07 -1.699944611743831e-07\n",
      "step: 58 Loss: 7.224529207492211e-15\n",
      "engine: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "manual: -2.447920266490655e-07 -3.671880399735983e-07 -1.2239601332453276e-07\n",
      "step: 59 Loss: 3.7451960194348e-15\n",
      "engine: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "manual: -1.7625026060841265e-07 -2.6437539091261897e-07 -8.812513030420632e-08\n",
      "step: 60 Loss: 1.941509647783336e-15\n",
      "engine: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "manual: -1.2690017570093914e-07 -1.9035026355140872e-07 -6.345008785046957e-08\n",
      "step: 61 Loss: 1.0064784120580766e-15\n",
      "engine: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "manual: -9.136812906263003e-08 -1.3705219359394505e-07 -4.5684064531315016e-08\n",
      "step: 62 Loss: 5.217584380253387e-16\n",
      "engine: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "manual: -6.57850591778697e-08 -9.867758876680455e-08 -3.289252958893485e-08\n",
      "step: 63 Loss: 2.7047962568973864e-16\n",
      "engine: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "manual: -4.736522640769181e-08 -7.104783961153771e-08 -2.3682613203845904e-08\n",
      "step: 64 Loss: 1.4021654204074408e-16\n",
      "engine: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "manual: -3.410296756101161e-08 -5.1154451341517415e-08 -1.7051483780505805e-08\n",
      "step: 65 Loss: 7.268827477921314e-17\n",
      "engine: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "manual: -2.455414005453349e-08 -3.6831210081800236e-08 -1.2277070027266745e-08\n",
      "step: 66 Loss: 3.768161211360287e-17\n",
      "engine: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "manual: -1.7678985386737622e-08 -2.6518478080106433e-08 -8.839492693368811e-09\n",
      "step: 67 Loss: 1.953415776903015e-17\n",
      "engine: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "manual: -1.2728868625799805e-08 -1.9093302938699708e-08 -6.3644343128999026e-09\n",
      "step: 68 Loss: 1.0126506030804414e-17\n",
      "engine: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "manual: -9.164779157799785e-09 -1.3747168736699678e-08 -4.5823895788998925e-09\n",
      "step: 69 Loss: 5.2495735632025836e-18\n",
      "engine: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "manual: -6.598646962174826e-09 -9.897970443262238e-09 -3.299323481087413e-09\n",
      "step: 70 Loss: 2.721383858213691e-18\n",
      "engine: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "manual: -4.751022686377837e-09 -7.126534029566756e-09 -2.3755113431889185e-09\n",
      "step: 71 Loss: 1.410763535404805e-18\n",
      "engine: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "manual: -3.42073747106042e-09 -5.13110620659063e-09 -1.71036873553021e-09\n",
      "step: 72 Loss: 7.313403028698023e-19\n",
      "engine: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "manual: -2.46294007411052e-09 -3.69441011116578e-09 -1.23147003705526e-09\n",
      "step: 73 Loss: 3.791296130412209e-19\n",
      "engine: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "manual: -1.7733086110638396e-09 -2.6599629165957595e-09 -8.866543055319198e-10\n",
      "step: 74 Loss: 1.9653896437957275e-19\n",
      "engine: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "manual: -1.2767955581693968e-09 -1.9151933372540952e-09 -6.383977790846984e-10\n",
      "step: 75 Loss: 1.0188793108506885e-19\n",
      "engine: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "manual: -9.192859806717024e-10 -1.3789289710075536e-09 -4.596429903358512e-10\n",
      "step: 76 Loss: 5.281791964122085e-20\n",
      "engine: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "manual: -6.618847692152485e-10 -9.928271538228728e-10 -3.3094238460762426e-10\n",
      "step: 77 Loss: 2.7380715482445175e-20\n",
      "engine: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "manual: -4.765610128742992e-10 -7.148415193114488e-10 -2.382805064371496e-10\n",
      "step: 78 Loss: 1.4194399936986123e-20\n",
      "engine: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "manual: -3.4312819252591e-10 -5.14692288788865e-10 -1.71564096262955e-10\n",
      "step: 79 Loss: 7.358559781631122e-21\n",
      "engine: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "manual: -2.4704149836907163e-10 -3.7056224755360745e-10 -1.2352074918453582e-10\n",
      "step: 80 Loss: 3.8143438697772514e-21\n",
      "engine: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "manual: -1.7787726847018348e-10 -2.668159027052752e-10 -8.893863423509174e-11\n",
      "step: 81 Loss: 1.977520164900858e-21\n",
      "engine: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "manual: -1.2806822269340046e-10 -1.9210233404010069e-10 -6.403411134670023e-11\n",
      "step: 82 Loss: 1.0250918539904007e-21\n",
      "engine: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "manual: -9.22142362469458e-11 -1.383213543704187e-10 -4.61071181234729e-11\n",
      "step: 83 Loss: 5.314665854129708e-22\n",
      "engine: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "manual: -6.639311322942376e-11 -9.958966984413564e-11 -3.319655661471188e-11\n",
      "step: 84 Loss: 2.755028427684428e-22\n",
      "engine: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "manual: -4.780531526193954e-11 -7.170797289290931e-11 -2.390265763096977e-11\n",
      "step: 85 Loss: 1.4283426045583935e-22\n",
      "engine: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "manual: -3.441869012021925e-11 -5.162803518032888e-11 -1.7209345060109627e-11\n",
      "step: 86 Loss: 7.40403893494799e-23\n",
      "engine: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "manual: -2.4776625195954693e-11 -3.716493779393204e-11 -1.2388312597977347e-11\n",
      "step: 87 Loss: 3.836757225630106e-23\n",
      "engine: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "manual: -1.7834622667578515e-11 -2.6751934001367772e-11 -8.917311333789257e-12\n",
      "step: 88 Loss: 1.9879610355931586e-23\n",
      "engine: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "manual: -1.2846612662542611e-11 -1.9269918993813917e-11 -6.423306331271306e-12\n",
      "step: 89 Loss: 1.031471605633751e-23\n",
      "engine: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "manual: -9.244160992238903e-12 -1.3866241488358355e-11 -4.622080496119452e-12\n",
      "step: 90 Loss: 5.340907028151959e-24\n",
      "engine: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "manual: -6.657785434072139e-12 -9.986678151108208e-12 -3.3288927170360694e-12\n",
      "step: 91 Loss: 2.770381680383946e-24\n",
      "engine: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "manual: -4.789058039023075e-12 -7.183587058534613e-12 -2.3945290195115376e-12\n",
      "step: 92 Loss: 1.4334423063207214e-24\n",
      "engine: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "manual: -3.460343123151688e-12 -5.190514684727532e-12 -1.730171561575844e-12\n",
      "step: 93 Loss: 7.483734081214486e-25\n",
      "engine: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "manual: -2.4797941478027496e-12 -3.7196912217041245e-12 -1.2398970739013748e-12\n",
      "step: 94 Loss: 3.8433618846729784e-25\n",
      "engine: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "manual: -1.7905676941154525e-12 -2.6858515411731787e-12 -8.952838470577262e-13\n",
      "step: 95 Loss: 2.0038329170062053e-25\n",
      "engine: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "manual: -1.2860823517257813e-12 -1.929123527588672e-12 -6.430411758628907e-13\n",
      "step: 96 Loss: 1.0337548846378227e-25\n",
      "engine: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "manual: -9.308109838457312e-13 -1.3962164757685969e-12 -4.654054919228656e-13\n",
      "step: 97 Loss: 5.415056797799113e-26\n",
      "engine: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "manual: -6.750155989720952e-13 -1.0125233984581428e-12 -3.375077994860476e-13\n",
      "step: 98 Loss: 2.8477878678478526e-26\n",
      "engine: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "manual: -4.831690603168681e-13 -7.247535904753022e-13 -2.4158453015843406e-13\n",
      "step: 99 Loss: 1.4590771302967834e-26\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    zero_grad(w1)\n",
    "    zero_grad(w2)\n",
    "    zero_grad(b)\n",
    "    \n",
    "    y = w1*x1 + w2*x2 + b\n",
    "    l = (y-ytrue)**2\n",
    "\n",
    "    backward(l)\n",
    "\n",
    "    delta = 2*(y.value - ytrue)\n",
    "\n",
    "    manual_dw1 = delta * x1\n",
    "    manual_dw2 = delta * x2\n",
    "    manual_db = delta\n",
    "    \n",
    "    print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "    print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "    w1.value -= n * w1.grad\n",
    "    w2.value -= n * w2.grad\n",
    "    b.value -= n * b.grad\n",
    "\n",
    "    print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21723baf",
   "metadata": {},
   "source": [
    "Now for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbe8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Node(0)\n",
    "w2 = Node(0)\n",
    "b = Node(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83bde1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64 -96 -32\n",
      "manual: -64 -96 -32\n",
      "step: 0 Loss: 256\n",
      "engine: -6.16 -6.16 -6.16\n",
      "manual: -6.16 -6.16 -6.16\n",
      "step: 0 Loss: 9.4864\n",
      "engine: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "manual: -44.601600000000005 -66.9024 -22.300800000000002\n",
      "step: 1 Loss: 124.33142016000002\n",
      "engine: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "manual: -3.114303999999999 -3.114303999999999 -3.114303999999999\n",
      "step: 1 Loss: 2.4247223511039984\n",
      "engine: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "manual: -31.365719039999995 -47.048578559999996 -15.682859519999997\n",
      "step: 2 Loss: 61.48802068101364\n",
      "engine: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "manual: -1.0455026176000004 -1.0455026176000004 -1.0455026176000004\n",
      "step: 2 Loss: 0.2732689308521132\n",
      "engine: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "manual: -22.332397080576 -33.498595620864 -11.166198540288\n",
      "step: 3 Loss: 31.170997460282468\n",
      "engine: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "manual: 0.3571713642905596 0.3571713642905596 0.3571713642905596\n",
      "step: 3 Loss: 0.03189284586729491\n",
      "engine: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "manual: -16.16504702544446 -24.247570538166688 -8.08252351272223\n",
      "step: 4 Loss: 16.33179658342692\n",
      "engine: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "manual: 1.3056439039597922 1.3056439039597922 1.3056439039597922\n",
      "step: 4 Loss: 0.4261765009868418\n",
      "engine: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "manual: -11.952188395270362 -17.928282592905543 -5.976094197635181\n",
      "step: 5 Loss: 8.92842546475222\n",
      "engine: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "manual: 1.944436573438427 1.944436573438427 1.944436573438427\n",
      "step: 5 Loss: 0.9452083970312428\n",
      "engine: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "manual: -9.072240422219878 -13.608360633329816 -4.536120211109939\n",
      "step: 6 Loss: 5.144096642410019\n",
      "engine: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "manual: 2.372104804365314 2.372104804365314 2.372104804365314\n",
      "step: 6 Loss: 1.4067203007232514\n",
      "engine: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "manual: -7.101318257045989 -10.651977385568983 -3.5506591285229945\n",
      "step: 7 Loss: 3.1517950617409176\n",
      "engine: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "manual: 2.655857611526155 2.655857611526155 2.655857611526155\n",
      "step: 7 Loss: 1.7633949131753535\n",
      "engine: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "manual: -5.750354971839393 -8.625532457759089 -2.8751774859196964\n",
      "step: 8 Loss: 2.0666613938848766\n",
      "engine: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "manual: 2.84152745314495 2.84152745314495 2.84152745314495\n",
      "step: 8 Loss: 2.0185695667441066\n",
      "engine: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "manual: -4.8222221684791435 -7.233333252718715 -2.4111110842395718\n",
      "step: 9 Loss: 1.4533641651357307\n",
      "engine: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "manual: 2.960369136065001 2.960369136065001 2.960369136065001\n",
      "step: 9 Loss: 2.19094635544156\n",
      "engine: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "manual: -4.182488553960589 -6.273732830940883 -2.0912442769802944\n",
      "step: 10 Loss: 1.0933256565007086\n",
      "engine: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "manual: 3.033696301138738 3.033696301138738 3.033696301138738\n",
      "step: 10 Loss: 2.300828311885715\n",
      "engine: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "manual: -3.739478871124909 -5.609218306687364 -1.8697394355624546\n",
      "step: 11 Loss: 0.8739813892243515\n",
      "engine: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "manual: 3.0760432553379076 3.0760432553379076 3.0760432553379076\n",
      "step: 11 Loss: 2.3655105271774577\n",
      "engine: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "manual: -3.4306751684910424 -5.1460127527365636 -1.7153375842455212\n",
      "step: 12 Loss: 0.7355957569813151\n",
      "engine: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "manual: 3.0973211701270955 3.0973211701270955 3.0973211701270955\n",
      "step: 12 Loss: 2.39834960772937\n",
      "engine: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "manual: -3.213443202144049 -4.820164803216073 -1.6067216010720244\n",
      "step: 13 Loss: 0.6453885758378625\n",
      "engine: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "manual: 3.104288492048111 3.104288492048111 3.104288492048111\n",
      "step: 13 Loss: 2.409151760465584\n",
      "engine: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "manual: -3.058708343635274 -4.588062515452911 -1.529354171817637\n",
      "step: 14 Loss: 0.5847310457140027\n",
      "engine: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "manual: 3.10155368314334 3.10155368314334 3.10155368314334\n",
      "step: 14 Loss: 2.404908812355005\n",
      "engine: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "manual: -2.946642891371795 -4.419964337057692 -1.4733214456858974\n",
      "step: 15 Loss: 0.5426690205794957\n",
      "engine: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "manual: 3.092259035637049 3.092259035637049 3.092259035637049\n",
      "step: 15 Loss: 2.3905164858697434\n",
      "engine: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "manual: -2.8637250503405767 -4.295587575510865 -1.4318625251702883\n",
      "step: 16 Loss: 0.5125575727467586\n",
      "engine: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "manual: 3.0785469965192593 3.0785469965192593 3.0785469965192593\n",
      "step: 16 Loss: 2.369362902444438\n",
      "engine: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "manual: -2.800733315409836 -4.201099973114754 -1.400366657704918\n",
      "step: 17 Loss: 0.4902566940029108\n",
      "engine: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "manual: 3.0618781756526943 3.0618781756526943 3.0618781756526943\n",
      "step: 17 Loss: 2.343774490634568\n",
      "engine: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "manual: -2.7513787492517423 -4.1270681238776135 -1.3756893746258712\n",
      "step: 18 Loss: 0.4731303138646301\n",
      "engine: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "manual: 3.0432482100686364 3.0432482100686364 3.0432482100686364\n",
      "step: 18 Loss: 2.3153399170214897\n",
      "engine: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "manual: -2.711372269877721 -4.067058404816581 -1.3556861349388605\n",
      "step: 19 Loss: 0.45947122411636654\n",
      "engine: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "manual: 3.023335653657183 3.023335653657183 3.023335653657183\n",
      "step: 19 Loss: 2.2851396186686763\n",
      "engine: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "manual: -2.677788591189689 -4.016682886784533 -1.3388942955948444\n",
      "step: 20 Loss: 0.44815948369410363\n",
      "engine: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "manual: 3.002602829909131 3.002602829909131 3.002602829909131\n",
      "step: 20 Loss: 2.2539059385445808\n",
      "engine: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "manual: -2.648632464834762 -3.972948697252143 -1.324316232417381\n",
      "step: 21 Loss: 0.43845337086104164\n",
      "engine: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "manual: 2.981364608004668 2.981364608004668 2.981364608004668\n",
      "step: 21 Loss: 2.222133731465707\n",
      "engine: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "manual: -2.622542880602154 -3.933814320903231 -1.311271440301077\n",
      "step: 22 Loss: 0.42985819753731525\n",
      "engine: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "manual: 2.959835304360519 2.959835304360519 2.959835304360519\n",
      "step: 22 Loss: 2.1901562572347317\n",
      "engine: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "manual: -2.598591347080074 -3.897887020620111 -1.299295673540037\n",
      "step: 23 Loss: 0.42204231181996454\n",
      "engine: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "manual: 2.938160666923693 2.938160666923693 2.938160666923693\n",
      "step: 23 Loss: 2.15819702616437\n",
      "engine: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "manual: -2.576144329959334 -3.8642164949390008 -1.288072164979667\n",
      "step: 24 Loss: 0.41478247554885156\n",
      "engine: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "manual: 2.9164396867058304 2.9164396867058304 2.9164396867058304\n",
      "step: 24 Loss: 2.1264051115482006\n",
      "engine: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "manual: -2.5547694423801275 -3.8321541635701912 -1.2773847211900637\n",
      "step: 25 Loss: 0.4079279314824542\n",
      "engine: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "manual: 2.894739472046286 2.894739472046286 2.894739472046286\n",
      "step: 25 Loss: 2.0948791527557025\n",
      "engine: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "manual: -2.534171471804811 -3.8012572077072164 -1.2670857359024055\n",
      "step: 26 Loss: 0.4013765655318351\n",
      "engine: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "manual: 2.873105392031798 2.873105392031798 2.873105392031798\n",
      "step: 26 Loss: 2.0636836484305476\n",
      "engine: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "manual: -2.5141487537870972 -3.771223130680646 -1.2570743768935486\n",
      "step: 27 Loss: 0.39505899726057586\n",
      "engine: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "manual: 2.851567993737115 2.851567993737115 2.851567993737115\n",
      "step: 27 Loss: 2.0328600057264787\n",
      "engine: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "manual: -2.494563421223617 -3.7418451318354258 -1.2472817106118086\n",
      "step: 28 Loss: 0.38892791640667984\n",
      "engine: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "manual: 2.830147719386307 2.830147719386307 2.830147719386307\n",
      "step: 28 Loss: 2.002434028386879\n",
      "engine: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "manual: -2.475321115933724 -3.712981673900586 -1.237660557966862\n",
      "step: 29 Loss: 0.38295091418671107\n",
      "engine: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "manual: 2.808858123179151 2.808858123179151 2.808858123179151\n",
      "step: 29 Loss: 1.9724209890373758\n",
      "engine: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "manual: -2.4563571530352633 -3.684535729552895 -1.2281785765176316\n",
      "step: 30 Loss: 0.37710565395421897\n",
      "engine: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "manual: 2.787708064970518 2.787708064970518 2.787708064970518\n",
      "step: 30 Loss: 1.9428290638754178\n",
      "engine: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "manual: -2.437627085778324 -3.656440628667486 -1.218813542889162\n",
      "step: 31 Loss: 0.3713766130825078\n",
      "engine: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "manual: 2.7667032062189847 2.7667032062189847 2.7667032062189847\n",
      "step: 31 Loss: 1.9136616578256025\n",
      "engine: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "manual: -2.419100271252951 -3.6286504068794265 -1.2095501356264755\n",
      "step: 32 Loss: 0.3657528826485063\n",
      "engine: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "manual: 2.7458470301210234 2.7458470301210234 2.7458470301210234\n",
      "step: 32 Loss: 1.884918978206111\n",
      "engine: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "manual: -2.4007554825311743 -3.6011332237967615 -1.2003777412655872\n",
      "step: 33 Loss: 0.3602266804314682\n",
      "engine: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "manual: 2.72514153726563 2.72514153726563 2.72514153726563\n",
      "step: 33 Loss: 1.8565990995326203\n",
      "engine: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "manual: -2.382577916366195 -3.5738668745492923 -1.1912889581830974\n",
      "step: 34 Loss: 0.3547923454722424\n",
      "engine: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "manual: 2.7045877200116664 2.7045877200116664 2.7045877200116664\n",
      "step: 34 Loss: 1.8286986838094759\n",
      "engine: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "manual: -2.364557152586464 -3.546835728879696 -1.182278576293232\n",
      "step: 35 Loss: 0.3494456579904879\n",
      "engine: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "manual: 2.684185885966153 2.684185885966153 2.684185885966153\n",
      "step: 35 Loss: 1.8012134676049756\n",
      "engine: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "manual: -2.3466857624941326 -3.520028643741199 -1.1733428812470663\n",
      "step: 36 Loss: 0.3441833792432918\n",
      "engine: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "manual: 2.6639358785578295 2.6639358785578295 2.6639358785578295\n",
      "step: 36 Loss: 1.7741385912669188\n",
      "engine: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "manual: -2.3289583598496577 -3.4934375397744866 -1.1644791799248289\n",
      "step: 37 Loss: 0.33900294011960047\n",
      "engine: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "manual: 2.6438372274353394 2.6438372274353394 2.6438372274353394\n",
      "step: 37 Loss: 1.7474688212932457\n",
      "engine: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "manual: -2.3113709536762315 -3.467056430514347 -1.1556854768381157\n",
      "step: 38 Loss: 0.33390223034363575\n",
      "engine: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "manual: 2.6238892510097926 2.6238892510097926 2.6238892510097926\n",
      "step: 38 Loss: 1.7211987003911826\n",
      "engine: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "manual: -2.2939205068892434 -3.440880760333865 -1.1469602534446217\n",
      "step: 39 Loss: 0.3288794557454377\n",
      "engine: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "manual: 2.60409112636256 2.60409112636256 2.60409112636256\n",
      "step: 39 Loss: 1.6953226486000565\n",
      "engine: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "manual: -2.2766046352872706 -3.414906952930906 -1.1383023176436353\n",
      "step: 40 Loss: 0.3239330415882179\n",
      "engine: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "manual: 2.5844419368980436 2.5844419368980436 2.5844419368980436\n",
      "step: 40 Loss: 1.6698350312993278\n",
      "engine: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "manual: -2.2594214022623618 -3.3891321033935426 -1.1297107011311809\n",
      "step: 41 Loss: 0.3190615670625761\n",
      "engine: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "manual: 2.5649407048199 2.5649407048199 2.5649407048199\n",
      "step: 41 Loss: 1.6447302048105017\n",
      "engine: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "manual: -2.2423691787856797 -3.3635537681785195 -1.1211845893928398\n",
      "step: 42 Loss: 0.3142637208729977\n",
      "engine: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "manual: 2.545586413257846 2.545586413257846 2.545586413257846\n",
      "step: 42 Loss: 1.6200025468407362\n",
      "engine: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "manual: -2.2254465479075733 -3.33816982186136 -1.1127232739537867\n",
      "step: 43 Loss: 0.30953827109960846\n",
      "engine: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "manual: 2.5263780213368303 2.5263780213368303 2.5263780213368303\n",
      "step: 43 Loss: 1.5956464766734495\n",
      "engine: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "manual: -2.2086522396142954 -3.312978359421443 -1.1043261198071477\n",
      "step: 44 Loss: 0.3048840447220777\n",
      "engine: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "manual: 2.5073144744334783 2.5073144744334783 2.5073144744334783\n",
      "step: 44 Loss: 1.5716564684259073\n",
      "engine: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "manual: -2.1919850863863175 -3.2879776295794763 -1.0959925431931588\n",
      "step: 45 Loss: 0.300299913683752\n",
      "engine: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "manual: 2.488394711150651 2.488394711150651 2.488394711150651\n",
      "step: 45 Loss: 1.5480270596206327\n",
      "engine: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "manual: -2.175443992874314 -3.263165989311471 -1.087721996437157\n",
      "step: 46 Loss: 0.2957847853833086\n",
      "engine: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "manual: 2.4696176680540702 2.4696176680540702 2.4696176680540702\n",
      "step: 46 Loss: 1.5247528565912059\n",
      "engine: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "manual: -2.1590279152024863 -3.2385418728037294 -1.0795139576012431\n",
      "step: 47 Loss: 0.29133759616397464\n",
      "engine: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "manual: 2.450982282882972 2.450982282882972 2.450982282882972\n",
      "step: 47 Loss: 1.5018285377515561\n",
      "engine: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "manual: -2.1427358468377022 -3.2141037702565534 -1.0713679234188511\n",
      "step: 48 Loss: 0.2869573068327053\n",
      "engine: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "manual: 2.432487496720256 2.432487496720256 2.432487496720256\n",
      "step: 48 Loss: 1.4792488554250944\n",
      "engine: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "manual: -2.126566808936005 -3.189850213404007 -1.0632834044680024\n",
      "step: 49 Loss: 0.2826428995542664\n",
      "engine: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "manual: 2.414132255453202 2.414132255453202 2.414132255453202\n",
      "step: 49 Loss: 1.4570086367048911\n",
      "engine: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "manual: -2.1105198437427006 -3.165779765614051 -1.0552599218713503\n",
      "step: 50 Loss: 0.2783933756769821\n",
      "engine: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "manual: 2.395915510750571 2.395915510750571 2.395915510750571\n",
      "step: 50 Loss: 1.4351027836637926\n",
      "engine: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "manual: -2.0945940100748786 -3.141891015112318 -1.0472970050374393\n",
      "step: 51 Loss: 0.2742077541900975\n",
      "engine: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "manual: 2.37783622071003 2.37783622071003 2.37783622071003\n",
      "step: 51 Loss: 1.4135262731301397\n",
      "engine: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "manual: -2.0787883802243243 -3.1181825703364865 -1.0393941901121622\n",
      "step: 52 Loss: 0.27008507060972936\n",
      "engine: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "manual: 2.3598933502808865 2.3598933502808865 2.3598933502808865\n",
      "step: 52 Loss: 1.3922741561749867\n",
      "engine: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "manual: -2.0631020378289335 -3.0946530567434003 -1.0315510189144668\n",
      "step: 53 Loss: 0.26602437615586866\n",
      "engine: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "manual: 2.3420858715337705 2.3420858715337705 2.3420858715337705\n",
      "step: 53 Loss: 1.3713415574095253\n",
      "engine: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "manual: -2.0475340764049363 -3.0713011146074045 -1.0237670382024682\n",
      "step: 54 Loss: 0.2620247371274635\n",
      "engine: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "manual: 2.3244127638260377 2.3244127638260377 2.3244127638260377\n",
      "step: 54 Loss: 1.3507236741593498\n",
      "engine: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "manual: -2.0320835983297982 -3.0481253974946974 -1.0160417991648991\n",
      "step: 55 Loss: 0.2580852344125613\n",
      "engine: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "manual: 2.306873013896263 2.306873013896263 2.306873013896263\n",
      "step: 55 Loss: 1.3304157755607073\n",
      "engine: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "manual: -2.0167497141325583 -3.0251245711988375 -1.0083748570662792\n",
      "step: 56 Loss: 0.2542049630908597\n",
      "engine: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "manual: 2.2894656159104425 2.2894656159104425 2.2894656159104425\n",
      "step: 56 Loss: 1.3104132016090455\n",
      "engine: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "manual: -2.0015315419939483 -3.0022973129909225 -1.0007657709969742\n",
      "step: 57 Loss: 0.250383032099792\n",
      "engine: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "manual: 2.2721895714754528 2.2721895714754528 2.2721895714754528\n",
      "step: 57 Loss: 1.2907113621804505\n",
      "engine: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "manual: -1.98642820738975 -2.979642311084625 -0.993214103694875\n",
      "step: 58 Loss: 0.24661856394460346\n",
      "engine: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "manual: 2.2550438896303113 2.2550438896303113 2.2550438896303113\n",
      "step: 58 Loss: 1.2713057360397508\n",
      "engine: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "manual: -1.9714388428319012 -2.9571582642478518 -0.9857194214159506\n",
      "step: 59 Loss: 0.2429106944391491\n",
      "engine: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "manual: 2.2380275868224047 2.2380275868224047 2.2380275868224047\n",
      "step: 59 Loss: 1.252191869844529\n",
      "engine: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "manual: -1.9565625876763377 -2.9348438815145066 -0.9782812938381689\n",
      "step: 60 Loss: 0.23925857246842042\n",
      "engine: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "manual: 2.2211396868736433 2.2211396868736433 2.2211396868736433\n",
      "step: 60 Loss: 1.2333653771512865\n",
      "engine: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "manual: -1.941798587976642 -2.912697881964963 -0.970899293988321\n",
      "step: 61 Loss: 0.23566135976675504\n",
      "engine: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "manual: 2.2043792209398205 2.2043792209398205 2.2043792209398205\n",
      "step: 61 Loss: 1.2148219374278124\n",
      "engine: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "manual: -1.9271459963687363 -2.8907189945531044 -0.9635729981843681\n",
      "step: 62 Loss: 0.2321182307075031\n",
      "engine: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "manual: 2.187745227465557 2.187745227465557 2.187745227465557\n",
      "step: 62 Loss: 1.1965572950745806\n",
      "engine: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "manual: -1.9126039719772265 -2.86890595796584 -0.9563019859886133\n",
      "step: 63 Loss: 0.22862837210144146\n",
      "engine: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "manual: 2.1712367521362577 2.1712367521362577 2.1712367521362577\n",
      "step: 63 Loss: 1.1785672584568012\n",
      "engine: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "manual: -1.89817168033629 -2.847257520504435 -0.949085840168145\n",
      "step: 64 Loss: 0.22519098300191842\n",
      "engine: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "manual: 2.154852847828261 2.154852847828261 2.154852847828261\n",
      "step: 64 Loss: 1.1608476989483916\n",
      "engine: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "manual: -1.8838482933209235 -2.8257724399813853 -0.9419241466604618\n",
      "step: 65 Loss: 0.22180527451550977\n",
      "engine: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "manual: 2.138592574557819 2.138592574557819 2.138592574557819\n",
      "step: 65 Loss: 1.1433945499884604\n",
      "engine: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "manual: -1.8696329890849341 -2.804449483627401 -0.9348164945424671\n",
      "step: 66 Loss: 0.2184704696171666\n",
      "engine: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "manual: 2.1224549994294435 2.1224549994294435 2.1224549994294435\n",
      "step: 66 Loss: 1.1262038061507598\n",
      "engine: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "manual: -1.8555249520042238 -2.7832874280063358 -0.9277624760021119\n",
      "step: 67 Loss: 0.21518580296939233\n",
      "engine: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "manual: 2.1064391965839313 2.1064391965839313 2.1064391965839313\n",
      "step: 67 Loss: 1.1092715222262894\n",
      "engine: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "manual: -1.841523372623179 -2.7622850589347685 -0.9207616863115895\n",
      "step: 68 Loss: 0.2119505207448405\n",
      "engine: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "manual: 2.090544247146287 2.090544247146287 2.090544247146287\n",
      "step: 68 Loss: 1.092593812319109\n",
      "engine: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "manual: -1.8276274476038026 -2.741441171405704 -0.9138137238019013\n",
      "step: 69 Loss: 0.2087638804521744\n",
      "engine: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "manual: 2.0747692391737385 2.0747692391737385 2.0747692391737385\n",
      "step: 69 Loss: 1.0761668489553933\n",
      "engine: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "manual: -1.813836379676431 -2.7207545695146464 -0.9069181898382155\n",
      "step: 70 Loss: 0.20562515076485635\n",
      "engine: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "manual: 2.0591132676039017 2.0591132676039017 2.0591132676039017\n",
      "step: 70 Loss: 1.0599868622056043\n",
      "engine: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "manual: -1.8001493775919641 -2.700224066387946 -0.9000746887959821\n",
      "step: 71 Loss: 0.202533611352796\n",
      "engine: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "manual: 2.0435754342031824 2.0435754342031824 2.0435754342031824\n",
      "step: 71 Loss: 1.0440501388196815\n",
      "engine: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "manual: -1.7865656560749912 -2.6798484841124868 -0.8932828280374956\n",
      "step: 72 Loss: 0.1994885527166665\n",
      "engine: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "manual: 2.0281548475154914 2.0281548475154914 2.0281548475154914\n",
      "step: 72 Loss: 1.0283530213751464\n",
      "engine: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "manual: -1.7730844357776974 -2.659626653666546 -0.8865422178888487\n",
      "step: 73 Loss: 0.19648927602481972\n",
      "engine: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "manual: 2.0128506228112233 2.0128506228112233 2.0128506228112233\n",
      "step: 73 Loss: 1.0128919074378824\n",
      "engine: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "manual: -1.7597049432346452 -2.639557414851968 -0.8798524716173226\n",
      "step: 74 Loss: 0.19353509295277788\n",
      "engine: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "manual: 1.9976618820366294 1.9976618820366294 1.9976618820366294\n",
      "step: 74 Loss: 0.997663248735532\n",
      "engine: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "manual: -1.7464264108177332 -2.6196396162266 -0.8732132054088666\n",
      "step: 75 Loss: 0.19062532552510686\n",
      "engine: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "manual: 1.9825877537634966 1.9825877537634966 1.9825877537634966\n",
      "step: 75 Loss: 0.9826635503432468\n",
      "engine: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "manual: -1.733248076692007 -2.5998721150380106 -0.8666240383460035\n",
      "step: 76 Loss: 0.18775930595978385\n",
      "engine: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "manual: 1.9676273731392069 1.9676273731392069 1.9676273731392069\n",
      "step: 76 Loss: 0.967889369881674\n",
      "engine: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "manual: -1.720169184771656 -2.580253777157484 -0.860084592385828\n",
      "step: 77 Loss: 0.184936376514874\n",
      "engine: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "manual: 1.952779881837154 1.952779881837154 1.952779881837154\n",
      "step: 77 Loss: 0.9533373167269823\n",
      "engine: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "manual: -1.7071889846765131 -2.5607834770147697 -0.8535944923382566\n",
      "step: 78 Loss: 0.1821558893375515\n",
      "engine: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "manual: 1.9380444280075153 1.9380444280075153 1.9380444280075153\n",
      "step: 78 Loss: 0.9390040512327443\n",
      "engine: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "manual: -1.69430673168889 -2.541460097533335 -0.847153365844445\n",
      "step: 79 Loss: 0.179417206315393\n",
      "engine: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "manual: 1.9234201662283965 1.9234201662283965 1.9234201662283965\n",
      "step: 79 Loss: 0.9248862839635181\n",
      "engine: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "manual: -1.6815216867108163 -2.5222825300662244 -0.8407608433554081\n",
      "step: 80 Loss: 0.17671969892992428\n",
      "engine: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "manual: 1.9089062574573425 1.9089062574573425 1.9089062574573425\n",
      "step: 80 Loss: 0.9109807749399496\n",
      "engine: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "manual: -1.668833116221549 -2.5032496743323236 -0.8344165581107745\n",
      "step: 81 Loss: 0.1740627481123579\n",
      "engine: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "manual: 1.8945018689831965 1.8945018689831965 1.8945018689831965\n",
      "step: 81 Loss: 0.8972843328952061\n",
      "engine: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "manual: -1.6562402922354806 -2.484360438353221 -0.8281201461177403\n",
      "step: 82 Loss: 0.1714457441015169\n",
      "engine: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "manual: 1.8802061743783334 1.8802061743783334 1.8802061743783334\n",
      "step: 82 Loss: 0.883793814542602\n",
      "engine: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "manual: -1.643742492260344 -2.465613738390516 -0.821871246130172\n",
      "step: 83 Loss: 0.16886808630389047\n",
      "engine: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "manual: 1.8660183534512544 1.8660183534512544 1.8660183534512544\n",
      "step: 83 Loss: 0.8705061238542326\n",
      "engine: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "manual: -1.631338999255746 -2.447008498883619 -0.815669499627873\n",
      "step: 84 Loss: 0.1663291831557962\n",
      "engine: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "manual: 1.8519375921995227 1.8519375921995227 1.8519375921995227\n",
      "step: 84 Loss: 0.8574182113504414\n",
      "engine: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "manual: -1.6190291015920195 -2.4285436523880293 -0.8095145507960098\n",
      "step: 85 Loss: 0.16382845198761636\n",
      "engine: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "manual: 1.8379630827630749 1.8379630827630749 1.8379630827630749\n",
      "step: 85 Loss: 0.8445270733999863\n",
      "engine: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "manual: -1.6068120930093954 -2.410218139514093 -0.8034060465046977\n",
      "step: 86 Loss: 0.16136531889007713\n",
      "engine: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "manual: 1.8240940233778513 1.8240940233778513 1.8240940233778513\n",
      "step: 86 Loss: 0.8318297515306993\n",
      "engine: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "manual: -1.5946872725774455 -2.392030908866168 -0.7973436362887227\n",
      "step: 87 Loss: 0.15893921858253074\n",
      "engine: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "manual: 1.8103296183298276 1.8103296183298276 1.8103296183298276\n",
      "step: 87 Loss: 0.8193233317505548\n",
      "engine: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "manual: -1.582653944654922 -2.373980916982383 -0.791326972327461\n",
      "step: 88 Loss: 0.15654959428323653\n",
      "engine: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "manual: 1.7966690779093355 1.7966690779093355 1.7966690779093355\n",
      "step: 88 Loss: 0.8070049438788954\n",
      "engine: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "manual: -1.5707114188497826 -2.356067128274674 -0.7853557094248913\n",
      "step: 89 Loss: 0.15419589758156857\n",
      "engine: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "manual: 1.7831116183657603 1.7831116183657603 1.7831116183657603\n",
      "step: 89 Loss: 0.7948717608877401\n",
      "engine: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "manual: -1.5588590099796207 -2.338288514969431 -0.7794295049898103\n",
      "step: 90 Loss: 0.1518775883121652\n",
      "engine: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "manual: 1.769656461862592 1.769656461862592 1.769656461862592\n",
      "step: 90 Loss: 0.782920998253007\n",
      "engine: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "manual: -1.5470960380323646 -2.320644057048547 -0.7735480190161823\n",
      "step: 91 Loss: 0.14959413443096498\n",
      "engine: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "manual: 1.7563028364327788 1.7563028364327788 1.7563028364327788\n",
      "step: 91 Loss: 0.7711499133154561\n",
      "engine: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "manual: -1.5354218281271628 -2.303132742190744 -0.7677109140635814\n",
      "step: 92 Loss: 0.1473450118930849\n",
      "engine: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "manual: 1.7430499759344418 1.7430499759344418 1.7430499759344418\n",
      "step: 92 Loss: 0.7595558046512645\n",
      "engine: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "manual: -1.5238357104758222 -2.2857535657137333 -0.7619178552379111\n",
      "step: 93 Loss: 0.1451297045325846\n",
      "engine: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "manual: 1.7298971200069229 1.7298971200069229 1.7298971200069229\n",
      "step: 93 Loss: 0.7481360114520615\n",
      "engine: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "manual: -1.5123370203442548 -2.268505530516382 -0.7561685101721274\n",
      "step: 94 Loss: 0.14294770394398368\n",
      "engine: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "manual: 1.716843514027163 1.716843514027163 1.716843514027163\n",
      "step: 94 Loss: 0.7368879129142843\n",
      "engine: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "manual: -1.5009250980143847 -2.251387647021577 -0.7504625490071923\n",
      "step: 95 Loss: 0.14079850936559313\n",
      "engine: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "manual: 1.7038884090663977 1.7038884090663977 1.7038884090663977\n",
      "step: 95 Loss: 0.725808927637705\n",
      "engine: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "manual: -1.489599288746291 -2.2343989331194365 -0.7447996443731455\n",
      "step: 96 Loss: 0.13868162756459101\n",
      "engine: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "manual: 1.691031061847191 1.691031061847191 1.691031061847191\n",
      "step: 96 Loss: 0.7148965130330096\n",
      "engine: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "manual: -1.4783589427406483 -2.2175384141109724 -0.7391794713703241\n",
      "step: 97 Loss: 0.13659657272382797\n",
      "engine: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "manual: 1.6782707347007992 1.6782707347007992 1.6782707347007992\n",
      "step: 97 Loss: 0.7041481647382901\n",
      "engine: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "manual: -1.4672034151014657 -2.2008051226521985 -0.7336017075507328\n",
      "step: 98 Loss: 0.13454286633033774\n",
      "engine: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "manual: 1.665606695524838 1.665606695524838 1.665606695524838\n",
      "step: 98 Loss: 0.6935614160442927\n",
      "engine: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "manual: -1.4561320657990109 -2.1841980986985163 -0.7280660328995054\n",
      "step: 99 Loss: 0.13252003706550594\n",
      "engine: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "manual: 1.6530382177412903 1.6530382177412903 1.6530382177412903\n",
      "step: 99 Loss: 0.6831338373283254\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        \n",
    "        zero_grad(w1)\n",
    "        zero_grad(w2)\n",
    "        zero_grad(b)\n",
    "\n",
    "        y = w1*x1 + w2*x2 + b\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "        \n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", w1.grad, w2.grad, b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        w1.value -= n * w1.grad\n",
    "        w2.value -= n * w2.grad\n",
    "        b.value -= n * b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fe5c5",
   "metadata": {},
   "source": [
    "### Observations with Dataset Training\n",
    "\n",
    "Training over multiple examples introduces SGD dynamics.\n",
    "\n",
    "We observe:\n",
    "\n",
    "- Loss oscillates because updates happen per sample\n",
    "- Gradients must be zeroed before backward\n",
    "- Parameters must persist across steps\n",
    "\n",
    "This validates the full training lifecycle:\n",
    "\n",
    "1. zero_grad  \n",
    "2. forward  \n",
    "3. backward  \n",
    "4. update  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e5b5a",
   "metadata": {},
   "source": [
    "## Class Functions of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c765068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4405ab",
   "metadata": {},
   "source": [
    "### Abstraction — Turning Neuron into a Class\n",
    "\n",
    "To scale the system, we abstract the neuron into a reusable class.\n",
    "\n",
    "This introduces:\n",
    "\n",
    "- Weight vector abstraction\n",
    "- Dimension safety checks\n",
    "- Dot-product computation\n",
    "- Random initialization to break symmetry\n",
    "\n",
    "Now the neuron represents a hyperplane in ℝⁿ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d95f",
   "metadata": {},
   "source": [
    "#### For a single input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron for a single input\n",
    "class neuron:\n",
    "    def __init__(self):\n",
    "        self.w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.b = Node(0)\n",
    "    def pred(self, x):\n",
    "        y = self.w * x + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39690",
   "metadata": {},
   "source": [
    "#### For multi-input neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee2ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated __init___ and pred for multi-inputs\n",
    "def __init__(self, dim=1):\n",
    "    self.w = []\n",
    "    self.b = Node(0)\n",
    "    self.dim = dim\n",
    "    for i in range (0, self.dim):\n",
    "        w = Node(random.uniform(-0.1, 0.1))\n",
    "        self.w.append(w)\n",
    "        i += 1\n",
    "\n",
    "neuron.__init__ = __init__\n",
    "\n",
    "def pred(self, x):\n",
    "    if len(x) != len(self.w):\n",
    "        raise ValueError(\"Input dimension does not match neuron weight dimension\")\n",
    "    else:\n",
    "        wx = Node(0)\n",
    "        for i in range(0, self.dim):\n",
    "            wx += x[i]*self.w[i]\n",
    "        y = wx + self.b\n",
    "        return y\n",
    "\n",
    "neuron.pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd57a4a",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba23d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine: -64.71184088262721 -97.06776132394083 -32.35592044131361\n",
      "manual: -64.71184088262721 -97.06776132394083 -32.35592044131361\n",
      "step: 0 Loss: 261.7263969011539\n",
      "engine: -6.253014227859012 -6.253014227859012 -6.253014227859012\n",
      "manual: -6.253014227859012 -6.253014227859012 -6.253014227859012\n",
      "step: 0 Loss: 9.77504673345181\n",
      "engine: -45.09180202080543 -67.63770303120815 -22.545901010402716\n",
      "manual: -45.09180202080543 -67.63770303120815 -22.545901010402716\n",
      "step: 1 Loss: 127.07941309271955\n",
      "engine: -3.172325252939145 -3.172325252939145 -3.172325252939145\n",
      "manual: -3.172325252939145 -3.172325252939145 -3.172325252939145\n",
      "step: 1 Loss: 2.5159118776088527\n",
      "engine: -31.70473939427452 -47.557109091411775 -15.85236969713726\n",
      "manual: -31.70473939427452 -47.557109091411775 -15.85236969713726\n",
      "step: 2 Loss: 62.82440625367891\n",
      "engine: -1.0797013741063264 -1.0797013741063264 -1.0797013741063264\n",
      "manual: -1.0797013741063264 -1.0797013741063264 -1.0797013741063264\n",
      "step: 2 Loss: 0.29143876431177235\n",
      "engine: -22.568284034092137 -33.85242605113821 -11.284142017046069\n",
      "manual: -22.568284034092137 -33.85242605113821 -11.284142017046069\n",
      "step: 3 Loss: 31.83296526521613\n",
      "engine: 0.3391777503855824 0.3391777503855824 0.3391777503855824\n",
      "manual: 0.3391777503855824 0.3391777503855824 0.3391777503855824\n",
      "step: 3 Loss: 0.02876038658915611\n",
      "engine: -16.33056716463888 -24.495850746958318 -8.16528358231944\n",
      "manual: -16.33056716463888 -24.495850746958318 -8.16528358231944\n",
      "step: 4 Loss: 16.667963994923845\n",
      "engine: 1.2986611152407797 1.2986611152407797 1.2986611152407797\n",
      "manual: 1.2986611152407797 1.2986611152407797 1.2986611152407797\n",
      "step: 4 Loss: 0.4216301730596064\n",
      "engine: -12.069687026197776 -18.104530539296665 -6.034843513098888\n",
      "manual: -12.069687026197776 -18.104530539296665 -6.034843513098888\n",
      "step: 5 Loss: 9.104834056897932\n",
      "engine: 1.944922669898201 1.944922669898201 1.944922669898201\n",
      "manual: 1.944922669898201 1.944922669898201 1.944922669898201\n",
      "step: 5 Loss: 0.9456810479709866\n",
      "engine: -9.156956099637966 -13.73543414945695 -4.578478049818983\n",
      "manual: -9.156956099637966 -13.73543414945695 -4.578478049818983\n",
      "step: 6 Loss: 5.2406153131685596\n",
      "engine: 2.3776446756825855 2.3776446756825855 2.3776446756825855\n",
      "manual: 2.3776446756825855 2.3776446756825855 2.3776446756825855\n",
      "step: 6 Loss: 1.4132985509504368\n",
      "engine: -7.163643113903163 -10.745464670854744 -3.5818215569515814\n",
      "manual: -7.163643113903163 -10.745464670854744 -3.5818215569515814\n",
      "step: 7 Loss: 3.207361416460763\n",
      "engine: 2.664804581975819 2.664804581975819 2.664804581975819\n",
      "manual: 2.664804581975819 2.664804581975819 2.664804581975819\n",
      "step: 7 Loss: 1.7752958650298296\n",
      "engine: -5.797376141684474 -8.69606421252671 -2.898688070842237\n",
      "manual: -5.797376141684474 -8.69606421252671 -2.898688070842237\n",
      "step: 8 Loss: 2.1005981330107724\n",
      "engine: 2.8527588755583384 2.8527588755583384 2.8527588755583384\n",
      "manual: 2.8527588755583384 2.8527588755583384 2.8527588755583384\n",
      "step: 8 Loss: 2.034558300519219\n",
      "engine: -4.858772952146815 -7.288159428220222 -2.4293864760734074\n",
      "manual: -4.858772952146815 -7.288159428220222 -2.4293864760734074\n",
      "step: 9 Loss: 1.475479662532092\n",
      "engine: 2.9731197201536492 2.9731197201536492 2.9731197201536492\n",
      "manual: 2.9731197201536492 2.9731197201536492 2.9731197201536492\n",
      "step: 9 Loss: 2.209860217591628\n",
      "engine: -4.211865258382581 -6.317797887573871 -2.1059326291912903\n",
      "manual: -4.211865258382581 -6.317797887573871 -2.1059326291912903\n",
      "step: 10 Loss: 1.108738059673135\n",
      "engine: 3.047444452447385 3.047444452447385 3.047444452447385\n",
      "manual: 3.047444452447385 3.047444452447385 3.047444452447385\n",
      "step: 10 Loss: 2.3217294226880854\n",
      "engine: -3.763929654622828 -5.645894481934242 -1.881964827311414\n",
      "manual: -3.763929654622828 -5.645894481934242 -1.881964827311414\n",
      "step: 11 Loss: 0.8854479028093201\n",
      "engine: 3.0904335645779124 3.0904335645779124 3.0904335645779124\n",
      "manual: 3.0904335645779124 3.0904335645779124 3.0904335645779124\n",
      "step: 11 Loss: 2.3876949042674354\n",
      "engine: -3.4517334068271452 -5.177600110240718 -1.7258667034135726\n",
      "manual: -3.4517334068271452 -5.177600110240718 -1.7258667034135726\n",
      "step: 12 Loss: 0.7446539694879082\n",
      "engine: 3.1121115551128646 3.1121115551128646 3.1121115551128646\n",
      "manual: 3.1121115551128646 3.1121115551128646 3.1121115551128646\n",
      "step: 12 Loss: 2.421309582866753\n",
      "engine: -3.232154826142626 -4.848232239213939 -1.616077413071313\n",
      "manual: -3.232154826142626 -4.848232239213939 -1.616077413071313\n",
      "step: 13 Loss: 0.6529265512598167\n",
      "engine: 3.1193141513746507 3.1193141513746507 3.1193141513746507\n",
      "manual: 3.1193141513746507 3.1193141513746507 3.1193141513746507\n",
      "step: 13 Loss: 2.4325301937415396\n",
      "engine: -3.0757868711526157 -4.613680306728924 -1.5378934355763079\n",
      "manual: -3.0757868711526157 -4.613680306728924 -1.5378934355763079\n",
      "step: 14 Loss: 0.5912790547971748\n",
      "engine: 3.116702514561327 3.116702514561327 3.116702514561327\n",
      "manual: 3.116702514561327 3.116702514561327 3.116702514561327\n",
      "step: 14 Loss: 2.4284586410682247\n",
      "engine: -2.9625751507245894 -4.443862726086884 -1.4812875753622947\n",
      "manual: -2.9625751507245894 -4.443862726086884 -1.4812875753622947\n",
      "step: 15 Loss: 0.5485532202306765\n",
      "engine: 3.1074548727311253 3.1074548727311253 3.1074548727311253\n",
      "manual: 3.1074548727311253 3.1074548727311253 3.1074548727311253\n",
      "step: 15 Loss: 2.4140689465151035\n",
      "engine: -2.8788432779771824 -4.318264916965774 -1.4394216389885912\n",
      "manual: -2.8788432779771824 -4.318264916965774 -1.4394216389885912\n",
      "step: 16 Loss: 0.5179836636971505\n",
      "engine: 3.093738177045889 3.093738177045889 3.093738177045889\n",
      "manual: 3.093738177045889 3.093738177045889 3.093738177045889\n",
      "step: 16 Loss: 2.3928039770278056\n",
      "engine: -2.815264322634583 -4.222896483951875 -1.4076321613172915\n",
      "manual: -2.815264322634583 -4.222896483951875 -1.4076321613172915\n",
      "step: 17 Loss: 0.49535707539369733\n",
      "engine: 3.0770297457812106 3.0770297457812106 3.0770297457812106\n",
      "manual: 3.0770297457812106 3.0770297457812106 3.0770297457812106\n",
      "step: 17 Loss: 2.3670280141055953\n",
      "engine: -2.76547745128439 -4.148216176926585 -1.382738725642195\n",
      "manual: -2.76547745128439 -4.148216176926585 -1.382738725642195\n",
      "step: 18 Loss: 0.4779915958476504\n",
      "engine: 3.0583366081114 3.0583366081114 3.0583366081114\n",
      "manual: 3.0583366081114 3.0583366081114 3.0583366081114\n",
      "step: 18 Loss: 2.338355702128586\n",
      "engine: -2.725144550871498 -4.087716826307247 -1.362572275435749\n",
      "manual: -2.725144550871498 -4.087716826307247 -1.362572275435749\n",
      "step: 19 Loss: 0.46415080144653864\n",
      "engine: 3.0383450846770046 3.0383450846770046 3.0383450846770046\n",
      "manual: 3.0383450846770046 3.0383450846770046 3.0383450846770046\n",
      "step: 19 Loss: 2.3078852133952283\n",
      "engine: -2.6913068969499676 -4.036960345424951 -1.3456534484749838\n",
      "manual: -2.6913068969499676 -4.036960345424951 -1.3456534484749838\n",
      "step: 20 Loss: 0.452695800848154\n",
      "engine: 3.017522793413381 3.017522793413381 3.017522793413381\n",
      "manual: 3.017522793413381 3.017522793413381 3.017522793413381\n",
      "step: 20 Loss: 2.276360952192323\n",
      "engine: -2.6619464362231753 -3.992919654334763 -1.3309732181115876\n",
      "manual: -2.6619464362231753 -3.992919654334763 -1.3309732181115876\n",
      "step: 21 Loss: 0.44287242683257894\n",
      "engine: 2.9961882119819716 2.9961882119819716 2.9961882119819716\n",
      "manual: 2.9961882119819716 2.9961882119819716 2.9961882119819716\n",
      "step: 21 Loss: 2.244285950404931\n",
      "engine: -2.635686604956362 -3.953529907434543 -1.317843302478181\n",
      "manual: -2.635686604956362 -3.953529907434543 -1.317843302478181\n",
      "step: 22 Loss: 0.43417774247164964\n",
      "engine: 2.974558115560436 2.974558115560436 2.974558115560436\n",
      "manual: 2.974558115560436 2.974558115560436 2.974558115560436\n",
      "step: 22 Loss: 2.211998995711613\n",
      "engine: -2.61158830330308 -3.9173824549546197 -1.30579415165154\n",
      "manual: -2.61158830330308 -3.9173824549546197 -1.30579415165154\n",
      "step: 23 Loss: 0.42627459162184117\n",
      "engine: 2.952779926824995 2.952779926824995 2.952779926824995\n",
      "manual: 2.952779926824995 2.952779926824995 2.952779926824995\n",
      "step: 23 Loss: 2.179727324065156\n",
      "engine: -2.5890107608162154 -3.883516141224323 -1.2945053804081077\n",
      "manual: -2.5890107608162154 -3.883516141224323 -1.2945053804081077\n",
      "step: 24 Loss: 0.4189360449763849\n",
      "engine: 2.9309537768644667 2.9309537768644667 2.9309537768644667\n",
      "manual: 2.9309537768644667 2.9309537768644667 2.9309537768644667\n",
      "step: 24 Loss: 2.1476225105290205\n",
      "engine: -2.567516654235149 -3.8512749813527236 -1.2837583271175745\n",
      "manual: -2.567516654235149 -3.8512749813527236 -1.2837583271175745\n",
      "step: 25 Loss: 0.4120088606109284\n",
      "engine: 2.9091475495067076 2.9091475495067076 2.9091475495067076\n",
      "manual: 2.9091475495067076 2.9091475495067076 2.9091475495067076\n",
      "step: 25 Loss: 2.1157848662002205\n",
      "engine: -2.5468074029309165 -3.8202111043963747 -1.2734037014654582\n",
      "manual: -2.5468074029309165 -3.8202111043963747 -1.2734037014654582\n",
      "step: 26 Loss: 0.40538924672648247\n",
      "engine: 2.8874071407121615 2.8874071407121615 2.8874071407121615\n",
      "manual: 2.8874071407121615 2.8874071407121615 2.8874071407121615\n",
      "step: 26 Loss: 2.084279999058895\n",
      "engine: -2.526679043881181 -3.7900185658217715 -1.2633395219405905\n",
      "manual: -2.526679043881181 -3.7900185658217715 -1.2633395219405905\n",
      "step: 27 Loss: 0.39900668692426994\n",
      "engine: 2.8657634549023 2.8657634549023 2.8657634549023\n",
      "manual: 2.8657634549023 2.8657634549023 2.8657634549023\n",
      "step: 27 Loss: 2.0531500448633917\n",
      "engine: -2.506992140771004 -3.760488211156506 -1.253496070385502\n",
      "manual: -2.506992140771004 -3.760488211156506 -1.253496070385502\n",
      "step: 28 Loss: 0.3928130996179739\n",
      "engine: 2.844237176054424 2.844237176054424 2.844237176054424\n",
      "manual: 2.844237176054424 2.844237176054424 2.844237176054424\n",
      "step: 28 Loss: 2.0224212784125113\n",
      "engine: -2.4876512636081856 -3.7314768954122783 -1.2438256318040928\n",
      "manual: -2.4876512636081856 -3.7314768954122783 -1.2438256318040928\n",
      "step: 29 Loss: 0.38677555058321267\n",
      "engine: 2.8228420213076504 2.8228420213076504 2.8228420213076504\n",
      "manual: 2.8228420213076504 2.8228420213076504 2.8228420213076504\n",
      "step: 29 Loss: 1.9921092693150653\n",
      "engine: -2.4685909949117217 -3.7028864923675826 -1.2342954974558609\n",
      "manual: -2.4685909949117217 -3.7028864923675826 -1.2342954974558609\n",
      "step: 30 Loss: 0.38087134375995274\n",
      "engine: 2.8015869597238936 2.8015869597238936 2.8015869597238936\n",
      "manual: 2.8015869597238936 2.8015869597238936 2.8015869597238936\n",
      "step: 30 Loss: 1.9622223732237423\n",
      "engine: -2.449766386670177 -3.6746495800052656 -1.2248831933350885\n",
      "manual: -2.449766386670177 -3.6746495800052656 -1.2248831933350885\n",
      "step: 31 Loss: 0.375084709328691\n",
      "engine: 2.7804777253406705 2.7804777253406705 2.7804777253406705\n",
      "manual: 2.7804777253406705 2.7804777253406705 2.7804777253406705\n",
      "step: 31 Loss: 1.9327640952789074\n",
      "engine: -2.431146452484299 -3.6467196787264484 -1.2155732262421495\n",
      "manual: -2.431146452484299 -3.6467196787264484 -1.2155732262421495\n",
      "step: 32 Loss: 0.36940456708918695\n",
      "engine: 2.7595178489692884 2.7595178489692884 2.7595178489692884\n",
      "manual: 2.7595178489692884 2.7595178489692884 2.7595178489692884\n",
      "step: 32 Loss: 1.903734689695022\n",
      "engine: -2.4127097295413265 -3.6190645943119897 -1.2063548647706632\n",
      "manual: -2.4127097295413265 -3.6190645943119897 -1.2063548647706632\n",
      "step: 33 Loss: 0.3638230149389613\n",
      "engine: 2.7387093618036094 2.7387093618036094 2.7387093618036094\n",
      "manual: 2.7387093618036094 2.7387093618036094 2.7387093618036094\n",
      "step: 33 Loss: 1.8751322421076833\n",
      "engine: -2.3944412521026237 -3.5916618781539356 -1.1972206260513119\n",
      "manual: -2.3944412521026237 -3.5916618781539356 -1.1972206260513119\n",
      "step: 34 Loss: 0.3583343068606738\n",
      "engine: 2.71805327522155 2.71805327522155 2.71805327522155\n",
      "manual: 2.71805327522155 2.71805327522155 2.71805327522155\n",
      "step: 34 Loss: 1.8469534017356486\n",
      "engine: -2.376330487567067 -3.5644957313506005 -1.1881652437835335\n",
      "manual: -2.376330487567067 -3.5644957313506005 -1.1881652437835335\n",
      "step: 35 Loss: 0.3529341616337959\n",
      "engine: 2.69754990796228 2.69754990796228 2.69754990796228\n",
      "manual: 2.69754990796228 2.69754990796228 2.69754990796228\n",
      "step: 35 Loss: 1.8191938764868263\n",
      "engine: -2.3583699289592346 -3.537554893438852 -1.1791849644796173\n",
      "manual: -2.3583699289592346 -3.537554893438852 -1.1791849644796173\n",
      "step: 36 Loss: 0.3476192951136991\n",
      "engine: 2.677199109222096 2.677199109222096 2.677199109222096\n",
      "manual: 2.677199109222096 2.677199109222096 2.677199109222096\n",
      "step: 36 Loss: 1.7918487676048962\n",
      "engine: -2.3405541350639467 -3.51083120259592 -1.1702770675319734\n",
      "manual: -2.3405541350639467 -3.51083120259592 -1.1702770675319734\n",
      "step: 37 Loss: 0.3423871036978087\n",
      "engine: 2.657000410772607 2.657000410772607 2.657000410772607\n",
      "manual: 2.657000410772607 2.657000410772607 2.657000410772607\n",
      "step: 37 Loss: 1.7649127957114508\n",
      "engine: -2.32287907583148 -3.48431861374722 -1.16143953791574\n",
      "manual: -2.32287907583148 -3.48431861374722 -1.16143953791574\n",
      "step: 38 Loss: 0.33723545005848193\n",
      "engine: 2.6369531306761385 2.6369531306761385 2.6369531306761385\n",
      "manual: 2.6369531306761385 2.6369531306761385 2.6369531306761385\n",
      "step: 38 Loss: 1.738380453345672\n",
      "engine: -2.30534168596094 -3.45801252894141 -1.15267084298047\n",
      "manual: -2.30534168596094 -3.45801252894141 -1.15267084298047\n",
      "step: 39 Loss: 0.3321625180643268\n",
      "engine: 2.617056443993228 2.617056443993228 2.617056443993228\n",
      "manual: 2.617056443993228 2.617056443993228 2.617056443993228\n",
      "step: 39 Loss: 1.7122461077616196\n",
      "engine: -2.2879395604502477 -3.4319093406753716 -1.1439697802251239\n",
      "manual: -2.2879395604502477 -3.4319093406753716 -1.1439697802251239\n",
      "step: 40 Loss: 0.32716671451707957\n",
      "engine: 2.5973094309806495 2.5973094309806495 2.5973094309806495\n",
      "manual: 2.5973094309806495 2.5973094309806495 2.5973094309806495\n",
      "step: 40 Loss: 1.6865040700652563\n",
      "engine: -2.2706707469595315 -3.4060061204392973 -1.1353353734797658\n",
      "manual: -2.2706707469595315 -3.4060061204392973 -1.1353353734797658\n",
      "step: 41 Loss: 0.3222466025686098\n",
      "engine: 2.577711109939381 2.577711109939381 2.577711109939381\n",
      "manual: 2.577711109939381 2.577711109939381 2.577711109939381\n",
      "step: 41 Loss: 1.6611486415762289\n",
      "engine: -2.2535336041963205 -3.380300406294481 -1.1267668020981603\n",
      "manual: -2.2535336041963205 -3.380300406294481 -1.1267668020981603\n",
      "step: 42 Loss: 0.3174008565776287\n",
      "engine: 2.5582604595947984 2.5582604595947984 2.5582604595947984\n",
      "manual: 2.5582604595947984 2.5582604595947984 2.5582604595947984\n",
      "step: 42 Loss: 1.6361741447815472\n",
      "engine: -2.2365267053241027 -3.354790057986154 -1.1182633526620513\n",
      "manual: -2.2365267053241027 -3.354790057986154 -1.1182633526620513\n",
      "step: 43 Loss: 0.31262823147674285\n",
      "engine: 2.538956434338555 2.538956434338555 2.538956434338555\n",
      "manual: 2.538956434338555 2.538956434338555 2.538956434338555\n",
      "step: 43 Loss: 1.6115749438672875\n",
      "engine: -2.2196487720746063 -3.3294731581119095 -1.1098243860373032\n",
      "manual: -2.2196487720746063 -3.3294731581119095 -1.1098243860373032\n",
      "step: 44 Loss: 0.3079275419607692\n",
      "engine: 2.519797974602719 2.519797974602719 2.519797974602719\n",
      "manual: 2.519797974602719 2.519797974602719 2.519797974602719\n",
      "step: 44 Loss: 1.5873454582029913\n",
      "engine: -2.2028986297983693 -3.304347944697554 -1.1014493148991846\n",
      "manual: -2.2028986297983693 -3.304347944697554 -1.1014493148991846\n",
      "step: 45 Loss: 0.3032976483229708\n",
      "engine: 2.5007840139144584 2.5007840139144584 2.5007840139144584\n",
      "manual: 2.5007840139144584 2.5007840139144584 2.5007840139144584\n",
      "step: 45 Loss: 1.5634801710625275\n",
      "engine: -2.1862751767942896 -3.2794127651914344 -1.0931375883971448\n",
      "manual: -2.1862751767942896 -3.2794127651914344 -1.0931375883971448\n",
      "step: 46 Loss: 0.2987374467916814\n",
      "engine: 2.481913483687249 2.481913483687249 2.481913483687249\n",
      "manual: 2.481913483687249 2.481913483687249 2.481913483687249\n",
      "step: 46 Loss: 1.539973635127144\n",
      "engine: -2.1697773633768307 -3.254666045065246 -1.0848886816884153\n",
      "manual: -2.1697773633768307 -3.254666045065246 -1.0848886816884153\n",
      "step: 47 Loss: 0.294245862913907\n",
      "engine: 2.4631853164686213 2.4631853164686213 2.4631853164686213\n",
      "manual: 2.4631853164686213 2.4631853164686213 2.4631853164686213\n",
      "step: 47 Loss: 1.5168204758166555\n",
      "engine: -2.153404177583802 -3.230106266375703 -1.076702088791901\n",
      "manual: -2.153404177583802 -3.230106266375703 -1.076702088791901\n",
      "step: 48 Loss: 0.28982184700221064\n",
      "engine: 2.444598448135533 2.444598448135533 2.444598448135533\n",
      "manual: 2.444598448135533 2.444598448135533 2.444598448135533\n",
      "step: 48 Loss: 1.4940153931566642\n",
      "engine: -2.1371546354128625 -3.205731953119294 -1.0685773177064313\n",
      "manual: -2.1371546354128625 -3.205731953119294 -1.0685773177064313\n",
      "step: 49 Loss: 0.2854643709791678\n",
      "engine: 2.4261518193721745 2.4261518193721745 2.4261518193721745\n",
      "manual: 2.4261518193721745 2.4261518193721745 2.4261518193721745\n",
      "step: 49 Loss: 1.471553162660728\n",
      "engine: -2.121027774146583 -3.1815416612198746 -1.0605138870732915\n",
      "manual: -2.121027774146583 -3.1815416612198746 -1.0605138870732915\n",
      "step: 50 Loss: 0.28117242616882554\n",
      "engine: 2.4078443766586375 2.4078443766586375 2.4078443766586375\n",
      "manual: 2.4078443766586375 2.4078443766586375 2.4078443766586375\n",
      "step: 50 Loss: 1.4494286355516557\n",
      "engine: -2.1050226477836134 -3.15753397167542 -1.0525113238918067\n",
      "manual: -2.1050226477836134 -3.15753397167542 -1.0525113238918067\n",
      "step: 51 Loss: 0.2769450217301209\n",
      "engine: 2.3896750729261367 2.3896750729261367 2.3896750729261367\n",
      "manual: 2.3896750729261367 2.3896750729261367 2.3896750729261367\n",
      "step: 51 Loss: 1.4276367385411342\n",
      "engine: -2.089138323906475 -3.1337074858597127 -1.0445691619532376\n",
      "manual: -2.089138323906475 -3.1337074858597127 -1.0445691619532376\n",
      "step: 52 Loss: 0.2727811835259223\n",
      "engine: 2.3716428679849564 2.3716428679849564 2.3716428679849564\n",
      "manual: 2.3716428679849564 2.3716428679849564 2.3716428679849564\n",
      "step: 52 Loss: 1.4061724733159773\n",
      "engine: -2.0733738815290437 -3.1100608222935655 -1.0366869407645218\n",
      "manual: -2.0733738815290437 -3.1100608222935655 -1.0366869407645218\n",
      "step: 53 Loss: 0.2686799532879258\n",
      "engine: 2.353746728797601 2.353746728797601 2.353746728797601\n",
      "manual: 2.353746728797601 2.353746728797601 2.353746728797601\n",
      "step: 53 Loss: 1.3850309158313518\n",
      "engine: -2.057728409612338 -3.0865926144185067 -1.028864204806169\n",
      "manual: -2.057728409612338 -3.0865926144185067 -1.028864204806169\n",
      "step: 54 Loss: 0.26464038798285755\n",
      "engine: 2.335985629646487 2.335985629646487 2.335985629646487\n",
      "manual: 2.335985629646487 2.335985629646487 2.335985629646487\n",
      "step: 54 Loss: 1.3642072154787235\n",
      "engine: -2.042201006036045 -3.0633015090540674 -1.0211005030180225\n",
      "manual: -2.042201006036045 -3.0633015090540674 -1.0211005030180225\n",
      "step: 55 Loss: 0.2606615593159146\n",
      "engine: 2.318358552229858 2.318358552229858 2.318358552229858\n",
      "manual: 2.318358552229858 2.318358552229858 2.318358552229858\n",
      "step: 55 Loss: 1.343696594174331\n",
      "engine: -2.0267907768811213 -3.040186165321682 -1.0133953884405607\n",
      "manual: -2.0267907768811213 -3.040186165321682 -1.0133953884405607\n",
      "step: 56 Loss: 0.2567425533281487\n",
      "engine: 2.300864485708935 2.300864485708935 2.300864485708935\n",
      "manual: 2.300864485708935 2.300864485708935 2.300864485708935\n",
      "step: 56 Loss: 1.3234943453991606\n",
      "engine: -2.0114968359245466 -3.01724525388682 -1.0057484179622733\n",
      "manual: -2.0114968359245466 -3.01724525388682 -1.0057484179622733\n",
      "step: 57 Loss: 0.2528824700584039\n",
      "engine: 2.2835024267218706 2.2835024267218706 2.2835024267218706\n",
      "manual: 2.2835024267218706 2.2835024267218706 2.2835024267218706\n",
      "step: 57 Loss: 1.3035958332111681\n",
      "engine: -1.9963183042789154 -2.994477456418373 -0.9981591521394577\n",
      "manual: -1.9963183042789154 -2.994477456418373 -0.9981591521394577\n",
      "step: 58 Loss: 0.24908042324994029\n",
      "engine: 2.2662713793752935 2.2662713793752935 2.2662713793752935\n",
      "manual: 2.2662713793752935 2.2662713793752935 2.2662713793752935\n",
      "step: 58 Loss: 1.283996491243899\n",
      "engine: -1.9812543101308933 -2.97188146519634 -0.9906271550654466\n",
      "manual: -1.9812543101308933 -2.97188146519634 -0.9906271550654466\n",
      "step: 59 Loss: 0.2453355400882651\n",
      "engine: 2.2491703552206292 2.2491703552206292 2.2491703552206292\n",
      "manual: 2.2491703552206292 2.2491703552206292 2.2491703552206292\n",
      "step: 59 Loss: 1.264691821700823\n",
      "engine: -1.9663039885472031 -2.9494559828208047 -0.9831519942736016\n",
      "manual: -1.9663039885472031 -2.9494559828208047 -0.9831519942736016\n",
      "step: 60 Loss: 0.24164696096103996\n",
      "engine: 2.232198373220225 2.232198373220225 2.232198373220225\n",
      "manual: 2.232198373220225 2.232198373220225 2.232198373220225\n",
      "step: 60 Loss: 1.2456773943517545\n",
      "engine: -1.9514664813268325 -2.927199721990249 -0.9757332406634163\n",
      "manual: -1.9514664813268325 -2.927199721990249 -0.9757332406634163\n",
      "step: 61 Loss: 0.23801383923388306\n",
      "engine: 2.2153544597066226 2.2153544597066226 2.2153544597066226\n",
      "manual: 2.2153544597066226 2.2153544597066226 2.2153544597066226\n",
      "step: 61 Loss: 1.2269488455355053\n",
      "engine: -1.9367409368849167 -2.905111405327375 -0.9683704684424583\n",
      "manual: -1.9367409368849167 -2.905111405327375 -0.9683704684424583\n",
      "step: 62 Loss: 0.23443534103786653\n",
      "engine: 2.198637648337318 2.198637648337318 2.198637648337318\n",
      "manual: 2.198637648337318 2.198637648337318 2.198637648337318\n",
      "step: 62 Loss: 1.2085018771715628\n",
      "engine: -1.9221265101580869 -2.8831897652371303 -0.9610632550790434\n",
      "manual: -1.9221265101580869 -2.8831897652371303 -0.9610632550790434\n",
      "step: 63 Loss: 0.23091064506578163\n",
      "engine: 2.1820469800465645 2.1820469800465645 2.1820469800465645\n",
      "manual: 2.1820469800465645 2.1820469800465645 2.1820469800465645\n",
      "step: 63 Loss: 1.190332255782583\n",
      "engine: -1.9076223625249895 -2.8614335437874843 -0.9538111812624948\n",
      "manual: -1.9076223625249895 -2.8614335437874843 -0.9538111812624948\n",
      "step: 64 Loss: 0.2274389423753389\n",
      "engine: 2.16558150299527 2.16558150299527 2.16558150299527\n",
      "manual: 2.16558150299527 2.16558150299527 2.16558150299527\n",
      "step: 64 Loss: 1.172435811528813\n",
      "engine: -1.8932276617368586 -2.839841492605288 -0.9466138308684293\n",
      "manual: -1.8932276617368586 -2.839841492605288 -0.9466138308684293\n",
      "step: 65 Loss: 0.22401943619785084\n",
      "engine: 2.1492402725197675 2.1492402725197675 2.1492402725197675\n",
      "manual: 2.1492402725197675 2.1492402725197675 2.1492402725197675\n",
      "step: 65 Loss: 1.154808437255211\n",
      "engine: -1.8789415818552868 -2.81841237278293 -0.9394707909276434\n",
      "manual: -1.8789415818552868 -2.81841237278293 -0.9394707909276434\n",
      "step: 66 Loss: 0.22065134175155296\n",
      "engine: 2.133022351079898 2.133022351079898 2.133022351079898\n",
      "manual: 2.133022351079898 2.133022351079898 2.133022351079898\n",
      "step: 66 Loss: 1.137446087551604\n",
      "engine: -1.8647633031949766 -2.797144954792465 -0.9323816515974883\n",
      "manual: -1.8647633031949766 -2.797144954792465 -0.9323816515974883\n",
      "step: 67 Loss: 0.21733388605891502\n",
      "engine: 2.1169268082068005 2.1169268082068005 2.1169268082068005\n",
      "manual: 2.1169268082068005 2.1169268082068005 2.1169268082068005\n",
      "step: 67 Loss: 1.120344777826158\n",
      "engine: -1.8506920122700166 -2.776038018405025 -0.9253460061350083\n",
      "manual: -1.8506920122700166 -2.776038018405025 -0.9253460061350083\n",
      "step: 68 Loss: 0.21406630776750268\n",
      "engine: 2.100952720450593 2.100952720450593 2.100952720450593\n",
      "manual: 2.100952720450593 2.100952720450593 2.100952720450593\n",
      "step: 68 Loss: 1.1035005833921867\n",
      "engine: -1.8367269017425585 -2.7550903526138377 -0.9183634508712792\n",
      "manual: -1.8367269017425585 -2.7550903526138377 -0.9183634508712792\n",
      "step: 69 Loss: 0.21084785697405112\n",
      "engine: 2.085099171328112 2.085099171328112 2.085099171328112\n",
      "manual: 2.085099171328112 2.085099171328112 2.085099171328112\n",
      "step: 69 Loss: 1.086909638568295\n",
      "engine: -1.8228671703733923 -2.7343007555600884 -0.9114335851866961\n",
      "manual: -1.8228671703733923 -2.7343007555600884 -0.9114335851866961\n",
      "step: 70 Loss: 0.20767779505156864\n",
      "engine: 2.0693652512708294 2.0693652512708294 2.0693652512708294\n",
      "manual: 2.0693652512708294 2.0693652512708294 2.0693652512708294\n",
      "step: 70 Loss: 1.0705681357917958\n",
      "engine: -1.8091120229738422 -2.7136680344607633 -0.9045560114869211\n",
      "manual: -1.8091120229738422 -2.7136680344607633 -0.9045560114869211\n",
      "step: 71 Loss: 0.20455539447928173\n",
      "engine: 2.0537500575730103 2.0537500575730103 2.0537500575730103\n",
      "manual: 2.0537500575730103 2.0537500575730103 2.0537500575730103\n",
      "step: 71 Loss: 1.0544723247452858\n",
      "engine: -1.7954606703586862 -2.6931910055380293 -0.8977303351793431\n",
      "manual: -1.7954606703586862 -2.6931910055380293 -0.8977303351793431\n",
      "step: 72 Loss: 0.20147993867530392\n",
      "engine: 2.0382526943401498 2.0382526943401498 2.0382526943401498\n",
      "manual: 2.0382526943401498 2.0382526943401498 2.0382526943401498\n",
      "step: 72 Loss: 1.03861851149622\n",
      "engine: -1.7819123292998995 -2.6728684939498493 -0.8909561646499498\n",
      "manual: -1.7819123292998995 -2.6728684939498493 -0.8909561646499498\n",
      "step: 73 Loss: 0.1984507218319371\n",
      "engine: 2.022872272437734 2.022872272437734 2.022872272437734\n",
      "manual: 2.022872272437734 2.022872272437734 2.022872272437734\n",
      "step: 73 Loss: 1.0230030576493507\n",
      "engine: -1.7684662224809742 -2.6526993337214613 -0.8842331112404871\n",
      "manual: -1.7684662224809742 -2.6526993337214613 -0.8842331112404871\n",
      "step: 74 Loss: 0.1954670487535079\n",
      "engine: 2.0076079094403294 2.0076079094403294 2.0076079094403294\n",
      "manual: 2.0076079094403294 2.0076079094403294 2.0076079094403294\n",
      "step: 74 Loss: 1.0076223795118424\n",
      "engine: -1.7551215784519911 -2.6326823676779867 -0.8775607892259956\n",
      "manual: -1.7551215784519911 -2.6326823676779867 -0.8775607892259956\n",
      "step: 75 Loss: 0.19252823469673805\n",
      "engine: 1.9924587295810294 1.9924587295810294 1.9924587295810294\n",
      "manual: 1.9924587295810294 1.9924587295810294 1.9924587295810294\n",
      "step: 75 Loss: 0.9924729472709124\n",
      "engine: -1.7418776315848703 -2.6128164473773055 -0.8709388157924352\n",
      "manual: -1.7418776315848703 -2.6128164473773055 -0.8709388157924352\n",
      "step: 76 Loss: 0.18963360521348233\n",
      "engine: 1.9774238637012598 1.9774238637012598 1.9774238637012598\n",
      "manual: 1.9774238637012598 1.9774238637012598 1.9774238637012598\n",
      "step: 76 Loss: 0.9775512841838045\n",
      "engine: -1.7287336220294094 -2.593100433044114 -0.8643668110147047\n",
      "manual: -1.7287336220294094 -2.593100433044114 -0.8643668110147047\n",
      "step: 77 Loss: 0.18678249599593255\n",
      "engine: 1.962502449200949 1.962502449200949 1.962502449200949\n",
      "manual: 1.962502449200949 1.962502449200949 1.962502449200949\n",
      "step: 77 Loss: 0.9628539657799309\n",
      "engine: -1.7156887956694007 -2.573533193504101 -0.8578443978347003\n",
      "manual: -1.7156887956694007 -2.573533193504101 -0.8578443978347003\n",
      "step: 78 Loss: 0.1839742527240949\n",
      "engine: 1.947693629989054 1.947693629989054 1.947693629989054\n",
      "manual: 1.947693629989054 1.947693629989054 1.947693629989054\n",
      "step: 78 Loss: 0.9483776190749844\n",
      "engine: -1.7027424040793449 -2.5541136061190173 -0.8513712020396724\n",
      "manual: -1.7027424040793449 -2.5541136061190173 -0.8513712020396724\n",
      "step: 79 Loss: 0.1812082309156192\n",
      "engine: 1.9329965564344747 1.9329965564344747 1.9329965564344747\n",
      "manual: 1.9329965564344747 1.9329965564344747 1.9329965564344747\n",
      "step: 79 Loss: 0.9341189217968843\n",
      "engine: -1.6898937044814062 -2.5348405567221093 -0.8449468522407031\n",
      "manual: -1.6898937044814062 -2.5348405567221093 -0.8449468522407031\n",
      "step: 80 Loss: 0.17848379577786813\n",
      "engine: 1.918410385317289 1.918410385317289 1.918410385317289\n",
      "manual: 1.918410385317289 1.918410385317289 1.918410385317289\n",
      "step: 80 Loss: 0.9200746016233072\n",
      "engine: -1.6771419597027588 -2.515712939554138 -0.8385709798513794\n",
      "manual: -1.6771419597027588 -2.515712939554138 -0.8385709798513794\n",
      "step: 81 Loss: 0.17580032206222565\n",
      "engine: 1.9039342797804188 1.9039342797804188 1.9039342797804188\n",
      "manual: 1.9039342797804188 1.9039342797804188 1.9039342797804188\n",
      "step: 81 Loss: 0.9062414354307455\n",
      "engine: -1.664486438133281 -2.4967296571999213 -0.8322432190666404\n",
      "manual: -1.664486438133281 -2.4967296571999213 -0.8322432190666404\n",
      "step: 82 Loss: 0.17315719392060103\n",
      "engine: 1.8895674092815913 1.8895674092815913 1.8895674092815913\n",
      "manual: 1.8895674092815913 1.8895674092815913 1.8895674092815913\n",
      "step: 82 Loss: 0.8926162485547862\n",
      "engine: -1.6519264136835545 -2.477889620525332 -0.8259632068417773\n",
      "manual: -1.6519264136835545 -2.477889620525332 -0.8259632068417773\n",
      "step: 83 Loss: 0.17055380476408813\n",
      "engine: 1.8753089495457083 1.8753089495457083 1.8753089495457083\n",
      "manual: 1.8753089495457083 1.8753089495457083 1.8753089495457083\n",
      "step: 83 Loss: 0.879195914061557\n",
      "engine: -1.6394611657431142 -2.4591917486146713 -0.8197305828715571\n",
      "manual: -1.6394611657431142 -2.4591917486146713 -0.8197305828715571\n",
      "step: 84 Loss: 0.16798955712373567\n",
      "engine: 1.8611580825175515 1.8611580825175515 1.8611580825175515\n",
      "manual: 1.8611580825175515 1.8611580825175515 1.8611580825175515\n",
      "step: 84 Loss: 0.8659773520301023\n",
      "engine: -1.6270899791392566 -2.440634968708885 -0.8135449895696283\n",
      "manual: -1.6270899791392566 -2.440634968708885 -0.8135449895696283\n",
      "step: 85 Loss: 0.16546386251346165\n",
      "engine: 1.8471139963148566 1.8471139963148566 1.8471139963148566\n",
      "manual: 1.8471139963148566 1.8471139963148566 1.8471139963148566\n",
      "step: 85 Loss: 0.8529575288455601\n",
      "engine: -1.6148121440958292 -2.4222182161437438 -0.8074060720479146\n",
      "manual: -1.6148121440958292 -2.4222182161437438 -0.8074060720479146\n",
      "step: 86 Loss: 0.16297614129496057\n",
      "engine: 1.8331758851817135 1.8331758851817135 1.8331758851817135\n",
      "manual: 1.8331758851817135 1.8331758851817135 1.8331758851817135\n",
      "step: 86 Loss: 0.8401334565029397\n",
      "engine: -1.6026269561926085 -2.403940434288913 -0.8013134780963043\n",
      "manual: -1.6026269561926085 -2.403940434288913 -0.8013134780963043\n",
      "step: 87 Loss: 0.16052582254469908\n",
      "engine: 1.8193429494423672 1.8193429494423672 1.8193429494423672\n",
      "manual: 1.8193429494423672 1.8193429494423672 1.8193429494423672\n",
      "step: 87 Loss: 0.8275021919214129\n",
      "engine: -1.590533716324849 -2.3858005744872735 -0.7952668581624245\n",
      "manual: -1.590533716324849 -2.3858005744872735 -0.7952668581624245\n",
      "step: 88 Loss: 0.15811234392288345\n",
      "engine: 1.8056143954553168 1.8056143954553168 1.8056143954553168\n",
      "manual: 1.8056143954553168 1.8056143954553168 1.8056143954553168\n",
      "step: 88 Loss: 0.8150608362688673\n",
      "engine: -1.5785317306631654 -2.367797595994748 -0.7892658653315827\n",
      "manual: -1.5785317306631654 -2.367797595994748 -0.7892658653315827\n",
      "step: 89 Loss: 0.155735151544403\n",
      "engine: 1.7919894355677854 1.7919894355677854 1.7919894355677854\n",
      "manual: 1.7919894355677854 1.7919894355677854 1.7919894355677854\n",
      "step: 89 Loss: 0.8028065342966376\n",
      "engine: -1.5666203106137502 -2.3499304659206253 -0.7833101553068751\n",
      "manual: -1.5666203106137502 -2.3499304659206253 -0.7833101553068751\n",
      "step: 90 Loss: 0.1533936998517202\n",
      "engine: 1.7784672880705443 1.7784672880705443 1.7784672880705443\n",
      "manual: 1.7784672880705443 1.7784672880705443 1.7784672880705443\n",
      "step: 90 Loss: 0.7907364736842492\n",
      "engine: -1.554798772778831 -2.3321981591682466 -0.7773993863894155\n",
      "manual: -1.554798772778831 -2.3321981591682466 -0.7773993863894155\n",
      "step: 91 Loss: 0.15108745148965994\n",
      "engine: 1.7650471771530398 1.7650471771530398 1.7650471771530398\n",
      "manual: 1.7650471771530398 1.7650471771530398 1.7650471771530398\n",
      "step: 91 Loss: 0.7788478843939786\n",
      "engine: -1.5430664389174922 -2.3145996583762383 -0.7715332194587461\n",
      "manual: -1.5430664389174922 -2.3145996583762383 -0.7715332194587461\n",
      "step: 92 Loss: 0.1488158771820944\n",
      "engine: 1.7517283328589084 1.7517283328589084 1.7517283328589084\n",
      "manual: 1.7517283328589084 1.7517283328589084 1.7517283328589084\n",
      "step: 92 Loss: 0.7671380380351627\n",
      "engine: -1.5314226359067291 -2.2971339538600937 -0.7657113179533646\n",
      "manual: -1.5314226359067291 -2.2971339538600937 -0.7657113179533646\n",
      "step: 93 Loss: 0.14657845561046964\n",
      "engine: 1.7385099910417772 1.7385099910417772 1.7385099910417772\n",
      "manual: 1.7385099910417772 1.7385099910417772 1.7385099910417772\n",
      "step: 93 Loss: 0.75560424723802\n",
      "engine: -1.5198666957028735 -2.27980004355431 -0.7599333478514367\n",
      "manual: -1.5198666957028735 -2.27980004355431 -0.7599333478514367\n",
      "step: 94 Loss: 0.1443746732941732\n",
      "engine: 1.725391393321443 1.725391393321443 1.725391393321443\n",
      "manual: 1.725391393321443 1.725391393321443 1.725391393321443\n",
      "step: 94 Loss: 0.7442438650369276\n",
      "engine: -1.5083979553032165 -2.262596932954825 -0.7541989776516083\n",
      "manual: -1.5083979553032165 -2.262596932954825 -0.7541989776516083\n",
      "step: 95 Loss: 0.14220402447268277\n",
      "engine: 1.7123717870403485 1.7123717870403485 1.7123717870403485\n",
      "manual: 1.7123717870403485 1.7123717870403485 1.7123717870403485\n",
      "step: 95 Loss: 0.7330542842629392\n",
      "engine: -1.4970157567079951 -2.2455236350619927 -0.7485078783539976\n",
      "manual: -1.4970157567079951 -2.2455236350619927 -0.7485078783539976\n",
      "step: 96 Loss: 0.1400660109895007\n",
      "engine: 1.6994504252204088 1.6994504252204088 1.6994504252204088\n",
      "manual: 1.6994504252204088 1.6994504252204088 1.6994504252204088\n",
      "step: 96 Loss: 0.7220329369454571\n",
      "engine: -1.485719446882662 -2.228579170323993 -0.742859723441331\n",
      "manual: -1.485719446882662 -2.228579170323993 -0.742859723441331\n",
      "step: 97 Loss: 0.1379601421778327\n",
      "engine: 1.6866265665201432 1.6866265665201432 1.6866265665201432\n",
      "manual: 1.6866265665201432 1.6866265665201432 1.6866265665201432\n",
      "step: 97 Loss: 0.7111772937228817\n",
      "engine: -1.4745083777203405 -2.2117625665805107 -0.7372541888601702\n",
      "manual: -1.4745083777203405 -2.2117625665805107 -0.7372541888601702\n",
      "step: 98 Loss: 0.1358859347479669\n",
      "engine: 1.6738994751921563 1.6738994751921563 1.6738994751921563\n",
      "manual: 1.6738994751921563 1.6738994751921563 1.6738994751921563\n",
      "step: 98 Loss: 0.7004848632621441\n",
      "engine: -1.46338190600477 -2.195072859007155 -0.731690953002385\n",
      "manual: -1.46338190600477 -2.195072859007155 -0.731690953002385\n",
      "step: 99 Loss: 0.13384291267638457\n",
      "engine: 1.6612684210409139 1.6612684210409139 1.6612684210409139\n",
      "manual: 1.6612684210409139 1.6612684210409139 1.6612684210409139\n",
      "step: 99 Loss: 0.6899531916869428\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "model = neuron(2)\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for w in model.w:\n",
    "            zero_grad(w)\n",
    "        zero_grad(model.b)\n",
    "\n",
    "        y = model.pred(x)\n",
    "        l = (y-ytrue)**2\n",
    "        \n",
    "        backward(l)\n",
    "\n",
    "        delta = 2*(y.value - ytrue)\n",
    "\n",
    "        manual_dw1 = delta * x1\n",
    "        manual_dw2 = delta * x2\n",
    "        manual_db = delta\n",
    "    \n",
    "        print(\"engine:\", model.w[0].grad, model.w[1].grad, model.b.grad)\n",
    "        print(\"manual:\", manual_dw1, manual_dw2, manual_db)\n",
    "\n",
    "        for w in model.w:\n",
    "            w.value -= n * w.grad\n",
    "        model.b.value -= n* model.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192012b",
   "metadata": {},
   "source": [
    "## Phase 3 — Adding Depth (2 → 3 → 1)\n",
    "\n",
    "Now we build a multi-layer network:\n",
    "\n",
    "Input (2)\n",
    "  ↓\n",
    "Linear (2 → 3)\n",
    "  ↓\n",
    "ReLU\n",
    "  ↓\n",
    "Linear (3 → 1)\n",
    "  ↓\n",
    "Loss\n",
    "\n",
    "This is where real backpropagation complexity begins.\n",
    "\n",
    "We now test:\n",
    "\n",
    "- Gradient flow through depth\n",
    "- Nonlinear gating via ReLU\n",
    "- Reverse topological traversal correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9daae",
   "metadata": {},
   "source": [
    "### Layer Abstraction\n",
    "\n",
    "A layer is simply a collection of neurons sharing the same input.\n",
    "\n",
    "Each neuron corresponds to one row of a weight matrix.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "y = W·x + b\n",
    "\n",
    "Even though everything is scalar-based,\n",
    "this reproduces matrix multiplication behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f700aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class for the different layers\n",
    "class layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.neurons = [neuron(dim_in) for _ in range(dim_out)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        yout = []\n",
    "        if len(x) != self.dim_in:\n",
    "            raise ValueError(f\"Layer expected input dimension {self.dim_in}, got {len(x)}\")\n",
    "        else:\n",
    "            for neuron in self.neurons:\n",
    "                yout.append(neuron.pred(x))\n",
    "        return yout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5a0c3",
   "metadata": {},
   "source": [
    "### ReLU — First Nonlinearity\n",
    "\n",
    "ReLU(z) = max(0, z)\n",
    "\n",
    "Backward rule:\n",
    "\n",
    "If z > 0:\n",
    "    dL/dz = dL/da\n",
    "Else:\n",
    "    dL/dz = 0\n",
    "\n",
    "This introduces gradient gating.\n",
    "\n",
    "If a neuron’s pre-activation is negative,\n",
    "it receives zero gradient and does not learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3789416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(node):\n",
    "    out = Node(\n",
    "        value = max(0, node.value),\n",
    "        parents = (node,),\n",
    "    )\n",
    "    def backward():\n",
    "        if node.value > 0:\n",
    "            node.grad += out.grad\n",
    "        else:\n",
    "            node.grad += 0\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13776fb",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e5b2b",
   "metadata": {},
   "source": [
    "### Full Forward Pass\n",
    "\n",
    "For each example:\n",
    "\n",
    "1. h = hidden.forward(x)\n",
    "2. a = relu(h)  (element-wise)\n",
    "3. y = output.forward(a)\n",
    "4. L = (y − y_true)²\n",
    "\n",
    "Backward then computes gradients for all 13 parameters\n",
    "in a single reverse traversal.\n",
    "\n",
    "This is where reverse-mode autodiff becomes powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d49dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layer(2,3)\n",
    "output = layer(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d81b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (2,3,16),\n",
    "    (1,1,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d511dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.895023979266121, -8.842535968899181] -2.9475119896330604\n",
      "[0, 0] 0\n",
      "[-2.0128499251202996, -3.0192748876804494] -1.0064249625601498\n",
      "step: 0 Loss: 255.04092904529298\n",
      "[-1.7048447311029908, -1.7048447311029908] -1.7048447311029908\n",
      "[0, 0] 0\n",
      "[-0.6072360562529331, -0.6072360562529331] -0.6072360562529331\n",
      "step: 0 Loss: 21.359826198667072\n",
      "[-12.954328073596951, -19.431492110395425] -6.4771640367984755\n",
      "[0, 0] 0\n",
      "[-4.6761363972598415, -7.014204595889762] -2.3380681986299208\n",
      "step: 1 Loss: 237.08551463525623\n",
      "[-3.576660520999451, -3.576660520999451] -3.576660520999451\n",
      "[0, 0] 0\n",
      "[-1.27742719047531, -1.27742719047531] -1.27742719047531\n",
      "step: 1 Loss: 15.262142061966232\n",
      "[-28.96059502492013, -43.44089253738019] -14.480297512460066\n",
      "[0, 0] 0\n",
      "[-10.396534161418689, -15.594801242128034] -5.1982670807093445\n",
      "step: 2 Loss: 198.40592152517436\n",
      "[-4.002312312574992, -4.002312312574992] -4.002312312574992\n",
      "[0, 0] 0\n",
      "[-1.4339319024270927, -1.4339319024270927] -1.4339319024270927\n",
      "step: 2 Loss: 3.5893044612046228\n",
      "[-42.94422917795499, -64.41634376693248] -21.472114588977494\n",
      "[0, 0] 0\n",
      "[-15.402942676471495, -23.104414014707242] -7.701471338235748\n",
      "step: 3 Loss: 91.79922311940034\n",
      "[11.76331304527963, 11.76331304527963] 11.76331304527963\n",
      "[0, 0] 0\n",
      "[4.217402551797069, 4.217402551797069] 4.217402551797069\n",
      "step: 3 Loss: 9.346462147213186\n",
      "[-14.207484614188147, -21.31122692128222] -7.103742307094073\n",
      "[0, 0] 0\n",
      "[-5.087930863285302, -7.631896294927953] -2.543965431642651\n",
      "step: 4 Loss: 4.195538244111338\n",
      "[13.875031420757688, 13.875031420757688] 13.875031420757688\n",
      "[0, 0] 0\n",
      "[4.969499720377746, 4.969499720377746] 4.969499720377746\n",
      "step: 4 Loss: 12.031905785670068\n",
      "[-10.106530991738728, -15.15979648760809] -5.053265495869364\n",
      "[0, 0] 0\n",
      "[-3.6148554499186147, -5.422283174877922] -1.8074277249593074\n",
      "step: 5 Loss: 2.0158070807488104\n",
      "[12.057638188679038, 12.057638188679038] 12.057638188679038\n",
      "[0, 0] 0\n",
      "[4.313604988501025, 4.313604988501025] 4.313604988501025\n",
      "step: 5 Loss: 9.379394726761594\n",
      "[-10.522030403637853, -15.78304560545678] -5.2610152018189265\n",
      "[0, 0] 0\n",
      "[-3.759325890357031, -5.638988835535546] -1.8796629451785154\n",
      "step: 6 Loss: 2.181902334516937\n",
      "[11.689180519620253, 11.689180519620253] 11.689180519620253\n",
      "[0, 0] 0\n",
      "[4.177664977655139, 4.177664977655139] 4.177664977655139\n",
      "step: 6 Loss: 8.74012450054937\n",
      "[-9.386973330929829, -14.080459996394744] -4.693486665464914\n",
      "[0, 0] 0\n",
      "[-3.350389205626018, -5.025583808439027] -1.675194602813009\n",
      "step: 7 Loss: 1.7026536026452561\n",
      "[10.865908262148197, 10.865908262148197] 10.865908262148197\n",
      "[0, 0] 0\n",
      "[3.87968648324333, 3.87968648324333] 3.87968648324333\n",
      "step: 7 Loss: 7.5854157397798305\n",
      "[-8.955448640661842, -13.433172960992763] -4.477724320330921\n",
      "[0, 0] 0\n",
      "[-3.193361222785561, -4.790041834178341] -1.5966806113927805\n",
      "step: 8 Loss: 1.529744375836367\n",
      "[10.289991907482744, 10.289991907482744] 10.289991907482744\n",
      "[0, 0] 0\n",
      "[3.6708031073690144, 3.6708031073690144] 3.6708031073690144\n",
      "step: 8 Loss: 6.7837532544107795\n",
      "[-8.294922550460855, -12.442383825691284] -4.147461275230428\n",
      "[0, 0] 0\n",
      "[-2.9552611899286787, -4.432891784893018] -1.4776305949643394\n",
      "step: 9 Loss: 1.2915552973335347\n",
      "[9.668631128549723, 9.668631128549723] 9.668631128549723\n",
      "[0, 0] 0\n",
      "[3.4462483129702206, 3.4462483129702206] 3.4462483129702206\n",
      "step: 9 Loss: 5.980955101090824\n",
      "[-7.802565929259026, -11.703848893888539] -3.901282964629513\n",
      "[0, 0] 0\n",
      "[-2.777605127230344, -4.166407690845515] -1.388802563615172\n",
      "step: 10 Loss: 1.1260745969640042\n",
      "[9.121819039681288, 9.121819039681288] 9.121819039681288\n",
      "[0, 0] 0\n",
      "[3.2488013253121584, 3.2488013253121584] 3.2488013253121584\n",
      "step: 10 Loss: 5.305176702249932\n",
      "[-7.287046561995914, -10.93056984299387] -3.643523280997957\n",
      "[0, 0] 0\n",
      "[-2.592148513549235, -3.8882227703238526] -1.2960742567746175\n",
      "step: 11 Loss: 0.967330251424461\n",
      "[8.588901332763111, 8.588901332763111] 8.588901332763111\n",
      "[0, 0] 0\n",
      "[3.0567471604936474, 3.0567471604936474] 3.0567471604936474\n",
      "step: 11 Loss: 4.6860811286417485\n",
      "[-6.835397643472167, -10.25309646520825] -3.4176988217360833\n",
      "[0, 0] 0\n",
      "[-2.4297983760935233, -3.644697564140285] -1.2148991880467617\n",
      "step: 12 Loss: 0.8386542819466392\n",
      "[8.094502485012445, 8.094502485012445] 8.094502485012445\n",
      "[0, 0] 0\n",
      "[2.8788081267257617, 2.8788081267257617] 2.8788081267257617\n",
      "step: 12 Loss: 4.142989430487361\n",
      "[-6.397691857877507, -9.59653778681626] -3.1988459289387534\n",
      "[0, 0] 0\n",
      "[-2.2727382185401366, -3.409107327810205] -1.1363691092700683\n",
      "step: 13 Loss: 0.7239740687400609\n",
      "[7.622107012150806, 7.622107012150806] 7.622107012150806\n",
      "[0, 0] 0\n",
      "[2.709047611147947, 2.709047611147947] 2.709047611147947\n",
      "step: 13 Loss: 3.65528450308951\n",
      "[-5.99605679991048, -8.99408519986572] -2.99802839995524\n",
      "[0, 0] 0\n",
      "[-2.128781566012852, -3.193172349019278] -1.064390783006426\n",
      "step: 14 Loss: 0.6268928144157405\n",
      "[7.177055848064816, 7.177055848064816] 7.177055848064816\n",
      "[0, 0] 0\n",
      "[2.549322512120674, 2.549322512120674] 2.549322512120674\n",
      "step: 14 Loss: 3.2232146708771476\n",
      "[-5.614261543495159, -8.421392315242738] -2.8071307717475795\n",
      "[0, 0] 0\n",
      "[-1.9921206151273119, -2.988180922690968] -0.9960603075636559\n",
      "step: 15 Loss: 0.5419533157995334\n",
      "[6.753482199014417, 6.753482199014417] 6.753482199014417\n",
      "[0, 0] 0\n",
      "[2.3975054186381466, 2.3975054186381466] 2.3975054186381466\n",
      "step: 15 Loss: 2.8376698924920816\n",
      "[-5.258290522142351, -7.887435783213527] -2.6291452610711756\n",
      "[0, 0] 0\n",
      "[-1.8648424391772687, -2.797263658765903] -0.9324212195886343\n",
      "step: 16 Loss: 0.468984388095837\n",
      "[6.352243327003551, 6.352243327003551] 6.352243327003551\n",
      "[0, 0] 0\n",
      "[2.2538661384023633, 2.2538661384023633] 2.2538661384023633\n",
      "step: 16 Loss: 2.4955182280424495\n",
      "[-4.921647610078068, -7.382471415117102] -2.460823805039034\n",
      "[0, 0] 0\n",
      "[-1.7446105576629256, -2.6169158364943885] -0.8723052788314628\n",
      "step: 17 Loss: 0.4054679119728243\n",
      "[5.970679586788713, 5.970679586788713] 5.970679586788713\n",
      "[0, 0] 0\n",
      "[2.1174282165942646, 2.1174282165942646] 2.1174282165942646\n",
      "step: 17 Loss: 2.1912703428432407\n",
      "[-4.605773176489304, -6.908659764733956] -2.302886588244652\n",
      "[0, 0] 0\n",
      "[-1.6319079300711128, -2.4478618951066693] -0.8159539650355564\n",
      "step: 18 Loss: 0.3505925823779889\n",
      "[5.608551706369833, 5.608551706369833] 5.608551706369833\n",
      "[0, 0] 0\n",
      "[1.9880789254608586, 1.9880789254608586] 1.9880789254608586\n",
      "step: 18 Loss: 1.9215481951692286\n",
      "[-4.307516597967967, -6.4612748969519505] -2.1537582989839836\n",
      "[0, 0] 0\n",
      "[-1.5255938687907642, -2.288390803186146] -0.7627969343953821\n",
      "step: 19 Loss: 0.30290868829551315\n",
      "[5.264384738782119, 5.264384738782119] 5.264384738782119\n",
      "[0, 0] 0\n",
      "[1.8652698538654016, 1.8652698538654016] 1.8652698538654016\n",
      "step: 19 Loss: 1.6824351407650577\n",
      "[-4.026941157601714, -6.040411736402571] -2.013470578800857\n",
      "[0, 0] 0\n",
      "[-1.4256697345726006, -2.1385046018589007] -0.7128348672863003\n",
      "step: 20 Loss: 0.26162399453598967\n",
      "[4.937697585059121, 4.937697585059121] 4.937697585059121\n",
      "[0, 0] 0\n",
      "[1.7488081646446243, 1.7488081646446243] 1.7488081646446243\n",
      "step: 20 Loss: 1.4709387372340677\n",
      "[-3.762239663297922, -5.643359494946883] -1.881119831648961\n",
      "[0, 0] 0\n",
      "[-1.331477206742191, -1.9972158101132864] -0.6657386033710955\n",
      "step: 21 Loss: 0.22578753293836212\n",
      "[4.627530326291765, 4.627530326291765] 4.627530326291765\n",
      "[0, 0] 0\n",
      "[1.638333605732454, 1.638333605732454] 1.638333605732454\n",
      "step: 21 Loss: 1.284037841582941\n",
      "[-3.5130520235642675, -5.2695780353464015] -1.7565260117821337\n",
      "[0, 0] 0\n",
      "[-1.242872696885628, -1.864309045328442] -0.621436348442814\n",
      "step: 22 Loss: 0.19474745648651123\n",
      "[4.333375111323281, 4.333375111323281] 4.333375111323281\n",
      "[0, 0] 0\n",
      "[1.5336484375769157, 1.5336484375769157] 1.5336484375769157\n",
      "step: 22 Loss: 1.1192076218120288\n",
      "[-3.278186432106732, -4.917279648160098] -1.639093216053366\n",
      "[0, 0] 0\n",
      "[-1.1594203577085374, -1.739130536562806] -0.5797101788542687\n",
      "step: 23 Loss: 0.1678343885945005\n",
      "[4.0545188724756045, 4.0545188724756045] 4.0545188724756045\n",
      "[0, 0] 0\n",
      "[1.4344841427366695, 1.4344841427366695] 1.4344841427366695\n",
      "step: 23 Loss: 0.9740371900950896\n",
      "[-3.057162909969533, -4.5857443649543] -1.5285814549847665\n",
      "[0, 0] 0\n",
      "[-1.0809379912144181, -1.6214069868216272] -0.5404689956072091\n",
      "step: 24 Loss: 0.14453659942161406\n",
      "[3.790469140510545, 3.790469140510545] 3.790469140510545\n",
      "[0, 0] 0\n",
      "[1.3406521330210313, 1.3406521330210313] 1.3406521330210313\n",
      "step: 24 Loss: 0.84643124196555\n",
      "[-2.849102003413131, -4.273653005119696] -1.4245510017065655\n",
      "[0, 0] 0\n",
      "[-1.0071037064041843, -1.5106555596062765] -0.5035518532020922\n",
      "step: 25 Loss: 0.12436477084486708\n",
      "[3.5406343970069725, 3.5406343970069725] 3.5406343970069725\n",
      "[0, 0] 0\n",
      "[1.2519303292011785, 1.2519303292011785] 1.2519303292011785\n",
      "step: 25 Loss: 0.7344402652606385\n",
      "[-2.653501036334969, -3.9802515545024537] -1.3267505181674846\n",
      "[0, 0] 0\n",
      "[-0.9377302223966075, -1.4065953335949113] -0.46886511119830376\n",
      "step: 26 Loss: 0.10692318962888869\n",
      "[3.304535360871402, 3.304535360871402] 3.304535360871402\n",
      "[0, 0] 0\n",
      "[1.1681378264587416, 1.1681378264587416] 1.1681378264587416\n",
      "step: 26 Loss: 0.6363408931636488\n",
      "[-2.4696519601509923, -3.7044779402264885] -1.2348259800754962\n",
      "[0, 0] 0\n",
      "[-0.8725588914049598, -1.3088383371074397] -0.4362794457024799\n",
      "step: 27 Loss: 0.09184678752704926\n",
      "[3.0816383595749235, 3.0816383595749235] 3.0816383595749235\n",
      "[0, 0] 0\n",
      "[1.0890757664462707, 1.0890757664462707] 1.0890757664462707\n",
      "step: 27 Loss: 0.5505526553644993\n",
      "[-2.2970590845299577, -3.4455886267949367] -1.1485295422649788\n",
      "[0, 0] 0\n",
      "[-0.811407190060559, -1.2171107850908385] -0.4057035950302795\n",
      "step: 28 Loss: 0.078831250668619\n",
      "[2.871468390174939, 2.871468390174939] 2.871468390174939\n",
      "[0, 0] 0\n",
      "[1.0145672219184054, 1.0145672219184054] 1.0145672219184054\n",
      "step: 28 Loss: 0.4756694806165205\n",
      "[-2.1351151499322265, -3.2026727248983398] -1.0675575749661133\n",
      "[0, 0] 0\n",
      "[-0.7540541311348704, -1.1310811967023056] -0.3770270655674352\n",
      "step: 29 Loss: 0.06760131254329858\n",
      "[2.6735174597800766, 2.6735174597800766] 2.6735174597800766\n",
      "[0, 0] 0\n",
      "[0.9444246556989452, 0.9444246556989452] 0.9444246556989452\n",
      "step: 29 Loss: 0.41041449968038757\n",
      "[-1.9833398816253442, -2.9750098224380164] -0.9916699408126721\n",
      "[0, 0] 0\n",
      "[-0.7003244238176884, -1.0504866357265326] -0.3501622119088442\n",
      "step: 30 Loss: 0.057923667175070075\n",
      "[2.4873095758937898, 2.4873095758937898] 2.4873095758937898\n",
      "[0, 0] 0\n",
      "[0.8784728019214203, 0.8784728019214203] 0.8784728019214203\n",
      "step: 30 Loss: 0.3536506896971631\n",
      "[-1.8411903650795125, -2.7617855476192688] -0.9205951825397563\n",
      "[0, 0] 0\n",
      "[-0.6500213994613927, -0.9750320991920891] -0.32501069973069635\n",
      "step: 31 Loss: 0.049589805150971895\n",
      "[2.312348267618453, 2.312348267618453] 2.312348267618453\n",
      "[0, 0] 0\n",
      "[0.8165300158500836, 0.8165300158500836] 0.8165300158500836\n",
      "step: 31 Loss: 0.30435363357599565\n",
      "[-1.7082045588457597, -2.56230683826864] -0.8541022794228799\n",
      "[0, 0] 0\n",
      "[-0.6029776018057084, -0.9044664027085625] -0.3014888009028542\n",
      "step: 32 Loss: 0.04242140703206479\n",
      "[2.1481565097384867, 2.1481565097384867] 2.1481565097384867\n",
      "[0, 0] 0\n",
      "[0.7584222992297408, 0.7584222992297408] 0.7584222992297408\n",
      "step: 32 Loss: 0.26161290966349343\n",
      "[-1.5838854256720174, -2.375828138508026] -0.7919427128360087\n",
      "[0, 0] 0\n",
      "[-0.5590137994767777, -0.8385206992151665] -0.2795068997383888\n",
      "step: 33 Loss: 0.03626041556269126\n",
      "[1.9942457666136921, 1.9942457666136921] 1.9942457666136921\n",
      "[0, 0] 0\n",
      "[0.7039722805184931, 0.7039722805184931] 0.7039722805184931\n",
      "step: 33 Loss: 0.2246139734852865\n",
      "[-1.4677912482662117, -2.2016868723993177] -0.7338956241331058\n",
      "[0, 0] 0\n",
      "[-0.5179707987399031, -0.7769561981098547] -0.2589853993699516\n",
      "step: 34 Loss: 0.03097104988758102\n",
      "[1.8501419668621855, 1.8501419668621855] 1.8501419668621855\n",
      "[0, 0] 0\n",
      "[0.6530083189311136, 0.6530083189311136] 0.6530083189311136\n",
      "step: 34 Loss: 0.19263544662015344\n",
      "[-1.3594621644573666, -2.03919324668605] -0.6797310822286833\n",
      "[0, 0] 0\n",
      "[-0.4796834519022469, -0.7195251778533704] -0.23984172595112346\n",
      "step: 35 Loss: 0.026433711535367253\n",
      "[1.7153667852346761, 1.7153667852346761] 1.7153667852346761\n",
      "[0, 0] 0\n",
      "[0.6053578251971483, 0.6053578251971483] 0.6053578251971483\n",
      "step: 35 Loss: 0.16503605648962438\n",
      "[-1.2584792059521108, -1.8877188089281662] -0.6292396029760554\n",
      "[0, 0] 0\n",
      "[-0.44400143749003196, -0.666002156235048] -0.22200071874501598\n",
      "step: 36 Loss: 0.02254547403033647\n",
      "[1.5894553060226795, 1.5894553060226795] 1.5894553060226795\n",
      "[0, 0] 0\n",
      "[0.5608534385167279, 0.5608534385167279] 0.5608534385167279\n",
      "step: 36 Loss: 0.14125026919133163\n",
      "[-1.1644161652493255, -1.7466242478739882] -0.5822080826246627\n",
      "[0, 0] 0\n",
      "[-0.41077222700907057, -0.6161583405136059] -0.20538611350453528\n",
      "step: 37 Loss: 0.01921614594345699\n",
      "[1.471944406630308, 1.471944406630308] 1.471944406630308\n",
      "[0, 0] 0\n",
      "[0.5193288687161487, 0.5193288687161487] 0.5193288687161487\n",
      "step: 37 Loss: 0.12077834162141865\n",
      "[-1.0768793996305495, -1.6153190994458242] -0.5384396998152747\n",
      "[0, 0] 0\n",
      "[-0.3798550850989722, -0.5697826276484583] -0.1899275425494861\n",
      "step: 38 Loss: 0.01636809644596637\n",
      "[1.3623851208675462, 1.3623851208675462] 1.3623851208675462\n",
      "[0, 0] 0\n",
      "[0.48062321036594485, 0.48062321036594485] 0.48062321036594485\n",
      "step: 38 Loss: 0.1031815179780189\n",
      "[-0.9954753279470845, -1.4932129919206267] -0.49773766397354224\n",
      "[0, 0] 0\n",
      "[-0.3511095643693759, -0.5266643465540639] -0.17555478218468795\n",
      "step: 39 Loss: 0.01393360408096312\n",
      "[1.260335180720184, 1.260335180720184] 1.260335180720184\n",
      "[0, 0] 0\n",
      "[0.44457826402986994, 0.44457826402986994] 0.44457826402986994\n",
      "step: 39 Loss: 0.0880742148986643\n",
      "[-0.9198379191969254, -1.379756878795388] -0.4599189595984627\n",
      "[0, 0] 0\n",
      "[-0.32440516766672994, -0.48660775150009494] -0.16220258383336497\n",
      "step: 40 Loss: 0.0118544260580774\n",
      "[1.1653677897085926, 1.1653677897085926] 1.1653677897085926\n",
      "[0, 0] 0\n",
      "[0.4110415913266106, 0.4110415913266106] 0.4110415913266106\n",
      "step: 40 Loss: 0.07511939921584403\n",
      "[-0.8496061075018679, -1.274409161252802] -0.42480305375093397\n",
      "[0, 0] 0\n",
      "[-0.299613351718716, -0.449420027578074] -0.149806675859358\n",
      "step: 41 Loss: 0.010079955622749591\n",
      "[1.0770666035137966, 1.0770666035137966] 1.0770666035137966\n",
      "[0, 0] 0\n",
      "[0.3798647062908033, 0.3798647062908033] 0.3798647062908033\n",
      "step: 41 Loss: 0.06402237109504712\n",
      "[-0.7844431159735823, -1.1766646739603734] -0.39222155798679115\n",
      "[0, 0] 0\n",
      "[-0.2766143156032714, -0.41492147340490704] -0.1383071578016357\n",
      "step: 42 Loss: 0.008566728156384104\n",
      "[0.9950319576355179, 0.9950319576355179] 0.9950319576355179\n",
      "[0, 0] 0\n",
      "[0.35090523946564783, 0.35090523946564783] 0.35090523946564783\n",
      "step: 42 Loss: 0.05452661990189719\n",
      "[-0.7240203287776599, -1.08603049316649] -0.36201016438882994\n",
      "[0, 0] 0\n",
      "[-0.25529128954321584, -0.3829369343148238] -0.12764564477160792\n",
      "step: 43 Loss: 0.007277111606853847\n",
      "[0.9188772786080663, 0.9188772786080663] 0.9188772786080663\n",
      "[0, 0] 0\n",
      "[0.3240256419208166, 0.3240256419208166] 0.3240256419208166\n",
      "step: 43 Loss: 0.04640887448618655\n",
      "[-0.6680310442807016, -1.0020465664210523] -0.3340155221403508\n",
      "[0, 0] 0\n",
      "[-0.2355353639659585, -0.35330304594893774] -0.11776768198297925\n",
      "step: 44 Loss: 0.006178837315367555\n",
      "[0.8482334538071896, 0.8482334538071896] 0.8482334538071896\n",
      "[0, 0] 0\n",
      "[0.29909470035037683, 0.29909470035037683] 0.29909470035037683\n",
      "step: 44 Loss: 0.03947555345116955\n",
      "[-0.6161786561772499, -0.9242679842658748] -0.30808932808862494\n",
      "[0, 0] 0\n",
      "[-0.21724130388222138, -0.32586195582333205] -0.10862065194111069\n",
      "step: 45 Loss: 0.005244055599513064\n",
      "[0.7827460873421358, 0.7827460873421358] 0.7827460873421358\n",
      "[0, 0] 0\n",
      "[0.275986545628268, 0.275986545628268] 0.275986545628268\n",
      "step: 45 Loss: 0.0335588471154604\n",
      "[-0.5681865193639508, -0.8522797790459262] -0.2840932596819754\n",
      "[0, 0] 0\n",
      "[-0.20031101153807793, -0.3004665173071169] -0.10015550576903896\n",
      "step: 46 Loss: 0.004448926817638817\n",
      "[0.7220784962653453, 0.7220784962653453] 0.7220784962653453\n",
      "[0, 0] 0\n",
      "[0.2545816888887688, 0.2545816888887688] 0.2545816888887688\n",
      "step: 46 Loss: 0.02851377554979713\n",
      "[-0.5237890791431252, -0.7856836187146878] -0.2618945395715626\n",
      "[0, 0] 0\n",
      "[-0.18465038495609154, -0.2769755774341373] -0.09232519247804577\n",
      "step: 47 Loss: 0.0037729343687425976\n",
      "[0.6659094774038469, 0.6659094774038469] 0.6659094774038469\n",
      "[0, 0] 0\n",
      "[0.23476621567227623, 0.23476621567227623] 0.23476621567227623\n",
      "step: 47 Loss: 0.024215115142217008\n",
      "[-0.4827389790397001, -0.7241084685595501] -0.24136948951985004\n",
      "[0, 0] 0\n",
      "[-0.17017181166322104, -0.25525771749483156] -0.08508590583161052\n",
      "step: 48 Loss: 0.003198545490992604\n",
      "[0.6139352874529651, 0.6139352874529651] 0.6139352874529651\n",
      "[0, 0] 0\n",
      "[0.21643246820311526, 0.21643246820311526] 0.21643246820311526\n",
      "step: 48 Loss: 0.02055502160031741\n",
      "[-0.44480025888755637, -0.6672003883313345] -0.22240012944377818\n",
      "[0, 0] 0\n",
      "[-0.15679176025262487, -0.2351876403789373] -0.07839588012631243\n",
      "step: 49 Loss: 0.0027107093323003612\n",
      "[0.565867736064294, 0.565867736064294] 0.565867736064294\n",
      "[0, 0] 0\n",
      "[0.1994783589274168, 0.1994783589274168] 0.1994783589274168\n",
      "step: 49 Loss: 0.01744064368424876\n",
      "[-0.4097534829802224, -0.6146302244703337] -0.2048767414901112\n",
      "[0, 0] 0\n",
      "[-0.14443257898681353, -0.2166488684802203] -0.07221628949340676\n",
      "step: 50 Loss: 0.002296584581695564\n",
      "[0.5214354207858812, 0.5214354207858812] 0.5214354207858812\n",
      "[0, 0] 0\n",
      "[0.18380779372342046, 0.18380779372342046] 0.18380779372342046\n",
      "step: 50 Loss: 0.014792241461254824\n",
      "[-0.3773904277819429, -0.5660856416729144] -0.18869521389097146\n",
      "[0, 0] 0\n",
      "[-0.1330206152701176, -0.1995309229051764] -0.0665103076350588\n",
      "step: 51 Loss: 0.001945172226030389\n",
      "[0.48038204772681814, 0.48038204772681814] 0.48038204772681814\n",
      "[0, 0] 0\n",
      "[0.16933006912558698, 0.16933006912558698] 0.16933006912558698\n",
      "step: 51 Loss: 0.012541352611577693\n",
      "[-0.34751777845526455, -0.5212766676828968] -0.17375888922763227\n",
      "[0, 0] 0\n",
      "[-0.12248751177441924, -0.18373126766162887] -0.06124375588720962\n",
      "step: 52 Loss: 0.001647101840332763\n",
      "[0.4424671286309998, 0.4424671286309998] 0.4424671286309998\n",
      "[0, 0] 0\n",
      "[0.155960108827492, 0.155960108827492] 0.155960108827492\n",
      "step: 52 Loss: 0.010629327452071196\n",
      "[-0.3199529163705344, -0.47992937455580165] -0.1599764581852672\n",
      "[0, 0] 0\n",
      "[-0.11276871570716032, -0.1691530735607405] -0.05638435785358016\n",
      "step: 53 Loss: 0.001394363005215225\n",
      "[0.40746447817978393, 0.40746447817978393] 0.40746447817978393\n",
      "[0, 0] 0\n",
      "[0.14361792587322447, 0.14361792587322447] 0.14361792587322447\n",
      "step: 53 Loss: 0.009005933578966296\n",
      "[-0.2945265758904575, -0.44178986383568625] -0.14726328794522875\n",
      "[0, 0] 0\n",
      "[-0.10380441013174961, -0.1557066151976244] -0.051902205065874805\n",
      "step: 54 Loss: 0.0011801405064314902\n",
      "[0.3751625313590352, 0.3751625313590352] 0.3751625313590352\n",
      "[0, 0] 0\n",
      "[0.1322287275905842, 0.1322287275905842] 0.1322287275905842\n",
      "step: 54 Loss: 0.007628231082686432\n",
      "[-0.2710794631661152, -0.4066191947491728] -0.1355397315830576\n",
      "[0, 0] 0\n",
      "[-0.09553831784527694, -0.14330747676791541] -0.04776915892263847\n",
      "step: 55 Loss: 0.000998618211780225\n",
      "[0.3453629914859754, 0.3453629914859754] 0.3453629914859754\n",
      "[0, 0] 0\n",
      "[0.12172243299371924, 0.12172243299371924] 0.12172243299371924\n",
      "step: 55 Loss: 0.0064595202025037775\n",
      "[-0.2494641563887641, -0.37419623458314616] -0.12473207819438205\n",
      "[0, 0] 0\n",
      "[-0.08791836733828971, -0.13187755100743456] -0.043959183669144856\n",
      "step: 56 Loss: 0.0008448536912859722\n",
      "[0.3178808873347303, 0.3178808873347303] 0.3178808873347303\n",
      "[0, 0] 0\n",
      "[0.1120336877458201, 0.1120336877458201] 0.1120336877458201\n",
      "step: 56 Loss: 0.005468487710943675\n",
      "[-0.2295423662306386, -0.34431354934595787] -0.1147711831153193\n",
      "[0, 0] 0\n",
      "[-0.08089572402590176, -0.12134358603885263] -0.04044786201295088\n",
      "step: 57 Loss: 0.0007146353129079052\n",
      "[0.2925433579834951, 0.2925433579834951] 0.2925433579834951\n",
      "[0, 0] 0\n",
      "[0.10310143137476221, 0.10310143137476221] 0.10310143137476221\n",
      "step: 57 Loss: 0.00462841958619827\n",
      "[-0.21118628563726186, -0.3167794284558928] -0.10559314281863093\n",
      "[0, 0] 0\n",
      "[-0.07442526317551669, -0.11163789476327504] -0.037212631587758345\n",
      "step: 58 Loss: 0.0006043878762542587\n",
      "[0.2691895386160878, 0.2691895386160878] 0.2691895386160878\n",
      "[0, 0] 0\n",
      "[0.09486885319292963, 0.09486885319292963] 0.09486885319292963\n",
      "step: 58 Loss: 0.003916559486729752\n",
      "[-0.1942763544529934, -0.2914145316794901] -0.0971381772264967\n",
      "[0, 0] 0\n",
      "[-0.06846477972813703, -0.10269716959220554] -0.03423238986406851\n",
      "step: 59 Loss: 0.0005110687366598569\n",
      "[0.24766947322457994, 0.24766947322457994] 0.24766947322457994\n",
      "[0, 0] 0\n",
      "[0.08728300577466078, 0.08728300577466078] 0.08728300577466078\n",
      "step: 59 Loss: 0.0033135238413561914\n",
      "[-0.17870220929046188, -0.2680533139356928] -0.08935110464523094\n",
      "[0, 0] 0\n",
      "[-0.06297532101172837, -0.09446298151759255] -0.031487660505864185\n",
      "step: 60 Loss: 0.00043209739940602225\n",
      "[0.22784389408573735, 0.22784389408573735] 0.22784389408573735\n",
      "[0, 0] 0\n",
      "[0.08029472441539949, 0.08029472441539949] 0.08029472441539949\n",
      "step: 60 Loss: 0.0028028236693027828\n",
      "[-0.16436085013022006, -0.2465412751953301] -0.08218042506511003\n",
      "[0, 0] 0\n",
      "[-0.05792053889161551, -0.08688080833742326] -0.028960269445807756\n",
      "step: 61 Loss: 0.00036528019809856256\n",
      "[0.20958325475430947, 0.20958325475430947] 0.20958325475430947\n",
      "[0, 0] 0\n",
      "[0.07385828390128488, 0.07385828390128488] 0.07385828390128488\n",
      "step: 61 Loss: 0.002370432713751748\n",
      "[-0.15115730043732994, -0.22673595065599492] -0.07557865021866497\n",
      "[0, 0] 0\n",
      "[-0.05326692091324868, -0.07990038136987301] -0.02663346045662434\n",
      "step: 62 Loss: 0.0003087581413523049\n",
      "[0.1927674497383627, 0.1927674497383627] 0.1927674497383627\n",
      "[0, 0] 0\n",
      "[0.06793129765035677, 0.06793129765035677] 0.06793129765035677\n",
      "step: 62 Loss: 0.0020044334831750086\n",
      "[-0.13900309791013243, -0.20850464686519865] -0.06950154895506622\n",
      "[0, 0] 0\n",
      "[-0.04898325717546618, -0.07347488576319927] -0.02449162858773309\n",
      "step: 63 Loss: 0.00026095242501534687\n",
      "[0.17728496008610942, 0.17728496008610942] 0.17728496008610942\n",
      "[0, 0] 0\n",
      "[0.06247441482932268, 0.06247441482932268] 0.06247441482932268\n",
      "step: 63 Loss: 0.001694700079283523\n",
      "[-0.1278167455013955, -0.19172511825209324] -0.06390837275069775\n",
      "[0, 0] 0\n",
      "[-0.04504079818817137, -0.06756119728225705] -0.022520399094085684\n",
      "step: 64 Loss: 0.00022052602831914203\n",
      "[0.16303254574213785, 0.16303254574213785] 0.16303254574213785\n",
      "[0, 0] 0\n",
      "[0.05745121042956361, 0.05745121042956361] 0.05745121042956361\n",
      "step: 64 Loss: 0.001432637722138286\n",
      "[-0.1175224658132327, -0.17628369871984906] -0.05876123290661635\n",
      "[0, 0] 0\n",
      "[-0.04141281501320431, -0.062119222519806463] -0.020706407506602156\n",
      "step: 65 Loss: 0.00018634438378319537\n",
      "[0.14991449543931276, 0.14991449543931276] 0.14991449543931276\n",
      "[0, 0] 0\n",
      "[0.052827919634266446, 0.052827919634266446] 0.052827919634266446\n",
      "step: 65 Loss: 0.0012109508792212884\n",
      "[-0.10805050202703444, -0.16207575304055166] -0.05402525101351722\n",
      "[0, 0] 0\n",
      "[-0.038074704525340335, -0.0571120567880105] -0.019037352262670167\n",
      "step: 66 Loss: 0.00015744723755301793\n",
      "[0.13784231372646633, 0.13784231372646633] 0.13784231372646633\n",
      "[0, 0] 0\n",
      "[0.04857332643154642, 0.04857332643154642] 0.04857332643154642\n",
      "step: 66 Loss: 0.0010234525323799778\n",
      "[-0.09933608808663688, -0.14900413212995534] -0.04966804404331844\n",
      "[0, 0] 0\n",
      "[-0.03500362585685969, -0.052505438785289536] -0.017501812928429845\n",
      "step: 67 Loss: 0.0001330203168832382\n",
      "[0.12673406634317191, 0.12673406634317191] 0.12673406634317191\n",
      "[0, 0] 0\n",
      "[0.04465853199111649, 0.04465853199111649] 0.04465853199111649\n",
      "step: 67 Loss: 0.0008648953331924667\n",
      "[-0.09131964263217585, -0.13697946394826377] -0.04565982131608792\n",
      "[0, 0] 0\n",
      "[-0.032178568170073425, -0.04826785225511014] -0.016089284085036713\n",
      "step: 68 Loss: 0.00011237479648873291\n",
      "[0.11651407621230465, 0.11651407621230465] 0.11651407621230465\n",
      "[0, 0] 0\n",
      "[0.04105684674100871, 0.04105684674100871] 0.04105684674100871\n",
      "step: 68 Loss: 0.00073083252257421\n",
      "[-0.08394591658999928, -0.12591887488499892] -0.04197295829499964\n",
      "[0, 0] 0\n",
      "[-0.02958004980753898, -0.04437007471130847] -0.01479002490376949\n",
      "step: 69 Loss: 9.492692470941037e-05\n",
      "[0.10711235520319747, 0.10711235520319747] 0.10711235520319747\n",
      "[0, 0] 0\n",
      "[0.03774358945084687, 0.03774358945084687] 0.03774358945084687\n",
      "step: 69 Loss: 0.0006174953941693761\n",
      "[-0.07716411151145458, -0.11574616726718187] -0.03858205575572729\n",
      "[0, 0] 0\n",
      "[-0.027190159583409566, -0.04078523937511435] -0.013595079791704783\n",
      "step: 70 Loss: 8.01830919311983e-05\n",
      "[0.09846431784203535, 0.09846431784203535] 0.09846431784203535\n",
      "[0, 0] 0\n",
      "[0.03469598577443999, 0.03469598577443999] 0.03469598577443999\n",
      "step: 70 Loss: 0.000521692237209971\n",
      "[-0.07092717346413792, -0.10639076019620688] -0.03546358673206896\n",
      "[0, 0] 0\n",
      "[-0.024992307624951516, -0.037488461437427276] -0.012496153812475758\n",
      "step: 71 Loss: 6.772520275081244e-05\n",
      "[0.09051029039882125, 0.09051029039882125] 0.09051029039882125\n",
      "[0, 0] 0\n",
      "[0.03189299477295091, 0.03189299477295091] 0.03189299477295091\n",
      "step: 71 Loss: 0.00044071966229675916\n",
      "[-0.0651918585845851, -0.09778778787687764] -0.03259592929229255\n",
      "[0, 0] 0\n",
      "[-0.022971248177429997, -0.034456872266144994] -0.011485624088714999\n",
      "step: 72 Loss: 5.7199849753152066e-05\n",
      "[0.08319524716391952, 0.08319524716391952] 0.08319524716391952\n",
      "[0, 0] 0\n",
      "[0.02931521557111377, 0.02931521557111377] 0.02931521557111377\n",
      "step: 72 Loss: 0.0003722893919780192\n",
      "[-0.059918147866636205, -0.0898772217999543] -0.029959073933318103\n",
      "[0, 0] 0\n",
      "[-0.021112873146163443, -0.03166930971924516] -0.010556436573081722\n",
      "step: 73 Loss: 4.830782547040137e-05\n",
      "[0.07646838808812624, 0.07646838808812624] 0.07646838808812624\n",
      "[0, 0] 0\n",
      "[0.026944738176434706, 0.026944738176434706] 0.026944738176434706\n",
      "step: 73 Loss: 0.00031446424782390737\n",
      "[-0.05506927643109745, -0.08260391464664618] -0.027534638215548725\n",
      "[0, 0] 0\n",
      "[-0.019404222198910505, -0.029106333298365757] -0.009702111099455252\n",
      "step: 74 Loss: 4.079629207937547e-05\n",
      "[0.0702828997884231, 0.0702828997884231] 0.0702828997884231\n",
      "[0, 0] 0\n",
      "[0.024765058966653694, 0.024765058966653694] 0.024765058966653694\n",
      "step: 74 Loss: 0.00026560524908303526\n",
      "[-0.05061124835028231, -0.07591687252542346] -0.025305624175141154\n",
      "[0, 0] 0\n",
      "[-0.017833311629380243, -0.026749967444070363] -0.008916655814690122\n",
      "step: 75 Loss: 3.445127042724243e-05\n",
      "[0.06459559346398325, 0.06459559346398325] 0.06459559346398325\n",
      "[0, 0] 0\n",
      "[0.0227609528493892, 0.0227609528493892] 0.0227609528493892\n",
      "step: 75 Loss: 0.00022432549829125466\n",
      "[-0.04651284151439562, -0.06976926227159344] -0.02325642075719781\n",
      "[0, 0] 0\n",
      "[-0.01638913592035475, -0.024583703880532125] -0.008194567960177375\n",
      "step: 76 Loss: 2.9091988366755436e-05\n",
      "[0.0593666910019078, 0.0593666910019078] 0.0593666910019078\n",
      "[0, 0] 0\n",
      "[0.020918397678898207, 0.020918397678898207] 0.020918397678898207\n",
      "step: 76 Loss: 0.00018945203641080138\n",
      "[-0.04274520529999532, -0.06411780794999299] -0.02137260264999766\n",
      "[0, 0] 0\n",
      "[-0.015061525850179168, -0.02259228877526875] -0.007530762925089584\n",
      "step: 77 Loss: 2.4565507242220103e-05\n",
      "[0.05455951551623787, 0.05455951551623787] 0.05455951551623787\n",
      "[0, 0] 0\n",
      "[0.019224465031163106, 0.019224465031163106] 0.019224465031163106\n",
      "step: 77 Loss: 0.00015999268084354828\n",
      "[-0.039281849528647605, -0.05892277429297141] -0.019640924764323803\n",
      "[0, 0] 0\n",
      "[-0.01384114449356494, -0.02076171674034741] -0.00692057224678247\n",
      "step: 78 Loss: 2.074264987400632e-05\n",
      "[0.05014030177641693, 0.05014030177641693] 0.05014030177641693\n",
      "[0, 0] 0\n",
      "[0.017667253252339556, 0.017667253252339556] 0.017667253252339556\n",
      "step: 78 Loss: 0.00013510857061419687\n",
      "[-0.036098310776121155, -0.05414746616418173] -0.018049155388060577\n",
      "[0, 0] 0\n",
      "[-0.012719369551570525, -0.019079054327355786] -0.006359684775785262\n",
      "step: 79 Loss: 1.7514159549775198e-05\n",
      "[0.046077932410556716, 0.046077932410556716] 0.046077932410556716\n",
      "[0, 0] 0\n",
      "[0.01623579437669641, 0.01623579437669641] 0.01623579437669641\n",
      "step: 79 Loss: 0.0001140903543250909\n",
      "[-0.033172131450489105, -0.049758197175733654] -0.016586065725244552\n",
      "[0, 0] 0\n",
      "[-0.011688285901809007, -0.01753242885271351] -0.005844142950904504\n",
      "step: 80 Loss: 1.4787770522001125e-05\n",
      "[0.04234377118826708, 0.04234377118826708] 0.04234377118826708\n",
      "[0, 0] 0\n",
      "[0.014919995272137932, 0.014919995272137932] 0.014919995272137932\n",
      "step: 80 Loss: 9.633845977231569e-05\n",
      "[-0.030482582988651516, -0.045723874482977274] -0.015241291494325758\n",
      "[0, 0] 0\n",
      "[-0.010740587999813972, -0.01611088199972096] -0.005370293999906986\n",
      "step: 81 Loss: 1.2485464449952988e-05\n",
      "[0.03891143864638395, 0.03891143864638395] 0.03891143864638395\n",
      "[0, 0] 0\n",
      "[0.013710558486748107, 0.013710558486748107] 0.013710558486748107\n",
      "step: 81 Loss: 8.13460172210642e-05\n",
      "[-0.028010639283822095, -0.04201595892573314] -0.014005319641911047\n",
      "[0, 0] 0\n",
      "[-0.009869570460417684, -0.014804355690626526] -0.004934785230208842\n",
      "step: 82 Loss: 1.0541365519714893e-05\n",
      "[0.035756666383170745, 0.035756666383170745] 0.035756666383170745\n",
      "[0, 0] 0\n",
      "[0.012598930829532071, 0.012598930829532071] 0.012598930829532071\n",
      "step: 82 Loss: 6.868469854959257e-05\n",
      "[-0.02573874703593839, -0.03860812055390758] -0.012869373517969195\n",
      "[0, 0] 0\n",
      "[-0.009069047093171816, -0.013603570639757725] -0.004534523546585908\n",
      "step: 83 Loss: 8.899781984559685e-06\n",
      "[0.0328571065705331, 0.0328571065705331] 0.0328571065705331\n",
      "[0, 0] 0\n",
      "[0.011577236184176368, 0.011577236184176368] 0.011577236184176368\n",
      "step: 83 Loss: 5.799248380016313e-05\n",
      "[-0.023650796454874456, -0.03547619468231168] -0.011825398227437228\n",
      "[0, 0] 0\n",
      "[-0.00833334053974393, -0.012500010809615894] -0.004166670269871965\n",
      "step: 84 Loss: 7.513695417324094e-06\n",
      "[0.03019220525699977, 0.03019220525699977] 0.03019220525699977\n",
      "[0, 0] 0\n",
      "[0.010638230810390897, 0.010638230810390897] 0.010638230810390897\n",
      "step: 84 Loss: 4.896350881752721e-05\n",
      "[-0.021731930699269368, -0.03259789604890405] -0.010865965349634684\n",
      "[0, 0] 0\n",
      "[-0.007657215095966849, -0.011485822643950275] -0.0038286075479834246\n",
      "step: 85 Loss: 6.3433634092973304e-06\n",
      "[0.027743040901315547, 0.027743040901315547] 0.027743040901315547\n",
      "[0, 0] 0\n",
      "[0.00977524640289935, 0.00977524640289935] 0.00977524640289935\n",
      "step: 85 Loss: 4.133930932910482e-05\n",
      "[-0.01996851584114625, -0.029952773761719374] -0.009984257920573125\n",
      "[0, 0] 0\n",
      "[-0.007035866100404675, -0.010553799150607012] -0.0035179330502023375\n",
      "step: 86 Loss: 5.3552362507675e-06\n",
      "[0.02549221466346097, 0.02549221466346097] 0.02549221466346097\n",
      "[0, 0] 0\n",
      "[0.008982151394887802, 0.008982151394887802] 0.008982151394887802\n",
      "step: 86 Loss: 3.490154915402602e-05\n",
      "[-0.01834798270878956, -0.02752197406318434] -0.00917399135439478\n",
      "[0, 0] 0\n",
      "[-0.006464864183815086, -0.00969729627572263] -0.003232432091907543\n",
      "step: 87 Loss: 4.520960430306024e-06\n",
      "[0.023423713716694824, 0.023423713716694824] 0.023423713716694824\n",
      "[0, 0] 0\n",
      "[0.008253302761651654, 0.008253302761651654] 0.008253302761651654\n",
      "step: 87 Loss: 2.946575804131991e-05\n",
      "[-0.01685879742192329, -0.025288196132884935] -0.008429398710961645\n",
      "[0, 0] 0\n",
      "[-0.005940144866925197, -0.008910217300387796] -0.0029700724334625985\n",
      "step: 88 Loss: 3.816602424199794e-06\n",
      "[0.021522816576441072, 0.021522816576441072] 0.021522816576441072\n",
      "[0, 0] 0\n",
      "[0.007583512634345577, 0.007583512634345577] 0.007583512634345577\n",
      "step: 88 Loss: 2.4876127225935035e-05\n",
      "[-0.015490330100190304, -0.023235495150285456] -0.007745165050095152\n",
      "[0, 0] 0\n",
      "[-0.005457962282992008, -0.008186943424488011] -0.002728981141496004\n",
      "step: 89 Loss: 3.2219382864985142e-06\n",
      "[0.019775977514294614, 0.019775977514294614] 0.019775977514294614\n",
      "[0, 0] 0\n",
      "[0.006968007548877031, 0.006968007548877031] 0.006968007548877031\n",
      "step: 89 Loss: 2.1001033654971938e-05\n",
      "[-0.014232826795725633, -0.02134924019358845] -0.007116413397862817\n",
      "[0, 0] 0\n",
      "[-0.00501487927379675, -0.007522318910695125] -0.002507439636898375\n",
      "step: 90 Loss: 2.719897859409435e-06\n",
      "[0.018170745098300984, 0.018170745098300984] 0.018170745098300984\n",
      "[0, 0] 0\n",
      "[0.006402399723053598, 0.006402399723053598] 0.006402399723053598\n",
      "step: 90 Loss: 1.7729317628767085e-05\n",
      "[-0.013077300476489936, -0.019615950714734903] -0.006538650238244968\n",
      "[0, 0] 0\n",
      "[-0.004607728965724345, -0.0069115934485865175] -0.0023038644828621726\n",
      "step: 91 Loss: 2.296058468856906e-06\n",
      "[0.0166956645387673, 0.0166956645387673] 0.0166956645387673\n",
      "[0, 0] 0\n",
      "[0.005882652630853823, 0.005882652630853823] 0.005882652630853823\n",
      "step: 91 Loss: 1.4967085545674144e-05\n",
      "[-0.01201550484497017, -0.018023257267455253] -0.006007752422485085\n",
      "[0, 0] 0\n",
      "[-0.004233605534511846, -0.006350408301767769] -0.002116802767255923\n",
      "step: 92 Loss: 1.938247192909907e-06\n",
      "[0.015340207765708427, 0.015340207765708427] 0.015340207765708427\n",
      "[0, 0] 0\n",
      "[0.0054050563506609335, 0.0054050563506609335] 0.0054050563506609335\n",
      "step: 92 Loss: 1.2635049072340421e-05\n",
      "[-0.011039843791216027, -0.01655976568682404] -0.005519921895608014\n",
      "[0, 0] 0\n",
      "[-0.0038898322926555796, -0.0058347484389833695] -0.0019449161463277898\n",
      "step: 93 Loss: 1.6361801092437545e-06\n",
      "[0.014094690985165121, 0.014094690985165121] 0.014094690985165121\n",
      "[0, 0] 0\n",
      "[0.004966198504013743, 0.004966198504013743] 0.004966198504013743\n",
      "step: 93 Loss: 1.0666242232551357e-05\n",
      "[-0.01014334734952927, -0.015215021024293906] -0.005071673674764635\n",
      "[0, 0] 0\n",
      "[-0.0035739532104611003, -0.00536092981569165] -0.0017869766052305502\n",
      "step: 94 Loss: 1.3811778140109458e-06\n",
      "[0.012950214782131295, 0.012950214782131295] 0.012950214782131295\n",
      "[0, 0] 0\n",
      "[0.004562943140568926, 0.004562943140568926] 0.004562943140568926\n",
      "step: 94 Loss: 9.004120293112138e-06\n",
      "[-0.0093195964684497, -0.01397939470267455] -0.00465979823422485\n",
      "[0, 0] 0\n",
      "[-0.0032837064028756985, -0.0049255596043135475] -0.0016418532014378492\n",
      "step: 95 Loss: 1.1659084285836423e-06\n",
      "[0.011898594559868003, 0.011898594559868003] 0.011898594559868003\n",
      "[0, 0] 0\n",
      "[0.004192406219770871, 0.004192406219770871] 0.004192406219770871\n",
      "step: 95 Loss: 7.600930337811262e-06\n",
      "[-0.008562701194391715, -0.012844051791587572] -0.004281350597195858\n",
      "[0, 0] 0\n",
      "[-0.0030170164373285176, -0.004525524655992776] -0.0015085082186642588\n",
      "step: 96 Loss: 9.841842090796603e-07\n",
      "[0.010932309319311958, 0.010932309319311958] 0.010932309319311958\n",
      "[0, 0] 0\n",
      "[0.0038519375559963234, 0.0038519375559963234] 0.0038519375559963234\n",
      "step: 96 Loss: 6.416353506930388e-06\n",
      "[-0.007867238147735088, -0.011800857221602633] -0.003933619073867544\n",
      "[0, 0] 0\n",
      "[-0.0027719722993251664, -0.0041579584489877496] -0.0013859861496625832\n",
      "step: 97 Loss: 8.307784985027975e-07\n",
      "[0.010044442997063195, 0.010044442997063195] 0.010044442997063195\n",
      "[0, 0] 0\n",
      "[0.0035391001429241117, 0.0035391001429241117] 0.0035391001429241117\n",
      "step: 97 Loss: 5.416342369855064e-06\n",
      "[-0.007228230912907791, -0.010842346369361686] -0.0036141154564538953\n",
      "[0, 0] 0\n",
      "[-0.0025468204792425744, -0.0038202307188638616] -0.0012734102396212872\n",
      "step: 98 Loss: 7.012803658090617e-07\n",
      "[0.009228640728449011, 0.009228640728449011] 0.009228640728449011\n",
      "[0, 0] 0\n",
      "[0.003251654737694688, 0.003251654737694688] 0.003251654737694688\n",
      "step: 98 Loss: 4.572151556674384e-06\n",
      "[-0.006641098057284419, -0.00996164708592663] -0.0033205490286422096\n",
      "[0, 0] 0\n",
      "[-0.0023399466539280114, -0.003509919980892017] -0.0011699733269640057\n",
      "step: 99 Loss: 5.919642436316433e-07\n",
      "[0.008479059395958688, 0.008479059395958688] 0.008479059395958688\n",
      "[0, 0] 0\n",
      "[0.0029875424323898485, 0.0029875424323898485] 0.0029875424323898485\n",
      "step: 99 Loss: 3.859508424741785e-06\n"
     ]
    }
   ],
   "source": [
    "n = 0.01\n",
    "\n",
    "for step in range(100):\n",
    "    for x1, x2, ytrue in dataset:\n",
    "        x = [x1, x2]\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                zero_grad(w)\n",
    "            zero_grad(neuron.b)\n",
    "\n",
    "        h = hidden.forward(x)\n",
    "        a = [relu(node) for node in h]\n",
    "        y = output.forward(a)[0]\n",
    "        l = (y - ytrue)**2\n",
    "\n",
    "        backward(l)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            print([w.grad for w in neuron.w], neuron.b.grad)\n",
    "\n",
    "        for neuron in hidden.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        for neuron in output.neurons:\n",
    "            for w in neuron.w:\n",
    "                w.value -= n * w.grad\n",
    "            neuron.b.value -= n * neuron.b.grad\n",
    "\n",
    "        print(\"step:\", step, \"Loss:\", l.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187deef",
   "metadata": {},
   "source": [
    "#### Dead Neurons and Gradient Flow\n",
    "\n",
    "If a hidden neuron has negative pre-activation:\n",
    "\n",
    "ReLU'(z) = 0\n",
    "\n",
    "Then:\n",
    "\n",
    "dL/dw_hidden = 0\n",
    "\n",
    "This demonstrates how nonlinearities gate gradients.\n",
    "\n",
    "Only active neurons update.\n",
    "\n",
    "This is also why initialization matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a821d5",
   "metadata": {},
   "source": [
    "## Key Engineering Insights\n",
    "\n",
    "- Reverse-mode autodiff computes all gradients in one backward pass.\n",
    "- Gradients accumulate via +=, not assignment.\n",
    "- zero_grad is mandatory before backward.\n",
    "- Random initialization breaks symmetry.\n",
    "- ReLU introduces gradient gating.\n",
    "- Depth multiplies derivative terms via chain rule.\n",
    "- Even multi-layer networks can collapse to simpler representations.\n",
    "\n",
    "This project transformed backpropagation\n",
    "from an abstract formula into a concrete computational system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
