{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad576c1",
   "metadata": {},
   "source": [
    "# AutoGrad Engine(Scalar, Reverse-Mode)\n",
    "\n",
    "This implementation builds a minimal reverse-mode automatic differentiation engine from first principles.\n",
    "\n",
    "Core idea:\n",
    "- Forward pass builds a computatuion DAG dynamically\n",
    "- Backward pass applies the chain rule in reverse topological order\n",
    "\n",
    "This is NOT symbolic differentiation.\n",
    "This is NOT numerical differentiation.\n",
    "This is graph-based reverse-mode autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c09a1",
   "metadata": {},
   "source": [
    "## Node initialization\n",
    "\n",
    "Represents a single value in the computation graph.\n",
    "\n",
    "Each Node stores:\n",
    "- Value: forward computed scalar value\n",
    "- grad: accumulated gradient doutput/dvalue\n",
    "- parents: nodes this value depends on (graph edges)\n",
    "- backward_fn: local backward function (closure)\n",
    "\n",
    "The computation graph is implicit:\n",
    "If each node knows its parents, the graph already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df2e1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Structure\n",
    "class Node:\n",
    "    def __init__(self, value, parents=(), backward_fn=None):\n",
    "        self.value = value\n",
    "        self.grad = 0\n",
    "        self.parents = parents\n",
    "        self.backward_fn = backward_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3d486",
   "metadata": {},
   "source": [
    "## FORWARD Operations (GRAPH CONSTRUCTION)\n",
    "\n",
    "Each operation:\n",
    "- computes a forward value\n",
    "- creates exactly one new Node\n",
    "- Records dependencies via parents\n",
    "- Attaches local backward logic as a closure\n",
    "\n",
    "Important:\n",
    "- Forward pass ONLY builds the graph\n",
    "- No gradients are computed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3ae69",
   "metadata": {},
   "source": [
    "### add(a, b)\n",
    "\n",
    "Forward:\n",
    "- value = a.value + b.value\n",
    "- parents = (a,b)\n",
    "\n",
    "Backward (local):\n",
    "- d(out)/d(a) = 1\n",
    "- d(out)/d(b) = 1\n",
    "\n",
    "Gradient contribution is accumulated into parent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db0bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for addition\n",
    "def add(a, b):\n",
    "    out = Node(\n",
    "        value = a.value + b.value,\n",
    "        parents = (a,b)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward():\n",
    "        a.grad += out.grad * 1\n",
    "        b.grad += out.grad * 1\n",
    "    out.backward_fn = backward\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ef185",
   "metadata": {},
   "source": [
    "### mul(a, b)\n",
    "\n",
    "Forward:\n",
    "- value = a.value * b.value\n",
    "- parents = (a,b)\n",
    "\n",
    "Backward (local):\n",
    "- d(out)/d(a) = b.value\n",
    "- d(out)/d(b) = a.value\n",
    "\n",
    "Uses closure to capture forward values needed for backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e237f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for multiplication\n",
    "def mul(a, b):\n",
    "    out = Node(\n",
    "        value = a.value * b.value,\n",
    "        parents = (a,b)\n",
    "    )\n",
    "    # Gradient Accumulation\n",
    "    def backward():\n",
    "        a.grad += out.grad * b.value\n",
    "        b.grad += out.grad * a.value\n",
    "    out.backward_fn = backward\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a4ee5",
   "metadata": {},
   "source": [
    "## Gradient Accumulation\n",
    "\n",
    "Gradients are accumulated (+=), Not overwritten.\n",
    "\n",
    "Calling backward multiple times without resetting gradients will accumulate contributions.\n",
    "\n",
    "This mirrors real frameworks like PyTorch.\n",
    "Resetting gradient is the user's responsibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c9c6a",
   "metadata": {},
   "source": [
    "## post_dfs(node)\n",
    "\n",
    "Post-order Depth-First Search over the computation DAG.\n",
    "\n",
    "Guarantees:\n",
    "- Each node is visited exactly once\n",
    "- Parents are processed before the node\n",
    "- Produces a valid topological ordering\n",
    "\n",
    "Why post-order:\n",
    "Backward pass requires all downstream gradients to be ready before propagating gradients upstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce9a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set()\n",
    "order = []\n",
    "def post_dfs(node):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    for parent in node.parents:\n",
    "        post_dfs(parent)\n",
    "    order.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691717c",
   "metadata": {},
   "source": [
    "## BACKWARD PASS (Core Autograd Engine)\n",
    "\n",
    "Computes gradients for all nodes contributing to the output.\n",
    "\n",
    "Steps:\n",
    "1. Seed output gradient: d(out)/d(out) = 1\n",
    "2. Discover computation graph via post-order DFS\n",
    "3. Build topological order (dependencies first)\n",
    "4. Execute backward functions in reverse order\n",
    "\n",
    "This separates:\n",
    "- Scheduling (engine)\n",
    "- Math (local backward functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "655ee5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Pass\n",
    "def backward(out_node):\n",
    "    post_dfs(out_node)\n",
    "    out_node.grad = 1\n",
    "    for node in reversed(order):\n",
    "        if node.backward_fn:\n",
    "            node.backward_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5e32a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This implementation demonstrates the core of reverse-mode autodiff:\n",
    "\n",
    "- Dynamic graph construction\n",
    "- Implicit DAG representation\n",
    "- Topological sorting via post-order DFS\n",
    "- Reverse execution of local backward functions\n",
    "\n",
    "This is the foundational mechanism behind modern deep learning frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13aa4cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0\n",
      "1 22 18\n"
     ]
    }
   ],
   "source": [
    "x = Node(5)\n",
    "y = Node(4)\n",
    "z = add(x,x)\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad) # Outputs are (1 2 0) because x is being used twice so it takes the blame twice.\n",
    "z = mul(add(x,y),add(x,y))\n",
    "backward(z)\n",
    "print(z.grad, x.grad, y.grad) # Outputs are (1 22 18) the answer should be (1 20 18) but due to the gradient not being reset before calling the x node again it added the previous gradient to the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23272a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
